# Cargar variables de entorno ANTES de cualquier otra cosa
from dotenv import load_dotenv
import pathlib
import os
project_root = pathlib.Path(__file__).parent.parent.parent.parent.parent
env_path = project_root / ".env"
load_dotenv(env_path)

# Verificar que se carg√≥ correctamente
political_url = os.getenv("POLITICAL_REFERRALS_SERVICE_URL")

"""
Servicio de IA simplificado para el Chatbot AI Service

Este servicio se enfoca √∫nicamente en procesamiento de IA y recibe
la configuraci√≥n del proyecto Political Referrals via HTTP.
"""
import logging
import time
from typing import Dict, Any, Optional, List

import google.generativeai as genai
import httpx
from chatbot_ai_service.services.configuration_service import configuration_service
from chatbot_ai_service.services.document_context_service import document_context_service
from chatbot_ai_service.services.session_context_service import session_context_service
from chatbot_ai_service.services.blocking_notification_service import BlockingNotificationService
from chatbot_ai_service.services.cache_service import cache_service
from chatbot_ai_service.services.user_blocking_service import user_blocking_service

logger = logging.getLogger(__name__)

class AIService:
    """Servicio de IA simplificado - solo procesamiento de IA"""
    
    def __init__(self):
        self.model = None
        self._initialized = False
        # üîß FIX: Inicializar api_key en el constructor para evitar AttributeError
        self.api_key = os.getenv("GEMINI_API_KEY")
        
        
        # üîß FIX: Inicializar atributos faltantes
        self.use_gemini_client = True
        self.gemini_client = None
        self.use_rag_orchestrator = False
        self.use_advanced_model_configs = True
        self.use_guardrails = False
        self.strict_guardrails = False
        self._common_responses = {}
        self._response_cache = {}
        
        # üîß FIX: Inicializar _intent_cache y _intent_cache_max_size
        self._intent_cache = {}
        self._intent_cache_max_size = 1000
    
    def _get_safety_settings(self):
        """
        Obtiene los safety settings configurados para permitir contenido pol√≠tico
        """
        # üöÄ CONFIGURACI√ìN SIMPLE: Sin safety settings expl√≠citos (como versi√≥n anterior)
        return None
        
        # üöÄ OPTIMIZACI√ìN: Cache para validaciones comunes
        self._validation_cache = {
            "name": {
                "santiago": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "maria": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "juan": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "carlos": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "ana": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "luis": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "sofia": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "diego": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "andrea": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "cristian": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "natalia": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "sebastian": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "daniel": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "valentina": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
                "alejandro": {"is_valid": True, "confidence": 0.95, "reason": "Nombre com√∫n v√°lido"},
            },
            "lastname": {
                "garcia": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "lopez": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "rodriguez": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "martinez": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "gonzalez": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "perez": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "sanchez": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "ramirez": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "flores": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "torres": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "buitrago": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "rojas": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "silva": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "morales": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
                "castro": {"is_valid": True, "confidence": 0.95, "reason": "Apellido com√∫n v√°lido"},
            },
            "city": {
                "bogota": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "medellin": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "cali": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "soacha": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "barranquilla": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "cartagena": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "bucaramanga": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "pereira": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "santa marta": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "ibague": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "manizales": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "neiva": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "villavicencio": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "armenia": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
                "pastata": {"is_valid": True, "confidence": 0.95, "reason": "Ciudad colombiana v√°lida"},
            }
        }
        
        # Servicio para notificar bloqueos
        self.blocking_notification_service = BlockingNotificationService()
        # Configurar URL del servicio Java desde variable de entorno
        java_service_url = os.getenv("POLITICAL_REFERRALS_SERVICE_URL")
        if java_service_url:
            self.blocking_notification_service.set_java_service_url(java_service_url)
        else:
            logger.warning("POLITICAL_REFERRALS_SERVICE_URL no configurado - funcionalidad de bloqueo limitada")
        
        # üîß OPTIMIZACI√ìN: Cache local para respuestas comunes
        # (Ya inicializado arriba, no duplicar)
        
        # üöÄ OPTIMIZACI√ìN: Respuestas precomputadas gen√©ricas para casos comunes
        self._precomputed_initial_messages = {
            "default": {
                'welcome': "¬°Bienvenido/a! Soy tu candidato. ¬°Juntos construimos el futuro!",
                'contact': "Por favor, guarda este n√∫mero como 'Mi Candidato' para recibir actualizaciones importantes de la campa√±a.",
                'name': "¬øMe confirmas tu nombre para guardarte en mis contactos y personalizar tu experiencia?"
            }
        }
        self._common_responses = {
            # Saludos comunes
            "hola": "saludo_apoyo",
            "hi": "saludo_apoyo", 
            "hello": "saludo_apoyo",
            "buenos d√≠as": "saludo_apoyo", 
            "buenas tardes": "saludo_apoyo",
            "buenas noches": "saludo_apoyo",
            "gracias": "saludo_apoyo",
            
            # Confirmaciones comunes
            "ok": "saludo_apoyo",
            "okay": "saludo_apoyo",
            "okey": "saludo_apoyo",
            "vale": "saludo_apoyo",
            "listo": "saludo_apoyo",
            "entendido": "saludo_apoyo",
            "perfecto": "saludo_apoyo",
            "bien": "saludo_apoyo",
            "si": "saludo_apoyo",
            "s√≠": "saludo_apoyo",
            "no": "saludo_apoyo",
            
            # Explicaciones sobre datos
            "solo puedo dar nombre y apellido": "registration_response",
            "solo puedo dar un nombre y un apellido": "registration_response",
            "puedo solo un nombre y un apellido": "registration_response",
            "solo tengo nombre y apellido": "registration_response",
            "no tengo ciudad": "registration_response",
            "no s√© mi ciudad": "registration_response",
            "no conozco mi ciudad": "registration_response",
            
            # Nombres comunes
            "santiago": "registration_response",
            "juan": "registration_response",
            "maria": "registration_response",
            "carlos": "registration_response",
            "ana": "registration_response",
            "luis": "registration_response",
            "pedro": "registration_response",
            
            # Ciudades comunes
            "bogot√°": "registration_response",
            "medell√≠n": "registration_response",
            "cali": "registration_response",
            "barranquilla": "registration_response",
            "bogota": "registration_response",
            "medellin": "registration_response"
        }
        
        # üîß OPTIMIZACI√ìN: Bypass completo de Gemini para casos comunes
        self.bypass_gemini = True
        
        # üîß OPTIMIZACI√ìN: Configuraci√≥n de rendimiento para Gemini
        self.gemini_performance_config = {
            "temperature": 0.1,  # M√°s determin√≠stico y r√°pido
            "top_p": 0.8,        # Reducir opciones para velocidad
            "top_k": 20,         # Limitar tokens para velocidad
            "max_output_tokens": 100,  # Respuestas m√°s cortas
            "candidate_count": 1  # Solo una respuesta
        }
        
        # [COHETE] FASE 1: Feature flag para usar GeminiClient
        # Permite migraci√≥n gradual sin romper funcionalidad existente
        # üöÄ OPTIMIZACI√ìN: Habilitado por defecto para usar pre-carga de modelos
        self.use_gemini_client = os.getenv("USE_GEMINI_CLIENT", "true").lower() == "true"
        self.gemini_client = None
        
        if self.use_gemini_client:
            logger.info("[OK] GeminiClient habilitado via feature flag USE_GEMINI_CLIENT=true")
            # La inicializaci√≥n se hace de forma lazy en _ensure_gemini_client()
            
        # La pre-carga se har√° despu√©s de cargar las variables de entorno
        
        # [COHETE] FASE 2: Feature flag para usar configuraciones avanzadas por tarea
        # Permite optimizar temperatura, top_p, etc. seg√∫n el tipo de tarea
        # üöÄ OPTIMIZACI√ìN: Habilitado por defecto para usar pre-carga optimizada
        self.use_advanced_model_configs = os.getenv("USE_ADVANCED_MODEL_CONFIGS", "true").lower() == "true"
        
        if self.use_advanced_model_configs and self.use_gemini_client:
            logger.info("[OK] Configuraciones avanzadas de modelo habilitadas (USE_ADVANCED_MODEL_CONFIGS=true)")
        elif self.use_advanced_model_configs and not self.use_gemini_client:
            logger.warning("[ADVERTENCIA] USE_ADVANCED_MODEL_CONFIGS=true pero USE_GEMINI_CLIENT=false. Las configs avanzadas requieren GeminiClient.")
            self.use_advanced_model_configs = False
        
        # [COHETE] FASE 6: Feature flag para usar RAGOrchestrator
        # Habilita el sistema completo de RAG con b√∫squeda h√≠brida y verificaci√≥n
        self.use_rag_orchestrator = os.getenv("USE_RAG_ORCHESTRATOR", "false").lower() == "true"
        self.rag_orchestrator = None
        
        # [ESCUDO] FASE 5: Feature flag para guardrails estrictos
        # Habilita prompts especializados y verificaci√≥n estricta de respuestas
        self.use_guardrails = os.getenv("USE_GUARDRAILS", "true").lower() == "true"
        self.strict_guardrails = os.getenv("STRICT_GUARDRAILS", "true").lower() == "true"
    
    def preload_models_on_startup(self):
        """
        Pre-carga los modelos de IA despu√©s de que se carguen las variables de entorno
        
        Este m√©todo debe ser llamado desde main.py despu√©s de cargar las variables
        de entorno para asegurar que la API key est√© disponible.
        """
        try:
            logger.info("üöÄ Iniciando pre-carga de modelos de IA al startup del servicio...")
            
            # Verificar si tenemos API key disponible
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                logger.warning("‚ö†Ô∏è GEMINI_API_KEY no disponible - saltando pre-carga")
                return
            
            # üîß FIX: Siempre inicializar el modelo principal, no solo el cliente
            logger.info("üöÄ Inicializando modelo principal de IA...")
            self._ensure_model_initialized()
            
            if self.use_gemini_client:
                logger.info("üöÄ Pre-cargando modelos de IA...")
                print(f"üöÄ DEBUG STARTUP - use_gemini_client: {self.use_gemini_client}")
                print(f"üöÄ DEBUG STARTUP - gemini_client antes: {self.gemini_client is not None}")
                self._ensure_gemini_client()
                print(f"üöÄ DEBUG STARTUP - gemini_client despu√©s: {self.gemini_client is not None}")
                logger.info("‚úÖ Pre-carga completada al startup del servicio")
            else:
                logger.info("‚ÑπÔ∏è GeminiClient no habilitado - usando l√≥gica original")
                
        except Exception as e:
            logger.error(f"‚ùå Error durante pre-carga al startup: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            # No fallar el startup si hay error en la pre-carga
    
    def _ensure_gemini_client(self):
        """
        Inicializa el GeminiClient de forma lazy con pre-carga de modelos
        
        Este m√©todo se ejecuta solo cuando se necesita usar el GeminiClient,
        asegurando que las variables de entorno ya est√©n cargadas.
        """
        if self.gemini_client is not None:
            logger.info("‚úÖ GeminiClient ya est√° inicializado")
            return
            
        if not self.use_gemini_client:
            logger.info("‚ö†Ô∏è GeminiClient no est√° habilitado")
            return
            
        try:
            logger.info("üöÄ Inicializando GeminiClient con pre-carga de modelos...")
            from chatbot_ai_service.clients.gemini_client import GeminiClient
            self.gemini_client = GeminiClient()
            logger.info(f"‚úÖ GeminiClient inicializado: {self.gemini_client is not None}")
            
            # üöÄ OPTIMIZACI√ìN: Pre-cargar modelos para mejorar tiempo de respuesta
            logger.info("üöÄ Iniciando pre-carga de modelos de IA...")
            self.gemini_client.preload_models()
            logger.info("‚úÖ Pre-carga de modelos completada")
            
            # üöÄ DEBUG: Verificar que el modelo principal est√© configurado correctamente
            if self.gemini_client and self.gemini_client.model:
                # LlamaIndex Gemini no tiene model_name, usa __class__.__name__ en su lugar
                model_name = getattr(self.gemini_client.model, 'model_name', 'LlamaIndex-Gemini')
                logger.info(f"üîç Modelo principal configurado: {model_name}")
                logger.info("üîç LlamaIndex Gemini inicializado correctamente")
            else:
                logger.warning("‚ö†Ô∏è Modelo principal no est√° configurado correctamente")
            
        except Exception as e:
            logger.error(f"[ERROR] Error inicializando GeminiClient: {e}")
            import traceback
            logger.error(f"[ERROR] Traceback: {traceback.format_exc()}")
            logger.warning("[ADVERTENCIA] Usando l√≥gica original de AIService como fallback")
            self.use_gemini_client = False
            self.gemini_client = None
        
        if self.use_rag_orchestrator:
            if not self.use_gemini_client:
                logger.warning("[ADVERTENCIA] USE_RAG_ORCHESTRATOR=true pero USE_GEMINI_CLIENT=false. RAG requiere GeminiClient.")
                self.use_rag_orchestrator = False
            else:
                try:
                    from chatbot_ai_service.orchestrators.rag_orchestrator import RAGOrchestrator
                    self.rag_orchestrator = RAGOrchestrator(
                        gemini_client=self.gemini_client,
                        document_service=document_context_service,
                        enable_verification=True,
                        enable_citations=True,
                        enable_guardrails=self.use_guardrails,
                        strict_guardrails=self.strict_guardrails
                    )
                    logger.info(
                        f"[OK] RAGOrchestrator habilitado (USE_RAG_ORCHESTRATOR=true) "
                        f"con guardrails={'ON' if self.use_guardrails else 'OFF'}"
                    )
                except Exception as e:
                    logger.error(f"[ERROR] Error inicializando RAGOrchestrator: {e}")
                    logger.warning("[ADVERTENCIA] Usando l√≥gica original sin RAG")
                    self.use_rag_orchestrator = False
        
        # [GRAFICO] Log resumen de features activadas
        features_status = {
            "GeminiClient": "[OK]" if self.use_gemini_client else "[ERROR]",
            "Advanced Configs": "[OK]" if self.use_advanced_model_configs else "[ERROR]",
            "RAG Orchestrator": "[OK]" if self.use_rag_orchestrator else "[ERROR]",
            "Guardrails": "[OK]" if self.use_guardrails else "[ERROR]",
            "Strict Guardrails": "[OK]" if self.strict_guardrails else "[ERROR]"
        }
        logger.info(f"[CONTROLES] AIService inicializado | Features: {features_status}")
        
        # [COHETE] OPTIMIZACI√ìN: Pre-inicializar modelo para reducir cold start
        self._pre_warm_model()
    
    def _pre_warm_model(self):
        """Pre-calienta el modelo para reducir latencia en primera respuesta"""
        try:
            logger.info("[FUEGO] Pre-calentando modelo Gemini...")
            self._ensure_model_initialized()
            if self.model:
                # Hacer una llamada simple para "despertar" el modelo
                test_prompt = "Responde solo: OK"
                self.model.generate_content(test_prompt)
                logger.info("[OK] Modelo pre-calentado exitosamente")
        except Exception as e:
            logger.warning(f"[ADVERTENCIA] No se pudo pre-calentar el modelo: {e}")
            # No es cr√≠tico, el modelo se inicializar√° en la primera llamada real
    
    # def _get_fallback_response(self, prompt: str) -> str:
    #     """Genera respuesta de fallback inteligente sin usar IA"""
    #     # M√âTODO NO SE USA - COMENTADO
    #     # Analizar el prompt para dar respuesta contextual
    #     prompt_lower = prompt.lower()
    #     
    #     if "nombre" in prompt_lower or "llamo" in prompt_lower:
    #         return "Por favor, comparte tu nombre completo para continuar con el registro."
    #     elif "ciudad" in prompt_lower or "vives" in prompt_lower:
    #         return "?En qu√© ciudad vives? Esto nos ayuda a conectar con promotores de tu regi√≥n."
    #     elif "apellido" in prompt_lower:
    #         return "Perfecto, ahora necesito tu apellido para completar tu informaci√≥n."
    #     elif "c√≥digo" in prompt_lower or "referido" in prompt_lower:
    #         return "Si tienes un c√≥digo de referido, comp√°rtelo. Si no, escribe 'no' para continuar."
    #     elif "t√©rminos" in prompt_lower or "condiciones" in prompt_lower:
    #         return "?Aceptas los t√©rminos y condiciones? Responde 's√≠' o 'no'."
    #     elif "confirmar" in prompt_lower or "correcto" in prompt_lower:
    #         return "?Confirmas que esta informaci√≥n es correcta? Responde 's√≠' o 'no'."
    #     else:
    #         return "Gracias por tu mensaje. Te ayudo a completar tu registro paso a paso."
    
    def _ensure_model_initialized(self):
        """Inicializa el modelo de forma lazy con timeout, probando m√∫ltiples modelos"""
        if self._initialized:
            return
            
        # üîß FIX: Solo obtener api_key si no est√° ya configurada
        if not hasattr(self, 'api_key') or not self.api_key:
            self.api_key = os.getenv("GEMINI_API_KEY")
        
        if self.api_key:
            logger.info(f"[OK] GEMINI_API_KEY cargada correctamente: {self.api_key[:10]}...")
            
            # Lista de modelos optimizada: m√°s moderno y r√°pido primero, fallback estable
            models_to_try = [
                'gemini-2.5-flash',           # M√°s moderno y r√°pido (recomendado)
                'gemini-2.5-pro',             # M√°s potente si flash falla
                'gemini-2.0-flash',           # Estable y r√°pido
                'gemini-1.5-flash-002',       # Versi√≥n espec√≠fica estable
                'gemini-1.5-pro-002'          # Fallback pro estable
            ]
            
            self.model = None
            successful_model = None
            
            for model_name in models_to_try:
                try:
                    # Configuraci√≥n b√°sica para Gemini AI con timeout
                    import signal
                    
                    def timeout_handler(signum, frame):
                        raise TimeoutError(f"Timeout inicializando {model_name}")
                    
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(3)  # 3 segundos timeout por modelo
                
                    try:
                        genai.configure(api_key=self.api_key)
                        test_model = genai.GenerativeModel(model_name)
                        
                        # Hacer una prueba r√°pida para verificar que funciona
                        test_response = test_model.generate_content("Responde solo: OK")
                        if test_response and test_response.text:
                            self.model = test_model
                            successful_model = model_name
                            break
                            
                    finally:
                        signal.alarm(0)  # Cancelar timeout
                    
                except TimeoutError:
                    continue
                except Exception as e:
                    continue
            
            if not self.model:
                self._initialized = False  # No marcar como inicializado si fall√≥
            else:
                self._initialized = True  # Solo marcar como inicializado si funcion√≥
        else:
            logger.warning("GEMINI_API_KEY no configurado")
            self.model = None
            self._initialized = False  # No marcar como inicializado sin API key
        
        # No mostrar lista de modelos disponibles
    
    # def _list_available_models(self):
    #     """Lista todos los modelos disponibles con la API key actual"""
    #     # M√âTODO NO SE USA - COMENTADO
    #     try:
    #         import requests
    #         api_key = os.getenv("GEMINI_API_KEY")
    #         if not api_key:
    #             print("‚ùå GEMINI_API_KEY no configurado")
    #             return []
    #         
    #         url = f"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}"
    #         response = requests.get(url, timeout=10)
    #         
    #         if response.status_code == 200:
    #             models_data = response.json()
    #             models = []
    #             for model in models_data.get('models', []):
    #                 model_name = model.get('name', '').replace('models/', '')
    #                 if 'gemini' in model_name.lower():
    #                     models.append(model_name)
    #                     print(f"üìã Modelo disponible: {model_name}")
    #             
    #             print(f"üéØ Total de modelos Gemini disponibles: {len(models)}")
    #             return models
    #         else:
    #             print(f"‚ùå Error obteniendo modelos: {response.status_code}")
    #             return []
    #             
    #     except Exception as e:
    #         print(f"‚ùå Error listando modelos: {str(e)}")
    #         return []
    
    async def _call_gemini_rest_api(self, prompt: str) -> str:
        """Llama a Gemini usando REST API en lugar de gRPC"""
        try:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={self.api_key}"
            
            payload = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }]
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, json=payload)
                response.raise_for_status()
                
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    return result['candidates'][0]['content']['parts'][0]['text']
                else:
                    return "No se pudo generar respuesta"
                    
        except Exception as e:
            logger.error(f"Error llamando a Gemini REST API: {str(e)}")
            return f"Error: {str(e)}"
    
    async def _generate_fast_ai_response(self, query: str, user_context: Dict[str, Any], 
                                        tenant_context: Dict[str, Any], session_context: str, 
                                        intent: str) -> str:
        """Genera respuesta r√°pida con IA usando contexto completo del usuario"""
        try:
            # üöÄ OPTIMIZACI√ìN: Verificar cach√© primero
            cache_key = f"fast_ai:{hash(query)}:{hash(session_context[:200])}"
            cached_response = self._response_cache.get(cache_key)
            if cached_response:
                logger.info(f"üöÄ RESPUESTA R√ÅPIDA DESDE CACH√â para '{query[:30]}...'")
                return cached_response
            
            # Obtener informaci√≥n del usuario desde el contexto
            user_name = user_context.get('user_name', '')
            user_city = user_context.get('user_city', '')
            user_country = user_context.get('user_country', '')
            user_state = user_context.get('user_state', '')
            
            # Construir contexto personalizado del usuario
            user_info = ""
            if user_name:
                user_info += f"El usuario se llama {user_name}. "
            if user_city:
                user_info += f"Vive en {user_city}. "
            if user_country:
                user_info += f"Pa√≠s: {user_country}. "
            if user_state:
                user_info += f"Estado actual: {user_state}. "
            
            # Obtener informaci√≥n de la campa√±a desde memoria precargada
            campaign_context = tenant_context.get('campaign_context', '')
            branding_config = tenant_context.get('tenant_config', {}).get('branding', {})
            contact_name = branding_config.get('contactName', 'el candidato')
            
            # Crear prompt ultra-optimizado con contexto completo
            prompt = f"""Asistente virtual de {contact_name}. Responde de manera personalizada y profesional.

CONTEXTO DEL USUARIO:
{user_info}

CONTEXTO DE LA CAMPA√ëA:
{campaign_context}

CONSULTA: "{query}"

INSTRUCCIONES:
- Responde de manera personalizada usando el nombre del usuario si est√° disponible
- Menciona su ciudad si es relevante
- S√© conciso pero completo (m√°ximo 999 caracteres)
- Mant√©n un tono profesional y cercano

RESPUESTA:"""
            
            # üîß OPTIMIZACI√ìN: Generaci√≥n ultra-r√°pida con IA
            response = await self._generate_content_ultra_fast(prompt, max_tokens=150)
            
            # üöÄ OPTIMIZACI√ìN: Guardar en cach√©
            self._response_cache[cache_key] = response
            
            return response
            
        except Exception as e:
            logger.error(f"Error en respuesta r√°pida con IA: {e}")
            return None  # Dejar que el flujo normal contin√∫e

    async def _generate_content_ultra_fast(self, prompt: str, max_tokens: int = 50, tenant_id: str = None, query: str = None) -> str:
        """
        Generaci√≥n ultra-r√°pida de contenido usando ULTRA_FAST_MODE
        """
        try:
            # üöÄ DEBUG: Verificar variables en generaci√≥n ultra-r√°pida
            import os
            ultra_fast_mode = os.getenv("ULTRA_FAST_MODE", "false").lower() == "true"
            is_local_dev = os.getenv("LOCAL_DEVELOPMENT", "false").lower() == "true"
            print(f"üöÄ DEBUG ULTRA-FAST - ULTRA_FAST_MODE: {ultra_fast_mode}")
            print(f"üöÄ DEBUG ULTRA-FAST - LOCAL_DEVELOPMENT: {is_local_dev}")
            
            print(f"üöÄ DEBUG ULTRA-FAST - use_gemini_client: {self.use_gemini_client}")
            print(f"üöÄ DEBUG ULTRA-FAST - gemini_client: {self.gemini_client is not None}")
            
            # üöÄ DEBUG: Verificar si necesitamos reinicializar el cliente
            if self.use_gemini_client and self.gemini_client is None:
                print("üöÄ DEBUG ULTRA-FAST: gemini_client es None, reinicializando...")
                self._ensure_gemini_client()
                print(f"üöÄ DEBUG ULTRA-FAST - gemini_client despu√©s de reinicializar: {self.gemini_client is not None}")
                
                # üöÄ DEBUG: Verificar configuraci√≥n del modelo despu√©s de reinicializar
                if self.gemini_client and self.gemini_client.model:
                    model_name = getattr(self.gemini_client.model, 'model_name', 'LlamaIndex-Gemini')
                    print(f"üîç Modelo reinicializado: {model_name}")
                    print("üîç LlamaIndex Gemini reinicializado correctamente")
                else:
                    print("‚ö†Ô∏è Modelo no disponible despu√©s de reinicializar")
            
            # üöÄ NUEVO: Si tenemos tenant_id y query, usar sistema de documentos
            if tenant_id and query and ultra_fast_mode:
                print("üöÄ DEBUG ULTRA-FAST: Usando sistema de documentos")
                from chatbot_ai_service.services.document_context_service import document_context_service
                
                # Extraer solo la pregunta actual si viene en formato de historial
                print(f"üîç DEBUG: Query recibida: '{query[:200]}...'")
                current_query = query
                if "Pregunta actual del usuario:" in query:
                    # Extraer solo la pregunta actual para la b√∫squeda
                    parts = query.split("Pregunta actual del usuario:")
                    current_query = parts[-1].strip()
                    print(f"üîç DEBUG: Extra√≠da pregunta actual: '{current_query}'")
                else:
                    print(f"üîç DEBUG: No hay historial, usando query completa")
                
                # Mejorar la query para mejor recuperaci√≥n de documentos
                enhanced_query = self._enhance_query_for_document_search(current_query)
                print(f"üîç DEBUG: Query mejorada para b√∫squeda: '{enhanced_query}'")
                
                # Obtener contenido de documentos (usar solo la pregunta actual)
                print(f"üîç DEBUG: ANTES de llamar a get_relevant_context con tenant_id={tenant_id}")
                document_content = await document_context_service.get_relevant_context(tenant_id, enhanced_query, max_results=1)
                print(f"üîç DEBUG: DESPU√âS de llamar a get_relevant_context")
                print(f"üîç DEBUG: document_content obtenido: {len(document_content) if document_content else 0} caracteres")
                if document_content:
                    print(f"üîç DEBUG: Primera l√≠nea de document_content: {document_content.split(chr(10))[0][:100]}")
                
                if document_content:
                    # Usar respuesta inmediata basada en documentos
                    # La IA se encarga de entender el contexto y responder apropiadamente
                    contact_name = "el candidato"  # Valor por defecto
                    print(f"ü§ñ ANTES de llamar a _generate_immediate_document_response")
                    print(f"ü§ñ query: '{query[:100]}...'")
                    print(f"ü§ñ document_content: {len(document_content)} caracteres")
                    response = await self._generate_immediate_document_response(query, document_content, contact_name)
                    print(f"ü§ñ DESPU√âS de llamar a _generate_immediate_document_response")
                    print(f"ü§ñ RESPUESTA INMEDIATA GENERADA: {response[:200]}...")
                    return response
                else:
                    print("üîç DEBUG: No hay documentos disponibles, usando fallback")
                    return "No tengo informaci√≥n suficiente sobre ese tema. Puedo ayudarte con otros temas de la campa√±a."
            
            # üöÄ FALLBACK: Usar Gemini si no hay documentos o no es ultra-fast mode
            if self.use_gemini_client and self.gemini_client:
                if ultra_fast_mode:
                    # üöÄ MODO ULTRA-R√ÅPIDO: Sin timeout para permitir procesamiento completo
                    print("üöÄ ULTRA-FAST MODE: Generando sin timeout")
                    try:
                        response = await self.gemini_client.generate_content(prompt)
                        # üîí GARANTIZAR: No exceder 1000 caracteres
                        if response and len(response) > 1000:
                            last_space = response[:1000].rfind(' ')
                            response = response[:last_space] if last_space > 900 else response[:1000]
                        print(f"üöÄ ULTRA-FAST MODE: Respuesta generada: {response[:100]}...")
                        return response
                    except Exception as e:
                        print(f"üöÄ ULTRA-FAST MODE: Error con Gemini: {e}")
                        # Fallback a respuesta gen√©rica
                        return "Sobre este tema, tengo informaci√≥n espec√≠fica que te puede interesar. Te puedo ayudar a conectarte con nuestro equipo para obtener m√°s detalles."
                else:
                    # üöÄ MODO NORMAL: Con timeout para evitar bloqueos
                    import asyncio
                    try:
                        response = await asyncio.wait_for(
                            self.gemini_client.generate_content(prompt),
                            timeout=5.0  # Timeout normal de 5 segundos
                        )
                        # üîí GARANTIZAR: No exceder 1000 caracteres
                        if response and len(response) > 1000:
                            last_space = response[:1000].rfind(' ')
                            response = response[:last_space] if last_space > 900 else response[:1000]
                        return response
                    except asyncio.TimeoutError:
                        logger.warning(f"‚ö†Ô∏è Timeout en generaci√≥n normal para prompt: {prompt[:50]}...")
                        return "saludo_apoyo"  # Fallback seguro
            else:
                # Fallback al m√©todo original
                print("üöÄ DEBUG ULTRA-FAST: Usando fallback al m√©todo original")
                return await self._generate_content(prompt, "intent_classification")
        except Exception as e:
            logger.error(f"Error en generaci√≥n ultra-r√°pida: {e}")
            return "saludo_apoyo"  # Fallback seguro

    async def _generate_immediate_document_response(self, query: str, document_content: str, contact_name: str) -> str:
        """
        Genera respuesta inmediata basada en documentos usando IA
        Respeta la conciencia individual de cada tenant
        """
        try:
            # Extraer informaci√≥n relevante del documento
            content_lower = document_content.lower()
            query_lower = query.lower()
            
            print(f"üîç DEBUG IMMEDIATE: query_lower = '{query_lower}'")
            print(f"üîç DEBUG IMMEDIATE: content_lower preview = '{content_lower[:200]}...'")
            
            # Limpiar el contenido para la IA (remover nombres de archivos y caracteres especiales)
            import re
            clean_content = document_content.replace('*', '').replace('\n', ' ')
            clean_content = re.sub(r'\s*\([^)]*\.pdf\)\s*', ' ', clean_content, flags=re.IGNORECASE)
            clean_content = re.sub(r'\.pdf', ' ', clean_content, flags=re.IGNORECASE)
            
            # Crear prompt para que la IA genere respuesta corta y natural
            print(f"üîç DEBUG: Creando summary_prompt con contact_name={contact_name}...")
            try:
                summary_prompt = f"""
Responde la siguiente pregunta de forma breve y concisa (m√°ximo 800 caracteres):

Pregunta: {query}

Informaci√≥n disponible:
{clean_content[:2000]}

INSTRUCCIONES CR√çTICAS: 
- NO menciones nombres de archivos o documentos
- NO digas "seg√∫n el documento" o "en el documento"
- NO uses la frase gen√©rica "el candidato" bajo ninguna circunstancia
- Si en la informaci√≥n hay nombres espec√≠ficos (personas, entidades), √öSALOS directamente
- Si la pregunta menciona "el responsable", "el candidato" o similares, busca el NOMBRE ESPEC√çFICO en la informaci√≥n y √∫salo
- Responde como si fueras un experto en el tema que conoce la informaci√≥n de primera mano
- S√© espec√≠fico: usa nombres reales, fechas, lugares exactos
- M√°ximo 800 caracteres
- Responde la pregunta directamente y con precisi√≥n

Ejemplo de lo que NO debes hacer:
‚ùå "El candidato propuso..."
‚úÖ "Federico Guti√©rrez propuso..." (si ese es el nombre en la informaci√≥n)

Respuesta:"""
                print(f"üîç DEBUG: summary_prompt creado exitosamente: {len(summary_prompt)} caracteres")
            except Exception as prompt_error:
                print(f"üîç DEBUG: ERROR creando summary_prompt: {prompt_error}")
                import traceback
                traceback.print_exc()
                raise
            
            # Usar IA disponible para generar respuesta
            print(f"üîç DEBUG: ¬øuse_gemini_client? {self.use_gemini_client}")
            print(f"üîç DEBUG: ¬øgemini_client disponible? {self.gemini_client is not None}")
            if self.use_gemini_client and self.gemini_client:
                try:
                    print(f"ü§ñ Llamando a generate_content con prompt de {len(summary_prompt)} caracteres")
                    ai_response = await self.gemini_client.generate_content(summary_prompt)
                    print(f"ü§ñ Respuesta recibida de IA: {len(ai_response) if ai_response else 0} caracteres")
                    if ai_response:
                        print(f"ü§ñ Pre-tratamiento respuesta: {ai_response[:500]}")
                        # üîí GARANTIZAR: No exceder 1000 caracteres bajo ninguna circunstancia
                        if len(ai_response) > 1000:
                            # Truncar de forma inteligente en el √∫ltimo espacio antes de 1000
                            last_space = ai_response[:1000].rfind(' ')
                            if last_space > 900:
                                ai_response = ai_response[:last_space]
                            else:
                                ai_response = ai_response[:1000]
                        print(f"ü§ñ Respuesta final despu√©s de truncamiento: {ai_response}")
                        return ai_response
                except Exception as e:
                    logger.warning(f"Error generando respuesta con IA: {e}")
            
            # Fallback: Si fall√≥ la IA, intentar generar respuesta b√°sica con IA una vez m√°s
            if self.use_gemini_client and self.gemini_client:
                try:
                    simple_prompt = f"Responde brevemente a: {query}. M√°ximo 200 caracteres."
                    ai_response = await self.gemini_client.generate_content(simple_prompt)
                    if ai_response and len(ai_response) > 50:
                        if len(ai_response) > 1000:
                            last_space = ai_response[:1000].rfind(' ')
                            ai_response = ai_response[:last_space] if last_space > 900 else ai_response[:1000]
                        return ai_response
                except Exception as e2:
                    logger.warning(f"Error en fallback de IA: {e2}")
            
            # √öltimo fallback: mensaje gen√©rico muy corto
            return "No tengo informaci√≥n suficiente sobre ese tema. Puedo ayudarte con otros temas de la campa√±a."
            
        except Exception as e:
            logger.error(f"Error generando respuesta inmediata: {e}")
            return "No tengo informaci√≥n suficiente sobre ese tema. Puedo ayudarte con otros temas de la campa√±a."

    async def _generate_content_with_documents(self, prompt: str, max_tokens: int = 200) -> str:
        """
        Generaci√≥n de contenido espec√≠fica para respuestas basadas en documentos
        Con timeout ultra-agresivo para desarrollo local
        """
        try:
            if self.use_gemini_client and self.gemini_client:
                # üöÄ OPTIMIZACI√ìN: Sin timeout para permitir procesamiento completo
                try:
                    response = await self.gemini_client.generate_content(prompt)
                    # üîí GARANTIZAR: No exceder 1000 caracteres
                    if response and len(response) > 1000:
                        last_space = response[:1000].rfind(' ')
                        response = response[:last_space] if last_space > 900 else response[:1000]
                    return response
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error en generaci√≥n con documentos: {e}")
                    # Respuesta de fallback m√°s r√°pida
                    return "Sobre este tema, tengo informaci√≥n espec√≠fica que te puede interesar. Te puedo ayudar a conectarte con nuestro equipo para obtener m√°s detalles."
            else:
                # Fallback al m√©todo original
                return await self._generate_content(prompt, "document_response")
        except Exception as e:
            logger.error(f"Error en generaci√≥n con documentos: {e}")
            return "Sobre este tema, tengo informaci√≥n espec√≠fica que te puede interesar. Te puedo ayudar a conectarte con nuestro equipo para obtener m√°s detalles."

    async def _generate_content_optimized(self, prompt: str, task_type: str = "general") -> str:
        """
        Generaci√≥n optimizada de contenido para m√°xima velocidad
        """
        try:
            if self.use_gemini_client and self.gemini_client:
                # Usar configuraci√≥n optimizada (ya pre-cargado al startup)
                response = await self.gemini_client.generate_content(prompt)
                # üîí GARANTIZAR: No exceder 1000 caracteres
                if response and len(response) > 1000:
                    last_space = response[:1000].rfind(' ')
                    response = response[:last_space] if last_space > 900 else response[:1000]
                return response
            else:
                # Fallback al m√©todo original
                return await self._generate_content(prompt, task_type)
        except Exception as e:
            logger.error(f"Error en generaci√≥n optimizada: {e}")
            return await self._generate_content(prompt, task_type)
    
    async def _generate_content(self, prompt: str, task_type: str = "chat_conversational") -> str:
        """
        Genera contenido usando Gemini, fallback a REST API si gRPC falla
        
        Args:
            prompt: Texto a enviar a Gemini
            task_type: Tipo de tarea para configuraci√≥n optimizada (Fase 2)
        
        Returns:
            Respuesta generada por Gemini
        """
        logger.info(f"üîç DEBUG: _generate_content llamado con task_type: '{task_type}'")
        logger.info(f"üîç DEBUG: Prompt length: {len(prompt)} caracteres")
        logger.info(f"üîç DEBUG: Prompt preview: {prompt[:200]}...")
        
        # üîß OPTIMIZACI√ìN: Cache local para evitar llamadas repetidas
        cache_key = self._generate_cache_key(prompt, task_type)
        cached_response = self._get_cached_response(cache_key)
        if cached_response:
            logger.info(f"‚úÖ CACHE HIT: Respuesta cacheada para '{prompt[:30]}...'")
            return cached_response
        
        # [COHETE] FASE 1 + 2: Delegar a GeminiClient si est√° habilitado
        if self.use_gemini_client and self.gemini_client:
            try:
                # Usar configuraciones avanzadas si est√°n habilitadas (Fase 2)
                use_custom_config = self.use_advanced_model_configs
                
                if use_custom_config:
                    logger.debug(f"üîÑ Delegando a GeminiClient con task_type='{task_type}'")
                else:
                    logger.debug("üîÑ Delegando generaci√≥n de contenido a GeminiClient")
                
                response = await self.gemini_client.generate_content(
                    prompt, 
                    task_type=task_type,
                    use_custom_config=use_custom_config
                )
                
                # üîí GARANTIZAR: No exceder 1000 caracteres
                if response and len(response) > 1000:
                    last_space = response[:1000].rfind(' ')
                    response = response[:last_space] if last_space > 900 else response[:1000]
                
                # üîß OPTIMIZACI√ìN: Guardar en cache
                self._cache_response(cache_key, response)
                return response
                
            except Exception as e:
                logger.warning(f"[ADVERTENCIA] GeminiClient fall√≥, usando l√≥gica original: {e}")
                # Continuar con l√≥gica original como fallback
        
        # MANTENER: L√≥gica original completa como fallback
        try:
            # Intentar con gRPC primero
            if self.model:
                response = self.model.generate_content(prompt, safety_settings=self._get_safety_settings())
                response_text = response.text
                
                # üîß OPTIMIZACI√ìN: Guardar en cache
                self._cache_response(cache_key, response_text)
                return response_text
        except Exception as e:
            logger.warning(f"gRPC fall√≥, usando REST API: {str(e)}")
        
        # Fallback a REST API
        response = await self._call_gemini_rest_api(prompt)
        
        # üîß OPTIMIZACI√ìN: Guardar en cache
        self._cache_response(cache_key, response)
        logger.info(f"üîç DEBUG: _generate_content devolviendo: {len(response)} caracteres")
        logger.info(f"üîç DEBUG: _generate_content respuesta: {response[:200]}...")
        return response
    
    def _get_cached_response(self, key: str) -> Optional[str]:
        """Obtiene respuesta del cache local"""
        return self._response_cache.get(key)
    
    def _cache_response(self, key: str, response: str):
        """Guarda respuesta en cache local"""
        self._response_cache[key] = response
        # Limitar tama√±o del cache
        if len(self._response_cache) > 1000:
            # Eliminar las primeras 200 entradas (m√°s antiguas)
            keys_to_remove = list(self._response_cache.keys())[:200]
            for k in keys_to_remove:
                del self._response_cache[k]
    
    def _generate_cache_key(self, prompt: str, task_type: str = "general") -> str:
        """Genera clave de cache basada en prompt y tipo de tarea"""
        import hashlib
        content = f"{task_type}:{prompt[:100]}"  # Solo primeros 100 chars
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    async def process_chat_message(self, tenant_id: str, query: str, user_context: Dict[str, Any], session_id: str = None, tenant_config: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Procesa un mensaje de chat usando IA espec√≠fica del tenant con sesi√≥n persistente y clasificaci√≥n
        
        Args:
            tenant_id: ID del tenant
            query: Mensaje del usuario
            user_context: Contexto del usuario
            session_id: ID de la sesi√≥n para mantener contexto
            tenant_config: Configuraci√≥n del tenant (incluye ai_config con documentation_bucket_url)
        """
        print(f"INICIANDO PROCESAMIENTO: '{query}' para tenant {tenant_id}")
        
        # üöÄ DEBUG: Verificar variables al inicio del procesamiento
        import os
        ultra_fast_mode = os.getenv("ULTRA_FAST_MODE", "false").lower() == "true"
        is_local_dev = os.getenv("LOCAL_DEVELOPMENT", "false").lower() == "true"
        print(f"üöÄ DEBUG PROCESAMIENTO - ULTRA_FAST_MODE: {ultra_fast_mode}")
        print(f"üöÄ DEBUG PROCESAMIENTO - LOCAL_DEVELOPMENT: {is_local_dev}")
        
        start_time = time.time()
        
        # Inicializar followup_message para evitar errores de None
        followup_message = ""
        
        try:
            logger.info(f"Procesando mensaje para tenant {tenant_id}, sesi√≥n: {session_id}")
            logger.info(f"üîç DEBUG: Iniciando process_chat_message - query: '{query}', tenant_id: {tenant_id}")
            
            # üöÄ NUEVO: Usar directamente el sistema de documentos que funciona
            logger.info(f"üîç DEBUG: ¬øEntrando en ultra_fast_mode? {ultra_fast_mode}")
            if ultra_fast_mode:
                logger.info(f"üöÄ ULTRA-FAST MODE: Usando sistema de documentos directo")
                from chatbot_ai_service.services.document_context_service import document_context_service
                
                # Extraer solo la pregunta actual si viene en formato de historial
                logger.info(f"üîç DEBUG: Query recibida en process_chat_message: '{query[:200]}...'")
                logger.info(f"üîç DEBUG: Longitud total del query: {len(query)}")
                logger.info(f"üîç DEBUG: ¬øContiene 'Pregunta actual del usuario:'? {('Pregunta actual del usuario:' in query)}")
                current_query = query
                if "Pregunta actual del usuario:" in query:
                    # Extraer solo la pregunta actual para la b√∫squeda
                    parts = query.split("Pregunta actual del usuario:")
                    current_query = parts[-1].strip()
                    logger.info(f"üîç DEBUG: Extra√≠da pregunta actual: '{current_query}'")
                    logger.info(f"üîç DEBUG: Longitud de current_query: {len(current_query)}")
                else:
                    logger.info(f"üîç DEBUG: No hay historial, usando query completa")
                
                # Obtener contenido de documentos (usar solo la pregunta actual)
                logger.info(f"üîç DEBUG: ANTES de get_relevant_context con current_query='{current_query}'")
                document_content = await document_context_service.get_relevant_context(tenant_id, current_query, max_results=1)
                logger.info(f"üîç DEBUG: DESPU√âS de get_relevant_context")
                logger.info(f"üîç DEBUG: document_content obtenido: {len(document_content) if document_content else 0} caracteres")
                if document_content:
                    logger.info(f"üîç DEBUG: Primera l√≠nea de document_content: {document_content.split(chr(10))[0][:100]}")
                else:
                    logger.warning(f"üîç DEBUG: document_content est√° vac√≠o o None")
                
                if document_content:
                    # Usar respuesta inmediata basada en documentos
                    contact_name = "el candidato"  # Valor por defecto
                    if tenant_config and tenant_config.get("branding_config"):
                        contact_name = tenant_config["branding_config"].get("contactName", "el candidato")
                    
                    # Usar el query completo (con historial) para que la IA tenga contexto
                    response = await self._generate_immediate_document_response(query, document_content, contact_name)
                    logger.info(f"ü§ñ RESPUESTA INMEDIATA GENERADA: {response[:200]}...")
                    
                    return {
                        "response": response,
                        "followup_message": "",
                        "processing_time": time.time() - start_time,
                        "intent": "conocer_candidato",
                        "confidence": 0.9,
                        "tenant_id": tenant_id
                    }
                else:
                    logger.info(f"üîç DEBUG: No hay documentos disponibles, usando fallback")
                    return {
                        "response": "Sobre este tema, tengo informaci√≥n espec√≠fica que te puede interesar. Te puedo ayudar a conectarte con nuestro equipo para obtener m√°s detalles.",
                        "followup_message": "",
                        "processing_time": time.time() - start_time,
                        "intent": "general_query",
                        "confidence": 0.5,
                        "tenant_id": tenant_id
                    }
            
            # üîß DEBUG CR√çTICO: Verificar par√°metros de entrada
            logger.info(f"üîç DEBUG: Par√°metros recibidos:")
            logger.info(f"   - tenant_id: {tenant_id}")
            logger.info(f"   - query: '{query}'")
            logger.info(f"   - user_context: {user_context}")
            logger.info(f"   - session_id: {session_id}")
            logger.info(f"   - tenant_config: {tenant_config}")
            
            # üöÄ OPTIMIZACI√ìN: Usar memoria precargada + contexto de sesi√≥n para acelerar clasificaci√≥n de IA
            from chatbot_ai_service.services.tenant_memory_service import tenant_memory_service
            from chatbot_ai_service.services.session_context_service import session_context_service
            
            # üöÄ OPTIMIZACI√ìN ULTRA-R√ÅPIDA: Contexto m√≠nimo para m√°xima velocidad
            tenant_context = tenant_memory_service.get_tenant_context(tenant_id)
            if tenant_context:
                logger.info(f"üß† Usando contexto precargado del tenant {tenant_id} para acelerar clasificaci√≥n")
                user_context['tenant_context'] = tenant_context
            
            # üöÄ OPTIMIZACI√ìN: Solo obtener contexto de sesi√≥n si es cr√≠tico
            if session_id and user_context.get("user_state") in ["WAITING_NAME", "WAITING_LASTNAME", "WAITING_CITY"]:
                session_context = session_context_service.build_context_for_ai(session_id)
                if session_context:
                    logger.info(f"üë§ Usando contexto de sesi√≥n cr√≠tico para personalizar respuesta")
                    user_context['session_context'] = session_context
            
            # Clasificar la intencion del mensaje usando IA (pero con contexto precargado)
            logger.info(f"üîç DEBUG: Clasificando intenci√≥n...")
            try:
                classification_result = await self.classify_intent(tenant_id, query, user_context, session_id, tenant_config)
                intent = classification_result.get("category", "saludo_apoyo").strip()
                confidence = classification_result.get("confidence", 0.0)
                logger.info(f"üîç DEBUG: Intenci√≥n clasificada: '{intent}' con confianza: {confidence}")
            except Exception as e:
                logger.error(f"‚ùå ERROR en clasificaci√≥n de intenci√≥n: {str(e)}")
                intent = "saludo_apoyo"
                confidence = 0.5
                logger.info(f"üîç DEBUG: Usando intenci√≥n por defecto: '{intent}'")
            
            # VERIFICAR SI EL USUARIO EST√Å BLOQUEADO PRIMERO
            user_state = user_context.get("user_state", "")
            if user_state == "BLOCKED":
                logger.warn(f"üö´ Usuario bloqueado intentando enviar mensaje: {user_context.get('user_id', 'unknown')}")
                return {
                    "response": "",  # No responder nada a usuarios bloqueados
                    "followup_message": "",
                    "processing_time": time.time() - start_time,
                    "tenant_id": tenant_id,
                    "session_id": session_id,
                    "intent": "blocked_user",
                    "confidence": 1.0
                }
            
            # üöÄ OPTIMIZACI√ìN: Usar configuraci√≥n del tenant desde memoria precargada
            if not tenant_config:
                tenant_context = user_context.get('tenant_context', {})
                tenant_config = tenant_context.get('tenant_config', {})
                if not tenant_config:
                    logger.warning(f"‚ö†Ô∏è No hay configuraci√≥n del tenant {tenant_id} en memoria precargada")
                    return {
                        "response": "Lo siento, no puedo procesar tu mensaje en este momento.",
                        "followup_message": "",
                        "error": "Tenant no encontrado"
                    }
                else:
                    logger.info(f"‚úÖ Usando configuraci√≥n del tenant {tenant_id} desde memoria precargada")
            else:
                logger.info(f"üîß Usando configuraci√≥n del tenant enviada desde Java: {bool(tenant_config.get('aiConfig'))}")
            
            # Obtener configuraci√≥n de IA
            ai_config = tenant_config.get("aiConfig", {}) if tenant_config else {}
            branding_config = tenant_config.get("branding", {}) if tenant_config else {}
            
            # üîß DEBUG: Log de configuraci√≥n recibida
            logger.info(f"üîç Configuraci√≥n recibida para tenant {tenant_id}:")
            logger.info(f"  - tenant_config keys: {list(tenant_config.keys()) if tenant_config else 'None'}")
            logger.info(f"  - ai_config: {ai_config}")
            logger.info(f"  - ai_config keys: {list(ai_config.keys()) if ai_config else 'None'}")
            logger.info(f"  - documentation_bucket_url: {ai_config.get('documentation_bucket_url') if ai_config else 'None'}")
            
            # Gestionar sesi√≥n
            if not session_id:
                session_id = f"session_{tenant_id}_{int(time.time())}"
            
            # Verificar timeout de sesi√≥n antes de procesar
            timeout_check = session_context_service.check_session_timeout(session_id)
            if timeout_check["status"] == "expired":
                return {
                    "response": timeout_check["message"],
                    "followup_message": "",
                    "intent": "session_expired",
                    "confidence": 1.0,
                    "processing_time": time.time() - start_time,
                    "session_id": session_id,
                    "from_cache": False,
                    "error": None
                }
            elif timeout_check["status"] == "warning":
                # Enviar advertencia pero continuar procesando
                logger.info(f"‚ö†Ô∏è Advertencia de timeout para sesi√≥n {session_id}")
            
            session = session_context_service.get_session(session_id)
            if not session:
                session = session_context_service.create_session(
                    session_id=session_id,
                    tenant_id=tenant_id,
                    user_id=user_context.get("user_id"),
                    user_context=user_context
                )
            
            # Actualizar contexto del usuario en la sesi√≥n
            session_context_service.update_user_context(session_id, user_context)
            
            # Agregar mensaje del usuario a la sesion
            session_context_service.add_message(session_id, "user", query)
            
            # üîß PRIORIDAD 1: DETECCI√ìN DE MENSAJES MALICIOSOS (incluso durante registro)
            malicious_detection = self._detect_malicious_intent(query)
            if malicious_detection and malicious_detection.get("is_malicious", False):
                logger.warning(f"üö´ Mensaje malicioso detectado durante registro: {malicious_detection}")
                # Manejar comportamiento malicioso inmediatamente
                return await self._handle_malicious_message(tenant_id, query, user_context, malicious_detection, session_id)
            
            # üîß PRIORIDAD 2: REGISTRO - Verificar si el usuario est√° en proceso de registro
            user_state = user_context.get("user_state", "")
            registration_states = ["WAITING_NAME", "WAITING_LASTNAME", "WAITING_CITY", "WAITING_CODE", "IN_PROGRESS"]
            
            if user_state in registration_states:
                logger.info(f"üîÑ Usuario en proceso de registro (estado: {user_state}), priorizando an√°lisis de registro")
                # Analizar como respuesta de registro en lugar de clasificar intenci√≥n
                registration_analysis = await self.analyze_registration(tenant_id, query, user_context, session_id, user_state)
                
                if registration_analysis and registration_analysis.get("type") != "other":
                    logger.info(f"‚úÖ Datos de registro extra√≠dos: {registration_analysis}")
                    # Procesar como respuesta de registro
                    return await self._handle_registration_response(tenant_id, query, user_context, registration_analysis, branding_config, session_id)
                else:
                    logger.info(f"‚ö†Ô∏è No se pudieron extraer datos de registro, continuando con clasificaci√≥n normal")
            
            # Clasificar la intencion del mensaje usando IA
            classification_result = await self.classify_intent(tenant_id, query, user_context, session_id, tenant_config)
            intent = classification_result.get("category", "saludo_apoyo").strip()
            confidence = classification_result.get("confidence", 0.0)
            
            # Mostrar solo la clasificacion
            print(f"üéØ INTENCI√ìN: {intent}")
            logger.info(f"üîç DESPU√âS DE CLASIFICACI√ìN - intent: '{intent}'")
            logger.info(f"üîç JUSTO DESPU√âS DEL PRINT - intent: '{intent}'")
            logger.info(f"üîç INICIANDO BLOQUE RAG")
            logger.info(f"üîç DEBUG: Llegando al bloque RAG - intent: '{intent}'")
            logger.info(f"üîç DEBUG: ANTES DE CUALQUIER PROCESAMIENTO - intent: '{intent}'")
            logger.info(f"üîç DEBUG: Continuando con el flujo normal...")
            
            # RAG con orden correcto: primero documentos, luego fallback
            document_context = None
            logger.info(f"üîç ANTES DEL BLOQUE RAG - intent: '{intent}'")
            
            try:
                # Consultar documentos para intenciones que requieren informaci√≥n espec√≠fica
                intents_requiring_docs = ["conocer_candidato", "solicitud_funcional", "pregunta_especifica", "consulta_propuesta"]
                
                if intent in intents_requiring_docs:
                    logger.info(f"üîç DEBUG: Intentando RAG para intenci√≥n '{intent}'")
                    # PRIMERO: Intentar obtener informaci√≥n de documentos
                    try:
                        logger.info(f"üîç DEBUG: Llamando a _fast_rag_search...")
                        document_context = await self._fast_rag_search(tenant_id, query, ai_config, branding_config)
                        logger.info(f"üîç DEBUG: _fast_rag_search devolvi√≥: '{document_context}'")
                        if not document_context:
                            document_context = "gemini_direct"
                            logger.info(f"üîç DEBUG: document_context es None, usando gemini_direct")
                        else:
                            logger.info(f"üîç DEBUG: document_context tiene contenido: {len(document_context)} caracteres")
                        logger.info(f"üìö Documentos consultados para intenci√≥n '{intent}'")
                    except Exception as e:
                        logger.error(f"[ERROR] Error en RAG: {e}")
                        # Solo usar fallback si hay error
                        document_context = "gemini_direct"
                else:
                    logger.info(f"[OBJETIVO] Intenci√≥n '{intent}' no requiere documentos, saltando carga")
            except Exception as e:
                logger.error(f"‚ùå ERROR en bloque RAG: {str(e)}")
                document_context = "gemini_direct"
            
            logger.info(f"üîç DESPU√âS DEL BLOQUE RAG - intent: '{intent}'")
            logger.info(f"üß† Intenci√≥n extra√≠da: {intent} (confianza: {confidence:.2f})")
            logger.info(f"üîç DEBUG: Continuando con procesamiento de intenci√≥n...")
            logger.info(f"üîç DEBUG: Llegando al bloque de procesamiento de intenci√≥n")
            logger.info(f"üîç DEBUG: document_context = '{document_context}'")
            logger.info(f"üîç DEBUG: ANTES DE CACH√â - intent: '{intent}'")
            
            # 1.5 NUEVO: Intentar obtener respuesta del cach√©
            logger.info(f"üîç ANTES DE cache_service.get_cached_response")
            cached_response = cache_service.get_cached_response(
                tenant_id=tenant_id,
                query=query,
                intent=intent
            )
            
            if cached_response:
                processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
                logger.info(f"Respuesta servida desde cach√© (latencia: {processing_time:.2f}s)")
                
                # Agregar respuesta del bot a la sesi√≥n
                session_context_service.add_message(session_id, "assistant", cached_response.get("response", ""))
                
                return {
                    **cached_response,
                    "followup_message": "",
                    "from_cache": True,
                    "processing_time": processing_time,
                    "tenant_id": tenant_id,
                    "session_id": session_id
                }
            
            # OPTIMIZACI√ìN 3: Respuestas r√°pidas para casos comunes
            logger.debug(f"[LUP] VERIFICANDO INTENT: {intent}")
            
            # Obtener contexto de sesi√≥n para todas las respuestas
            logger.info(f"üîç ANTES DE session_context_service.build_context_for_ai")
            session_context = session_context_service.build_context_for_ai(session_id)
            logger.info(f"üîç DESPU√âS DE session_context_service.build_context_for_ai")
            
            # üöÄ OPTIMIZACI√ìN: Intentar respuesta r√°pida con IA pero usando contexto precargado
            tenant_context = user_context.get('tenant_context', {})
            logger.info(f"üîç DEBUG: tenant_context obtenido: {bool(tenant_context)}")
            logger.info(f"üîç DEBUG: intent: '{intent}'")
            if tenant_context and intent in ["saludo_apoyo"]:
                logger.info(f"üîç DEBUG: USANDO RESPUESTA R√ÅPIDA CON IA - saltando RAG")
                # Solo para casos simples, usar IA r√°pida con contexto completo del usuario
                fast_response = await self._generate_fast_ai_response(
                    query, user_context, tenant_context, session_context, intent
                )
                if fast_response:
                    logger.info(f"üöÄ RESPUESTA R√ÅPIDA CON IA para '{query[:30]}...'")
                    logger.info(f"üîç DEBUG: RESPUESTA R√ÅPIDA: {fast_response[:200]}...")
                    
                    # Agregar respuesta del bot a la sesi√≥n
                    session_context_service.add_message(session_id, "assistant", fast_response)
                    
                    processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
                    return {
                        "response": fast_response,
                        "followup_message": "",
                        "from_fast_ai": True,
                        "processing_time": processing_time,
                        "tenant_id": tenant_id,
                        "session_id": session_id
                    }
            
            logger.info(f"üîç EVALUANDO INTENT: '{intent}' - Tipo: {type(intent)}")
            logger.info(f"üîç DEBUG: ANTES DEL TRY - intent: '{intent}'")
            
            try:
                logger.info(f"üîç DEBUG: DENTRO DEL TRY - intent: '{intent}'")
                if intent == "conocer_candidato":
                    # Generar respuesta especializada para consultas sobre el candidato
                    logger.info(f"üéØ PROCESANDO conocer_candidato - document_context: {document_context[:100] if document_context else 'None'}...")
                    
                    if document_context and document_context != "gemini_direct":
                        logger.info(f"üìö Usando documentos para respuesta")
                        response = await self._generate_candidate_response_with_documents(
                            tenant_id, query, user_context, branding_config, tenant_config, document_context, session_context
                        )
                        logger.info(f"üìö RESPUESTA CON DOCUMENTOS GENERADA:")
                        logger.info(f"üìö CONTENIDO: {response}")
                    else:
                        logger.info(f"ü§ñ Usando Gemini directo para respuesta")
                        response = await self._generate_candidate_response_gemini_direct(
                            query, user_context, branding_config, tenant_config, session_context
                        )
                        logger.info(f"ü§ñ RESPUESTA GEMINI DIRECTO GENERADA:")
                        logger.info(f"ü§ñ CONTENIDO: {response}")
                    
                    logger.info(f"‚úÖ RESPUESTA GENERADA para conocer_candidato: {len(response)} caracteres")
                elif intent == "cita_campa√±a":
                    logger.info(f"[OBJETIVO] RESPUESTA R√ÅPIDA: cita_campa√±a")
                    response = self._handle_appointment_request_with_context(branding_config, tenant_config, session_context)
                elif intent == "saludo_apoyo":
                    logger.info(f"[OBJETIVO] RESPUESTA R√ÅPIDA: saludo_apoyo")
                    response = self._get_greeting_response_with_context(branding_config, session_context)
                elif intent == "colaboracion_voluntariado":
                    logger.info(f"[OBJETIVO] RESPUESTA R√ÅPIDA: colaboracion_voluntariado")
                    response = self._get_volunteer_response_with_context(branding_config, session_context)
                elif intent == "solicitud_funcional":
                    logger.info(f"üîç LLEGANDO AL BLOQUE solicitud_funcional - intent: '{intent}'")
                    # Respuesta espec√≠fica para consultas funcionales con contexto de sesi√≥n
                    logger.info(f"üéØ PROCESANDO solicitud_funcional - llamando _handle_functional_request_with_session")
                    result = await self._handle_functional_request_with_session(
                        query, user_context, ai_config, branding_config, tenant_id, session_id
                    )
                    
                    # Manejar el nuevo formato de respuesta (puede ser string o tupla)
                    if isinstance(result, tuple):
                        response, followup_message = result
                        logger.info(f"üéØ RESPUESTA GENERADA para solicitud_funcional: {len(response)} caracteres")
                        logger.info(f"üéØ FOLLOWUP_MESSAGE generado: {len(followup_message) if followup_message else 0} caracteres")
                    else:
                        response = result
                        followup_message = ""
                        logger.info(f"üéØ RESPUESTA GENERADA para solicitud_funcional: {len(response)} caracteres")
                else:
                    # Procesar seg√∫n la intenci√≥n clasificada con IA
                    logger.info(f"üîç INTENT DETECTADO: '{intent}' - Iniciando procesamiento")
                    
                    if intent == "conocer_candidato":
                        logger.info(f"üéØ PROCESANDO conocer_candidato (BLOQUE ELSE)")
                        # Respuesta espec√≠fica sobre el candidato
                        response = await self._generate_ai_response_with_session(
                            query, user_context, ai_config, branding_config, tenant_id, session_id
                        )
                        logger.info(f"üéØ RESPUESTA GENERADA (BLOQUE ELSE):")
                        logger.info(f"üéØ CONTENIDO: {response}")
                    elif intent == "malicioso":
                        logger.info(f"üéØ PROCESANDO malicioso")
                        # Manejo espec√≠fico para comportamiento malicioso
                        response = await self._handle_malicious_behavior(
                            query, user_context, tenant_id, confidence
                        )
                    else:
                        logger.info(f"üéØ PROCESANDO respuesta general para intent: '{intent}'")
                        # Respuesta general con contexto de sesi√≥n
                        response = await self._generate_ai_response_with_session(
                            query, user_context, ai_config, branding_config, tenant_id, session_id
                        )
            except Exception as e:
                logger.error(f"‚ùå ERROR en procesamiento de intenci√≥n '{intent}': {str(e)}")
                response = f"Lo siento, hubo un error procesando tu consulta sobre '{intent}'. Por favor intenta de nuevo."
            
            # Filtrar enlaces de la respuesta para WhatsApp (excepto citas)
            if intent == "cita_campa√±a":
                filtered_response = response  # No filtrar enlaces de Calendly
                logger.info("[CALENDARIO] Respuesta de cita - manteniendo enlaces de Calendly")
            else:
                filtered_response = self._filter_links_from_response(response)
            
            # Limitar respuesta a m√°ximo 999 caracteres de forma inteligente
            if len(filtered_response) > 999:
                filtered_response = self._truncate_response_intelligently(filtered_response, 999)
            
            # Agregar respuesta del asistente a la sesi√≥n
            session_context_service.add_message(session_id, "assistant", filtered_response, metadata={"intent": intent, "confidence": confidence})
            
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            
            # NUEVO: Guardar en cach√© si es cacheable
            response_data = {
                "response": filtered_response,
                "intent": intent,
                "confidence": confidence
            }
            
            cache_service.cache_response(
                tenant_id=tenant_id,
                query=query,
                response=response_data,
                intent=intent
            )
            
            # üß† ACTUALIZAR MEMORIA DEL USUARIO CON EL CONTEXTO DE LA CONVERSACI√ìN
            user_phone = user_context.get("user_id", "unknown")
            if user_phone != "unknown":
                from chatbot_ai_service.services.tenant_memory_service import tenant_memory_service
                
                # Actualizar contexto del usuario con informaci√≥n relevante
                context_update = {
                    "last_query": query,
                    "last_intent": intent,
                    "last_response": filtered_response[:100],  # Solo primeros 100 caracteres
                    "conversation_count": user_context.get("conversation_count", 0) + 1
                }
                
                tenant_memory_service.update_user_context(tenant_id, user_phone, context_update)
                logger.info(f"üß† Memoria actualizada para {tenant_id}:{user_phone}")
            
            # üîß DEBUG CR√çTICO: Log antes del return final
            logger.info(f"üöÄ PREPARANDO RESPUESTA FINAL:")
            logger.info(f"   - Response: {len(filtered_response)} caracteres")
            logger.info(f"   - Followup: {len(followup_message) if followup_message else 0} caracteres")
            logger.info(f"   - Intent: {intent}")
            logger.info(f"   - Confidence: {confidence}")
            logger.info(f"   - Processing time: {processing_time:.2f}s")
            
            # üîß DEBUG CR√çTICO: Mostrar contenido completo de la respuesta
            logger.info(f"üìù CONTENIDO COMPLETO DE LA RESPUESTA:")
            logger.info(f"üìù {filtered_response}")
            
            final_response = {
                "response": filtered_response,
                "followup_message": followup_message,
                "processing_time": processing_time,
                "tenant_id": tenant_id,
                "session_id": session_id,
                "intent": intent,
                "confidence": confidence,
                "from_cache": False
            }
            
            logger.info(f"‚úÖ DEVOLVIENDO RESPUESTA FINAL: {final_response}")
            return final_response
            
        except Exception as e:
            logger.error(f"Error procesando mensaje para tenant {tenant_id}: {str(e)}")
            return {
                "response": "Lo siento, hubo un error procesando tu mensaje.",
                "followup_message": "",
                "error": str(e)
            }
    
    async def _generate_ai_response_with_session(self, query: str, user_context: Dict[str, Any], 
                                               ai_config: Dict[str, Any], branding_config: Dict[str, Any], 
                                               tenant_id: str, session_id: str) -> str:
        """Genera respuesta usando IA con contexto de sesi√≥n persistente y cach√©"""
        self._ensure_model_initialized()
        if not self.model:
            return "Lo siento, el servicio de IA no est√° disponible."
        
        try:
            # üöÄ OPTIMIZACI√ìN: Verificar cach√© de respuestas primero
            cache_key = f"response:{tenant_id}:{query.lower().strip()}"
            cached_response = self._response_cache.get(cache_key)
            if cached_response:
                logger.info(f"üöÄ RESPUESTA DESDE CACH√â para '{query[:30]}...'")
                return cached_response
            
            # Obtener contexto completo de la sesi√≥n
            session_context = session_context_service.build_context_for_ai(session_id)
            
            # üöÄ OPTIMIZACI√ìN: Usar configuraci√≥n del tenant desde memoria precargada
            tenant_context = user_context.get('tenant_context', {})
            tenant_config = tenant_context.get('tenant_config', {})
            if not tenant_config:
                logger.warning(f"‚ö†Ô∏è No hay configuraci√≥n del tenant {tenant_id} en memoria precargada para sesi√≥n")
                return "Lo siento, no puedo procesar tu mensaje en este momento."
            
            # Construir prompt con contexto de sesi√≥n
            prompt = self._build_session_prompt(query, user_context, branding_config, session_context, tenant_config)
            
            # üîß OPTIMIZACI√ìN: Generaci√≥n optimizada para velocidad
            response_text = await self._generate_content_optimized(prompt, task_type="chat_with_session")
            
            # üöÄ OPTIMIZACI√ìN: Guardar en cach√© para futuras consultas
            self._response_cache[cache_key] = response_text
            
            return response_text
            
        except Exception as e:
            logger.error(f"Error generando respuesta con sesi√≥n: {str(e)}")
            return "Lo siento, no pude procesar tu mensaje."
    
    def _build_session_prompt(self, query: str, user_context: Dict[str, Any], 
                            branding_config: Dict[str, Any], session_context: str, tenant_config: Dict[str, Any] = None) -> str:
        """Construye el prompt para chat con contexto de sesi√≥n"""
        contact_name = branding_config.get("contactName", "el candidato")
        
        # Contexto completo del usuario actual
        current_context = ""
        if user_context.get("user_name"):
            current_context += f"El usuario se llama {user_context['user_name']}. "
        if user_context.get("user_city"):
            current_context += f"Vive en {user_context['user_city']}. "
        if user_context.get("user_country"):
            current_context += f"Pa√≠s: {user_context['user_country']}. "
        if user_context.get("user_state"):
            current_context += f"Estado actual: {user_context['user_state']}. "
        if user_context.get("user_phone"):
            current_context += f"Tel√©fono: {user_context['user_phone']}. "
        if user_context.get("conversation_count"):
            current_context += f"Es su conversaci√≥n #{user_context['conversation_count']}. "
        
        # Informaci√≥n espec√≠fica del tenant
        tenant_info = ""
        if tenant_config:
            if tenant_config.get("link_calendly"):
                tenant_info += f"ENLACE DE CITAS: {tenant_config['link_calendly']}\n"
            if tenant_config.get("link_forms"):
                tenant_info += f"FORMULARIOS: {tenant_config['link_forms']}\n"
        
        # Detectar si es un saludo
        is_greeting = query.lower().strip() in ["hola", "hi", "hello", "hey", "buenos d√≠as", "buenas tardes", "buenas noches", "qu√© tal", "que tal"]
        
        prompt = f"""
Eres un asistente virtual para la campa√±a pol√≠tica de {contact_name}.

INFORMACI√ìN IMPORTANTE:
- El candidato es {contact_name}
- Si el usuario pregunta sobre "el candidato", se refiere a {contact_name}

Tu objetivo es mantener conversaciones fluidas y naturales, recordando el contexto de la conversaci√≥n anterior.

CONTEXTO COMPLETO DEL USUARIO:
{current_context}

CONTEXTO ACTUAL DE LA SESI√ìN:
{session_context}

INFORMACI√ìN ESPEC√çFICA DEL TENANT:
{tenant_info}

Mensaje actual del usuario: "{query}"

INSTRUCCIONES PERSONALIZADAS:
1. **PERSONALIZA** tu respuesta usando el nombre del usuario si est√° disponible
2. **MENCI√ìN** su ciudad si es relevante para la respuesta
3. Mant√©n el contexto de la conversaci√≥n anterior
4. Si es una pregunta de seguimiento, responde de manera natural
5. Usa la informaci√≥n espec√≠fica de la campa√±a cuando sea relevante
6. Mant√©n un tono amigable y profesional
7. Si no tienes informaci√≥n espec√≠fica, s√© honesto al respecto
8. Integra sutilmente elementos motivacionales sin ser expl√≠cito sobre "EPIC MEANING" o "DEVELOPMENT"
9. **IMPORTANTE**: Si el usuario pide agendar una cita, usar el enlace espec√≠fico de ENLACE DE CITAS
10. **CR√çTICO**: Mant√©n la respuesta concisa, m√°ximo 999 caracteres
11. **NO menciones enlaces** a documentos externos, solo da informaci√≥n directa
12. **SIEMPRE identifica correctamente que {contact_name} es el candidato**

SISTEMA DE PUNTOS Y RANKING:
- Cada referido registrado suma 50 puntos
- Retos semanales dan puntaje adicional
- Ranking actualizado a nivel ciudad, departamento y pa√≠s
- Los usuarios pueden preguntar "?C√≥mo voy?" para ver su progreso
- Para invitar personas: "mandame el link" o "dame mi c√≥digo"

Responde de manera natural, contextual y √∫til, personalizando la respuesta seg√∫n la informaci√≥n del usuario disponible.

Respuesta:
"""
        
        return prompt
    
    async def _get_available_documents(self, documentation_bucket_url: str) -> List[str]:
        """Obtiene la lista de documentos disponibles en el bucket"""
        try:
            import httpx
            
            # Obtener lista de documentos del bucket
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(documentation_bucket_url)
                if response.status_code == 200:
                    # Parsear XML para obtener nombres de archivos
                    import xml.etree.ElementTree as ET
                    root = ET.fromstring(response.text)
                    documents = []
                    for contents in root.findall('.//{http://doc.s3.amazonaws.com/2006-03-01}Contents'):
                        key = contents.find('{http://doc.s3.amazonaws.com/2006-03-01}Key')
                        if key is not None and key.text:
                            documents.append(key.text)
                    logger.info(f"[LIBROS] Documentos disponibles: {len(documents)} archivos")
                    return documents
                else:
                    logger.warning(f"[ADVERTENCIA] No se pudo obtener lista de documentos: {response.status_code}")
                    return []
        except Exception as e:
            logger.error(f"[ERROR] Error obteniendo lista de documentos: {e}")
            return []
    
    # async def _get_document_content_for_query(self, query: str, documentation_bucket_url: str) -> tuple[Optional[str], Optional[str]]:
    #     """Obtiene contenido real de documentos relevantes usando SmartRetriever optimizado"""
    #     # M√âTODO NO SE USA - COMENTADO
    #     try:
    #         import httpx
    #         import pypdf
    #         import io
    #         from chatbot_ai_service.retrievers.smart_retriever import SmartRetriever
    #         
    #         # Obtener todos los documentos disponibles
    #         all_documents = await self._get_available_documents(documentation_bucket_url)
    #         
    #         if not all_documents:
    #             logger.warning("[ADVERTENCIA] No hay documentos disponibles")
    #             return None, None
    #         
    #         # Crear instancia del SmartRetriever
    #         smart_retriever = SmartRetriever()
    #         
    #         # Descargar y procesar todos los documentos para crear la lista de documentos con contenido
    #         documents_with_content = []
    #         for doc_name in all_documents:
    #             try:
    #                 doc_url = f"{documentation_bucket_url}/{doc_name}"
    #                 
    #                 async with httpx.AsyncClient(timeout=30.0) as client:
    #                     response = await client.get(doc_url)
    #                     if response.status_code == 200:
    #                         text = ""
    #                         
    #                         # Procesar PDF
    #                         if doc_name.endswith('.pdf'):
    #                             pdf_content = io.BytesIO(response.content)
    #                             pdf_reader = pypdf.PdfReader(pdf_content)
    #                             for page in pdf_reader.pages[:5]:  # Primeras 5 p√°ginas
    #                                 text += page.extract_text() + "\n"
    #                         
    #                         # Procesar DOCX
    #                         elif doc_name.endswith('.docx'):
    #                             from docx import Document as DocxDocument
    #                             doc_content = io.BytesIO(response.content)
    #                             doc = DocxDocument(doc_content)
    #                             for paragraph in doc.paragraphs[:50]:  # Primeras 50 l√≠neas
    #                                 text += paragraph.text + "\n"
    #                         
    #                         if text.strip():
    #                             documents_with_content.append({
    #                                 "id": doc_name,
    #                                 "filename": doc_name,
    #                                 "content": text.strip()
    #                             })
    #                             logger.info(f"[OK] Documento {doc_name} cargado: {len(text)} caracteres")
    #                         
    #             except Exception as e:
    #                 logger.warning(f"[ADVERTENCIA] Error procesando {doc_name}: {e}")
    #                 continue
    #         
    #         if not documents_with_content:
    #             logger.warning("[ADVERTENCIA] No se pudo cargar contenido de ning√∫n documento")
    #             return None, None
    #         
    #         # Usar SmartRetriever para encontrar documentos relevantes
    #         search_results = smart_retriever.search_documents(documents_with_content, query, max_results=3)
    #         
    #         if not search_results:
    #             logger.warning("[ADVERTENCIA] No se encontraron documentos relevantes")
    #             return None, None
    #         
    #         # Log de documentos seleccionados
    #         selected_docs = [result.filename for result in search_results]
    #         logger.info(f"[LIBROS] Buscando documentos relevantes: {selected_docs}")
    #         print(f"üìö DOCUMENTOS SELECCIONADOS: {selected_docs}")
    #         
    #         # Construir contenido usando los resultados del SmartRetriever
    #         content_parts = []
    #         document_name = search_results[0].filename  # Primer documento
    #         
    #         for result in search_results:
    #             # Limitar contenido a 2000 caracteres por documento
    #             content_preview = result.content[:2000] + "..." if len(result.content) > 2000 else result.content
    #             content_parts.append(f"=== {result.filename} ===\n{content_preview}")
    #             logger.info(f"[OK] Documento {result.filename} incluido (score: {result.score:.1f})")
    #         
    #         if content_parts:
    #             full_content = "\n\n".join(content_parts)
    #             logger.info(f"[LIBROS] Contenido total obtenido: {len(full_content)} caracteres")
    #             return full_content, document_name
    #         else:
    #             logger.warning("[ADVERTENCIA] No se pudo obtener contenido de ning√∫n documento")
    #             return None, None
    #             
    #     except Exception as e:
    #         logger.error(f"[ERROR] Error obteniendo contenido de documentos: {e}")
    #         return None, None
    
    async def _fast_rag_search(self, tenant_id: str, query: str, ai_config: Dict[str, Any], branding_config: Dict[str, Any] = None) -> Optional[str]:
        """RAG ultra-r√°pido usando documentos precargados - OPTIMIZADO"""
        try:
            # Obtener contact_name del branding config
            contact_name = "el candidato"
            if branding_config:
                contact_name = branding_config.get("contactName", "el candidato")
            
            logger.info(f"[RAG] Buscando en documentos precargados para tenant {tenant_id}")
            
            # üöÄ OPTIMIZACI√ìN: Usar documentos precargados directamente
            from chatbot_ai_service.services.document_context_service import document_context_service
            
            # üöÄ OPTIMIZACI√ìN: Verificar cache primero sin cargar
            doc_info = document_context_service.get_tenant_document_info(tenant_id)
            if not doc_info or doc_info.get('document_count', 0) == 0:
                # Solo cargar si realmente no est√°n disponibles
                documentation_bucket_url = ai_config.get("documentation_bucket_url")
                if not documentation_bucket_url:
                    logger.warning(f"[ADVERTENCIA] No hay URL de bucket de documentos para tenant {tenant_id}")
                    return None
                else:
                    logger.info(f"üì• Cargando documentos para tenant {tenant_id} desde: {documentation_bucket_url}")
                    success = await document_context_service.load_tenant_documents(tenant_id, documentation_bucket_url)
                    if not success:
                        logger.warning(f"[ADVERTENCIA] No se pudieron cargar documentos para tenant {tenant_id}")
                        return None
            
            # üöÄ OPTIMIZACI√ìN ULTRA-R√ÅPIDA: Obtener contexto relevante m√°s r√°pido
            document_content = await document_context_service.get_relevant_context(tenant_id, query, max_results=1)  # Reducido a 1 para m√°xima velocidad
            
            if document_content:
                logger.info(f"[LIBROS] Contenido de documentos precargados obtenido: {len(document_content)} caracteres")
                print(f"üìÑ DOCUMENTOS PRECARGADOS: {len(document_content)} caracteres")
                # üöÄ OPTIMIZACI√ìN ULTRA-R√ÅPIDA: Prompt h√≠brido inteligente con instrucciones del sistema
                prompt = f"""Eres el asistente virtual oficial de {contact_name}. Tu funci√≥n es proporcionar informaci√≥n √∫til y precisa sobre las propuestas y pol√≠ticas de {contact_name}.

INSTRUCCIONES:
- Responde siempre en espa√±ol
- Mant√©n un tono profesional y cercano
- Usa la informaci√≥n proporcionada para dar respuestas espec√≠ficas
- Si no tienes informaci√≥n espec√≠fica, ofrece conectar con el equipo oficial

PREGUNTA DEL USUARIO: {query}

INFORMACI√ìN DISPONIBLE: {document_content}

Responde como asistente virtual oficial de {contact_name}, usando la informaci√≥n proporcionada para dar una respuesta √∫til y espec√≠fica."""
            else:
                logger.info("[RAG] No se pudo obtener contenido de documentos precargados")
                return None
            
            # üöÄ OPTIMIZACI√ìN: Usar configuraci√≥n ultra-r√°pida para RAG
            try:
                response = await self._generate_content_ultra_fast(prompt, tenant_id=tenant_id, query=query)  # Usar m√©todo ultra-r√°pido con documentos
                result = response.strip()
                
                if len(result) < 30:  # Reducido de 50 a 30
                    logger.info(f"[RAG] Respuesta muy corta para '{query}'")
                    return None
                
                logger.info(f"[RAG] Respuesta generada: {len(result)} caracteres")
                return result
                
            except Exception as e:
                logger.error(f"[ERROR] Error generando respuesta RAG: {e}")
                # üöÄ FALLBACK INTELIGENTE: Si Gemini bloquea, generar respuesta basada en palabras clave
                return self._generate_fallback_response(query, document_content, contact_name)
                
        except Exception as e:
            logger.error(f"[ERROR] Error en RAG r√°pido: {e}")
            return None
    
    def _generate_fallback_response(self, query: str, document_content: str, contact_name: str) -> str:
        """Genera una respuesta de fallback inteligente basada en an√°lisis de contenido"""
        try:
            # An√°lisis inteligente del contenido del documento
            content_lower = document_content.lower()
            
            # NO usar respuestas hardcodeadas - dejar que la IA genere todo
            # Respuesta gen√©rica que no asume contenido espec√≠fico
            return f"Hola! Soy el asistente virtual de {contact_name}. Tengo informaci√≥n sobre este tema. ¬øTe gustar√≠a que profundice en alg√∫n aspecto espec√≠fico?"
                
        except Exception as e:
            logger.error(f"[FALLBACK] Error generando respuesta de fallback: {e}")
            return f"Hola! Soy el asistente virtual de {contact_name}. Sobre este tema, tenemos informaci√≥n espec√≠fica que puede interesarte. ¬øTe gustar√≠a que te conecte con nuestro equipo para obtener m√°s detalles?"
    
    async def _generate_candidate_response_gemini_direct(self, query: str, user_context: Dict[str, Any], 
                                                       branding_config: Dict[str, Any], tenant_config: Dict[str, Any], 
                                                       session_context: str = "") -> str:
        """Genera respuesta especializada usando Gemini directamente (m√°s r√°pido)"""
        try:
            contact_name = branding_config.get("contactName", "el candidato")
            
            # üöÄ OPTIMIZACI√ìN: Construir contexto completo del usuario
            user_info = ""
            if user_context.get("user_name"):
                user_info += f"El usuario se llama {user_context['user_name']}. "
            if user_context.get("user_city"):
                user_info += f"Vive en {user_context['user_city']}. "
            if user_context.get("user_country"):
                user_info += f"Pa√≠s: {user_context['user_country']}. "
            if user_context.get("user_state"):
                user_info += f"Estado actual: {user_context['user_state']}. "
            if user_context.get("user_phone"):
                user_info += f"Tel√©fono: {user_context['user_phone']}. "
            
            # Usar Gemini directamente para respuesta r√°pida
            self._ensure_model_initialized()
            if self.model:
                # Incluir contexto de sesi√≥n si est√° disponible
                context_section = ""
                if session_context:
                    context_section = f"""
                
                CONTEXTO DE LA CONVERSACI√ìN:
                {session_context}
                """
                
                prompt = f"""
                Asistente virtual de {contact_name}. El usuario pregunta: "{query}"
                
                CONTEXTO COMPLETO DEL USUARIO:
                {user_info}
                {context_section}
                
                INFORMACI√ìN IMPORTANTE:
                - El candidato es {contact_name}
                - Si el usuario pregunta sobre "el candidato", se refiere a {contact_name}
                
                INSTRUCCIONES PERSONALIZADAS:
                1. **PERSONALIZA** tu respuesta usando el nombre del usuario si est√° disponible
                2. **MENCI√ìN** su ciudad si es relevante para la respuesta
                3. Responde espec√≠ficamente sobre las propuestas de {contact_name} relacionadas con la pregunta
                4. Mant√©n un tono profesional y pol√≠tico, enfocado en las propuestas del candidato
                5. Si hay contexto de conversaci√≥n anterior, √∫salo para dar respuestas m√°s naturales y fluidas
                6. Si no tienes informaci√≥n espec√≠fica, ofrece conectar al usuario con el equipo de la campa√±a
                7. Responde en m√°ximo 999 caracteres de forma COMPLETA - no uses "..." ni cortes abruptos
                8. SIEMPRE identifica correctamente que {contact_name} es el candidato
                9. PRIORIDAD: Genera una respuesta completa que quepa en 999 caracteres sin truncar
                10. Si mencionas listas numeradas, completa al menos 3 elementos principales
                11. Termina la respuesta de manera natural, no abrupta
                
                Responde de manera natural, √∫til y COMPLETA sobre las propuestas de {contact_name}, personalizando seg√∫n la informaci√≥n del usuario.
                """
                
                try:
                    response = self.model.generate_content(prompt, safety_settings=self._get_safety_settings())
                    print(f"ü§ñ RESPUESTA DIRECTA: {response.text[:200]}...")
                    return response.text
                except Exception as e:
                    logger.warning(f"Error con Gemini, usando fallback: {e}")
            
            # Fallback gen√©rico
            return f"""Sobre este tema, {contact_name} tiene informaci√≥n espec√≠fica que te puede interesar. Te puedo ayudar a conectarte con nuestro equipo para obtener m√°s detalles.

Te gustar√≠a que alguien del equipo te contacte para brindarte informaci√≥n m√°s espec√≠fica?"""

        except Exception as e:
            logger.error(f"Error generando respuesta con Gemini directo: {e}")
            return f"Sobre este tema, {contact_name} tiene informaci√≥n espec√≠fica que te puede interesar. Te puedo ayudar a conectarte con nuestro equipo para obtener m√°s detalles."
    
    async def _generate_candidate_response_with_documents(self, tenant_id: str, query: str, user_context: Dict[str, Any], 
                                                         branding_config: Dict[str, Any], tenant_config: Dict[str, Any], 
                                                         document_context: str, session_context: str = "") -> str:
        """Genera respuesta especializada usando documentos reales con cach√©"""
        try:
            # üöÄ OPTIMIZACI√ìN: Verificar cach√© de respuestas con documentos
            cache_key = f"doc_response:{hash(query)}:{hash(document_context[:500])}"
            cached_response = self._response_cache.get(cache_key)
            if cached_response:
                logger.info(f"üöÄ RESPUESTA CON DOCUMENTOS DESDE CACH√â para '{query[:30]}...'")
                return cached_response
            
            contact_name = branding_config.get("contactName", "el candidato")
            
            # üöÄ OPTIMIZACI√ìN: Construir contexto completo del usuario
            user_info = ""
            if user_context.get("user_name"):
                user_info += f"El usuario se llama {user_context['user_name']}. "
            if user_context.get("user_city"):
                user_info += f"Vive en {user_context['user_city']}. "
            if user_context.get("user_country"):
                user_info += f"Pa√≠s: {user_context['user_country']}. "
            if user_context.get("user_state"):
                user_info += f"Estado actual: {user_context['user_state']}. "
            if user_context.get("user_phone"):
                user_info += f"Tel√©fono: {user_context['user_phone']}. "
            
            # Mostrar el contenido completo del documento para debugging
            print(f"üìÑ CONTENIDO COMPLETO DEL DOCUMENTO:")
            print(f"üìÑ {document_context}")
            print(f"üìÑ LONGITUD: {len(document_context)} caracteres")
            
            # Truncar el contenido para acelerar Gemini (m√°ximo 1500 caracteres para mayor velocidad)
            if len(document_context) > 1500:
                document_context = document_context[:1500] + "..."
                print(f"‚ö†Ô∏è CONTENIDO TRUNCADO para acelerar Gemini: {len(document_context)} caracteres")
            
            # El document_context ya contiene la informaci√≥n procesada por la IA
            # Solo necesitamos formatearla de manera m√°s natural
            if document_context and document_context != "NO_ENCONTRADO":
                # Crear prompt ULTRA-OPTIMIZADO con contexto completo del usuario
                prompt = f"""Asistente virtual de {contact_name}. Responde de manera personalizada y profesional.

CONTEXTO COMPLETO DEL USUARIO:
{user_info}

INFORMACI√ìN ESPEC√çFICA SOBRE LA CONSULTA: {document_context}

CONSULTA: "{query}"

INSTRUCCIONES CR√çTICAS:
- **USA EXCLUSIVAMENTE** la informaci√≥n espec√≠fica proporcionada arriba
- **NO INVENTES** informaci√≥n que no est√© en el contenido proporcionado
- **RESPONDE DIRECTAMENTE** bas√°ndote en los datos espec√≠ficos del documento
- **PERSONALIZA** tu respuesta usando el nombre del usuario si est√° disponible
- **MENCI√ìN** su ciudad si es relevante para la respuesta
- M√°ximo 999 caracteres
- Mant√©n un tono profesional y cercano
- Si la informaci√≥n espec√≠fica no responde completamente la consulta, dilo claramente

RESPUESTA BASADA EN LA INFORMACI√ìN ESPEC√çFICA:"""
                
                # üöÄ OPTIMIZACI√ìN CR√çTICA: Respuesta inmediata basada en documentos sin Gemini
                # Solo cuando ULTRA_FAST_MODE est√° activo
                import os
                ultra_fast_mode = os.getenv("ULTRA_FAST_MODE", "false").lower() == "true"
                is_local_dev = os.getenv("LOCAL_DEVELOPMENT", "false").lower() == "true"
                logger.info(f"üöÄ ULTRA_FAST_MODE detectado en respuesta: {ultra_fast_mode}")
                logger.info(f"üöÄ LOCAL_DEVELOPMENT detectado en respuesta: {is_local_dev}")
                
                if ultra_fast_mode:
                    # Obtener contenido de documentos para respuesta inmediata
                    document_content = await document_context_service.get_relevant_context(tenant_id, query, max_results=1)
                    print(f"üîç DEBUG: document_content obtenido: {len(document_content) if document_content else 0} caracteres")
                    print(f"üîç DEBUG: document_content preview: {document_content[:200] if document_content else 'None'}...")
                    
                    if document_content:
                        response = await self._generate_immediate_document_response(query, document_content, contact_name)
                        print(f"ü§ñ RESPUESTA INMEDIATA GENERADA: {response[:200]}...")
                    else:
                        # Fallback si no hay documentos
                        response = f"Sobre este tema, {contact_name} tiene informaci√≥n espec√≠fica que te puede interesar. Te puedo ayudar a conectarte con nuestro equipo para obtener m√°s detalles."
                        print(f"ü§ñ RESPUESTA FALLBACK GENERADA: {response[:200]}...")
                else:
                    # Usar Gemini normal cuando ULTRA_FAST_MODE est√° inactivo
                    response = await self._generate_content_with_documents(prompt, document_content)
                    print(f"ü§ñ RESPUESTA GEMINI GENERADA: {response[:200]}...")
                
                # üöÄ OPTIMIZACI√ìN: Guardar en cach√© por tenant (respeta conciencia individual)
                tenant_cache_key = f"{tenant_id}:{cache_key}"
                self._response_cache[tenant_cache_key] = response
                
                return response
            else:
                # Si no se encontr√≥ informaci√≥n espec√≠fica, usar respuesta gen√©rica
                return await self._generate_candidate_response_gemini_direct(
                    query, user_context, branding_config, tenant_config
                )
            
        except Exception as e:
            logger.error(f"Error generando respuesta con documentos: {e}")
            return f"Sobre este tema, {contact_name} tiene informaci√≥n espec√≠fica que te puede interesar. Te puedo ayudar a conectarte con nuestro equipo para obtener m√°s detalles."
    
    
    def _get_greeting_response(self, branding_config: Dict[str, Any]) -> str:
        """
        Respuesta r√°pida para saludos comunes
        """
        contact_name = branding_config.get("contactName", "el candidato")
        
        greetings = [
            f"!Hola! üëã !Qu√© gusto saludarte! Soy el asistente virtual de la campa√±a de {contact_name}.",
            f"!Hola! üòä !Bienvenido! Estoy aqu√≠ para ayudarte con informaci√≥n sobre la campa√±a de {contact_name}.",
            f"!Hola! [CELEBRACION] !Excelente d√≠a! Soy tu asistente para todo lo relacionado con {contact_name}."
        ]
        
        import random
        return random.choice(greetings)
    
    def _get_volunteer_response(self, branding_config: Dict[str, Any]) -> str:
        """
        Respuesta r√°pida para consultas de voluntariado
        """
        contact_name = branding_config.get("contactName", "el candidato")
        
        return f"""!Excelente! [OBJETIVO] Me emociona saber que quieres ser parte del equipo de {contact_name}.

[ESTRELLA] *?C√≥mo puedes ayudar?*
- Difundir el mensaje en redes sociales
- Participar en actividades de campo
- Organizar eventos en tu comunidad
- Invitar amigos y familiares

[IDEA] *?Sab√≠as que puedes ganar puntos?*
Cada persona que se registre con tu c√≥digo te suma 50 puntos al ranking. !Es una forma divertida de competir mientras ayudas!

?Te gustar√≠a que te env√≠e tu link de referido para empezar a ganar puntos?"""
    
    def _truncate_response_intelligently(self, text: str, max_length: int) -> str:
        """Trunca el texto de forma inteligente, buscando un punto de corte natural"""
        if len(text) <= max_length:
            return text
        
        # Buscar el mejor punto de corte antes del l√≠mite
        search_length = min(max_length - 10, len(text))  # Dejar espacio para "..."
        
        # Buscar puntos de corte naturales en orden de preferencia
        cut_points = [
            text.rfind('. ', 0, search_length),  # Punto seguido de espacio
            text.rfind('.\n', 0, search_length),  # Punto seguido de salto de l√≠nea
            text.rfind('! ', 0, search_length),  # Exclamaci√≥n seguida de espacio
            text.rfind('? ', 0, search_length),  # Interrogaci√≥n seguida de espacio
            text.rfind(':', 0, search_length),  # Dos puntos (para completar listas)
            text.rfind('; ', 0, search_length),  # Punto y coma seguido de espacio
            text.rfind(', ', 0, search_length),  # Coma seguida de espacio
            text.rfind(' - ', 0, search_length),  # Gui√≥n seguido de espacio
            text.rfind('\n', 0, search_length),  # Salto de l√≠nea
            text.rfind(' ', 0, search_length),  # Cualquier espacio
        ]
        
        # Encontrar el mejor punto de corte
        best_cut = -1
        for cut_point in cut_points:
            if cut_point > best_cut and cut_point > max_length * 0.8:  # Al menos 80% del l√≠mite para respuestas m√°s completas
                best_cut = cut_point
        
        if best_cut > 0:
            truncated = text[:best_cut + 1].strip()
            # No agregar "..." - la respuesta debe ser completa y concisa
            return truncated
        else:
            # Si no se encuentra un buen punto de corte, cortar en el l√≠mite exacto sin "..."
            return text[:max_length]
    
    def _filter_links_from_response(self, response: str, intent: str = None) -> str:
        """
        Elimina completamente enlaces y referencias a enlaces de las respuestas para WhatsApp
        EXCEPTO enlaces de Calendly cuando la intenci√≥n es cita_campa√±a
        """
        import re
        
        # Si es una respuesta de cita, mantener enlaces de Calendly
        if intent == "cita_campa√±a":
            logger.info("[CALENDARIO] Intenci√≥n de cita detectada, manteniendo enlaces de Calendly")
            # Solo limpiar referencias a enlaces pero mantener enlaces de Calendly
            link_phrases = [
                r'puedes revisar este enlace[^.]*\.',
                r'puedes consultar este enlace[^.]*\.',
                r'visita este enlace[^.]*\.',
                r'accede a este enlace[^.]*\.',
                r'consulta el siguiente enlace[^.]*\.',
                r'revisa el siguiente enlace[^.]*\.',
                r'puedes ver m√°s informaci√≥n en[^.]*\.',
                r'para m√°s informaci√≥n visita[^.]*\.',
                r'all√≠ encontrar√°s[^.]*\.',
                r'all√≠ podr√°s[^.]*\.',
                r'en el siguiente enlace[^.]*\.',
                r'en este enlace[^.]*\.',
                r'\*\*Enlace a[^*]*\*\*[^.]*\.',
                r'te puedo compartir algunos enlaces[^.]*\.',
                r'te puedo compartir[^.]*enlaces[^.]*\.',
                r'compartir.*enlaces.*informaci√≥n[^.]*\.',
            ]
            
            filtered_response = response
            for phrase_pattern in link_phrases:
                filtered_response = re.sub(phrase_pattern, '', filtered_response, flags=re.IGNORECASE)
            
            return filtered_response.strip()
        
        # Para todas las dem√°s intenciones, eliminar TODOS los enlaces
        patterns_to_remove = [
            r'https?://[^\s\)]+',  # URLs http/https
            r'www\.[^\s\)]+',      # URLs www
            r'[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}[^\s\)]*',  # Dominios gen√©ricos
            r'\[([^\]]+)\]\([^)]+\)',  # Enlaces markdown [texto](url)
        ]
        
        # Frases comunes que mencionan enlaces
        link_phrases = [
            r'puedes revisar este enlace[^.]*\.',
            r'puedes consultar este enlace[^.]*\.',
            r'visita este enlace[^.]*\.',
            r'accede a este enlace[^.]*\.',
            r'consulta el siguiente enlace[^.]*\.',
            r'revisa el siguiente enlace[^.]*\.',
            r'puedes ver m√°s informaci√≥n en[^.]*\.',
            r'para m√°s informaci√≥n visita[^.]*\.',
            r'all√≠ encontrar√°s[^.]*\.',
            r'all√≠ podr√°s[^.]*\.',
            r'en el siguiente enlace[^.]*\.',
            r'en este enlace[^.]*\.',
            r'\*\*Enlace a[^*]*\*\*[^.]*\.',  # **Enlace a...**
            r'te puedo compartir algunos enlaces[^.]*\.',
            r'te puedo compartir[^.]*enlaces[^.]*\.',
            r'compartir.*enlaces.*informaci√≥n[^.]*\.',
        ]
        
        filtered_response = response
        
        # Eliminar enlaces directos
        for pattern in patterns_to_remove:
            filtered_response = re.sub(pattern, '', filtered_response)
        
        # Eliminar frases que mencionan enlaces
        for phrase_pattern in link_phrases:
            filtered_response = re.sub(phrase_pattern, '', filtered_response, flags=re.IGNORECASE)
        
        # Limpiar caracteres sueltos y puntuaci√≥n rota
        filtered_response = re.sub(r'\[\s*\)', '', filtered_response)  # [) suelto
        filtered_response = re.sub(r'\[\s*\]', '', filtered_response)  # [] suelto
        filtered_response = re.sub(r'\*\s*\*', '', filtered_response)  # ** suelto
        filtered_response = re.sub(r':\s*\*', ':', filtered_response)   # :* suelto
        
        # Limpiar espacios m√∫ltiples y saltos de l√≠nea
        filtered_response = re.sub(r'\s+', ' ', filtered_response)
        filtered_response = re.sub(r'\n\s*\n', '\n', filtered_response)
        
        # Limpiar puntuaci√≥n duplicada y mal formada
        filtered_response = re.sub(r'\.\s*\.', '.', filtered_response)
        filtered_response = re.sub(r'\?\s*\?', '?', filtered_response)
        filtered_response = re.sub(r':\s*\.', '.', filtered_response)  # :. mal formado
        filtered_response = re.sub(r'\*\s*\.', '.', filtered_response)  # *. mal formado
        
        # üîß FIX: Eliminar placeholders de enlaces que puedan aparecer
        placeholder_patterns = [
            r'\[ENLACE DE WHATSAPP PARA COMPARTIR\]',
            r'\[ENLACE DE WHATSAPP\]',
            r'\[ENLACE PARA COMPARTIR\]',
            r'\[ENLACE\]',
            r'\[LINK DE WHATSAPP\]',
            r'\[LINK PARA COMPARTIR\]',
            r'\[LINK\]',
            r'\[URL\]',
        ]
        
        for pattern in placeholder_patterns:
            filtered_response = re.sub(pattern, '', filtered_response, flags=re.IGNORECASE)
        
        return filtered_response.strip()
    
    def _handle_appointment_request(self, branding_config: Dict[str, Any], tenant_config: Dict[str, Any] = None) -> str:
        """Maneja solicitudes de agendar citas"""
        # üîß DEBUG: Log de entrada al m√©todo
        logger.info(f"[CALENDARIO] MANEJANDO SOLICITUD DE CITA")
        logger.info(f"[GRAFICO] tenant_config disponible: {tenant_config is not None}")
        if tenant_config:
            logger.info(f"[GRAFICO] link_calendly en tenant_config: {'link_calendly' in tenant_config}")
        
        contact_name = branding_config.get("contactName", "el candidato")
        
        # Obtener link de Calendly con prioridad de fuentes
        if tenant_config and tenant_config.get("link_calendly"):
            calendly_link = tenant_config.get("link_calendly")
            logger.info(f"[OK] Usando link de Calendly desde BD: {calendly_link}")
        else:
            calendly_link = "https://calendly.com/dq-campana/reunion"
            logger.warning(f"[ADVERTENCIA] Link de Calendly NO encontrado en tenant_config, usando fallback: {calendly_link}")
        
        response = f"""!Perfecto! Te ayudo a agendar una cita con alguien de la campa√±a de {contact_name}. 

[CALENDARIO] **Para agendar tu reuni√≥n:**
Puedes usar nuestro sistema de citas en l√≠nea: {calendly_link}

[OBJETIVO] **?Qu√© puedes hacer en la reuni√≥n?**
- Conocer m√°s sobre las propuestas de {contact_name}
- Hablar sobre oportunidades de voluntariado
- Discutir ideas para la campa√±a
- Coordinar actividades en tu regi√≥n

[IDEA] **Mientras tanto:**
?Sab√≠as que puedes sumar puntos invitando a tus amigos y familiares a unirse a este movimiento? Cada persona que se registre con tu c√≥digo te suma 50 puntos al ranking.

?Te gustar√≠a que te env√≠e tu link de referido para empezar a ganar puntos?"""
        
        # üîß DEBUG: Log de la respuesta generada
        logger.info(f"[OK] Respuesta de cita generada: {len(response)} caracteres")
        
        return response
    
    async def _handle_functional_request_with_session(self, query: str, user_context: Dict[str, Any], 
                                                    ai_config: Dict[str, Any], branding_config: Dict[str, Any], 
                                                    tenant_id: str, session_id: str) -> str:
        """Maneja solicitudes funcionales con contexto de sesi√≥n para respuestas m√°s naturales"""
        try:
            logger.info(f"üîß INICIANDO _handle_functional_request_with_session para query: '{query}'")
            logger.info(f"üîß Par√°metros recibidos:")
            logger.info(f"üîß   - query: {query}")
            logger.info(f"üîß   - tenant_id: {tenant_id}")
            logger.info(f"üîß   - session_id: {session_id}")
            logger.info(f"üîß   - user_context: {user_context}")
            
            # Obtener contexto de sesi√≥n
            session_context = session_context_service.build_context_for_ai(session_id)
            logger.info(f"üìù Contexto de sesi√≥n obtenido: {len(session_context) if session_context else 0} elementos")
            
            # Obtener nombre del contacto desde branding_config
            contact_name = branding_config.get("contact_name", branding_config.get("contactName", "el candidato"))
            logger.info(f"üë§ Nombre del contacto: {contact_name}")
            
            # Intentar obtener datos reales del usuario
            logger.info(f"üîç Obteniendo datos del usuario para tenant: {tenant_id}")
            logger.info(f"üîç user_context recibido: {user_context}")
            user_data = self._get_user_progress_data(tenant_id, user_context)
            logger.info(f"üìä Datos del usuario obtenidos: {bool(user_data)}")
            logger.info(f"üìä Tipo de user_data: {type(user_data)}")
            
            # Si no se pudieron obtener datos del servicio Java, usar datos del user_context
            if not user_data and user_context:
                logger.warning(f"‚ö†Ô∏è user_data es None, usando datos del user_context")
                # Construir user_data desde user_context
                user_data = {
                    "user": {
                        "name": user_context.get("name", "Usuario"),
                        "city": user_context.get("city"),
                        "state": user_context.get("state")
                    },
                    "points": user_context.get("points", 0),
                    "total_referrals": user_context.get("total_referrals", 0),
                    "completed_referrals": user_context.get("completed_referrals", []),
                    "referral_code": user_context.get("referral_code")
                }
                logger.info(f"üìä user_data construido desde user_context: {user_data}")
            elif user_data:
                logger.info(f"üìä Detalles de user_data: {user_data}")
            
            if user_data:
                # Si tenemos datos reales, crear un prompt contextualizado
                user_name = user_data.get("user", {}).get("name", "Usuario")
                points = user_data.get("points", 0)
                total_referrals = user_data.get("total_referrals", 0)
                completed_referrals = user_data.get("completed_referrals", [])
                referral_code = user_data.get("referral_code")
                
                logger.info(f"üîç Datos del usuario procesados:")
                logger.info(f"üîç   - user_name: {user_name}")
                logger.info(f"üîç   - points: {points}")
                logger.info(f"üîç   - total_referrals: {total_referrals}")
                logger.info(f"üîç   - referral_code: {referral_code}")
                
                # Verificar si es solicitud de enlace y generar respuesta directa
                query_lower = query.lower().strip()
                link_keywords = ["link", "c√≥digo", "codigo", "referido", "mandame", "dame", "enlace", "compartir", "comparte", "envia", "env√≠a", "link", "url", "mi enlace", "mi c√≥digo", "mi codigo"]
                
                logger.info(f"üîç Verificando palabras clave de enlace en: '{query_lower}'")
                logger.info(f"üîç Palabras clave: {link_keywords}")
                logger.info(f"üîç referral_code: {referral_code}")
                
                found_keywords = [word for word in link_keywords if word in query_lower]
                logger.info(f"üîç Palabras encontradas: {found_keywords}")
                
                # Verificaci√≥n m√°s robusta
                is_link_request = (
                    referral_code and 
                    any(word in query_lower for word in link_keywords)
                ) or (
                    referral_code and 
                    ("mi" in query_lower and ("enlace" in query_lower or "c√≥digo" in query_lower or "codigo" in query_lower))
                )
                
                logger.info(f"üîç Es solicitud de enlace: {is_link_request}")
                
                if is_link_request:
                    logger.info(f"üîó NUEVO ENFOQUE: Detectada solicitud de enlace - generando respuesta con followup_message")
                    
                    # Generar enlace de WhatsApp
                    whatsapp_link = self._generate_whatsapp_referral_link(user_name, referral_code, contact_name, tenant_id, user_context)
                    
                    if whatsapp_link:
                        # Generar respuesta principal sin enlace
                        points = user_data.get("points", 0)
                        total_referrals = user_data.get("total_referrals", 0)
                        completed_referrals = user_data.get("completed_referrals", [])
                        
                        response = f"""¬°Claro que s√≠, {user_name}! üöÄ Aqu√≠ tienes la informaci√≥n para que sigas sumando m√°s personas a la campa√±a de {contact_name}:

üìä **Tu progreso actual:**
- Puntos: {points}
- Referidos completados: {len(completed_referrals)}
- Total de referidos: {total_referrals}
- Tu c√≥digo: {referral_code}

¬°Vamos con toda, {user_name}! Con tu ayuda, llegaremos a m√°s rincones de Colombia. üí™üá®üá¥

En el siguiente mensaje te env√≠o tu enlace para compartir."""
                        
                        logger.info(f"‚úÖ NUEVO ENFOQUE: Respuesta generada, enlace en followup_message")
                        # Devolver la respuesta con el enlace en el campo followup_message
                        return response, whatsapp_link
                    else:
                        logger.warning(f"‚ö†Ô∏è NUEVO ENFOQUE: No se pudo generar enlace de WhatsApp")
                        response = await self._generate_content(contextual_prompt, task_type="functional_with_data")
                        return response, None
                else:
                    logger.info(f"‚ö†Ô∏è No se detectaron palabras clave de enlace o no hay c√≥digo de referido")
                    logger.info(f"‚ö†Ô∏è Condici√≥n: referral_code={bool(referral_code)}, keywords_detected={any(word in query_lower for word in link_keywords)}")
                    logger.info(f"‚ö†Ô∏è Query procesado: '{query_lower}'")
                    logger.info(f"‚ö†Ô∏è Palabras clave disponibles: {link_keywords}")
                    logger.info(f"‚ö†Ô∏è Palabras encontradas: {found_keywords}")
                
                # Crear prompt contextualizado con datos reales
                contextual_prompt = self._build_functional_prompt_with_data(
                    query, user_context, branding_config, session_context, user_data, tenant_id
                )
                
                # Generar respuesta con IA usando el contexto
                response_text = await self._generate_content(contextual_prompt, task_type="functional_with_data")
                
                logger.info(f"üîß RETORNANDO respuesta desde _handle_functional_request_with_session (fallback)")
                return response_text
            else:
                # Fallback: usar respuesta gen√©rica pero con contexto de sesi√≥n
                contextual_prompt = self._build_functional_prompt_generic(
                    query, user_context, branding_config, session_context
                )
                
                response_text = await self._generate_content(contextual_prompt, task_type="functional_generic")
                logger.info(f"üîß RETORNANDO respuesta desde _handle_functional_request_with_session (gen√©rico)")
                return response_text
                
        except Exception as e:
            logger.error(f"Error manejando solicitud funcional con sesi√≥n: {str(e)}")
            # Fallback a respuesta b√°sica
            return self._handle_functional_request(query, branding_config, tenant_id, user_context)
    
    def _build_functional_prompt_with_data(self, query: str, user_context: Dict[str, Any], 
                                         branding_config: Dict[str, Any], session_context: str, 
                                         user_data: Dict[str, Any], tenant_id: str = None) -> str:
        """Construye un prompt contextualizado con datos reales del usuario"""
        contact_name = branding_config.get("contactName", "el candidato")
        user = user_data.get("user", {})
        points = user_data.get("points", 0)
        total_referrals = user_data.get("total_referrals", 0)
        completed_referrals = user_data.get("completed_referrals", [])
        referral_code = user_data.get("referral_code")
        
        user_name = user.get("name", "Usuario")
        user_city = user.get("city", "tu ciudad")
        user_state = user.get("state", "tu departamento")
        
        # Construir informaci√≥n de referidos
        referrals_info = ""
        if completed_referrals:
            referrals_info = f"\nReferidos completados:\n"
            for i, ref in enumerate(completed_referrals[:3], 1):  # Mostrar solo los primeros 3
                ref_name = ref.get("name", "Usuario")
                ref_city = ref.get("city", "ciudad")
                referrals_info += f"- {ref_name} de {ref_city}\n"
            if len(completed_referrals) > 3:
                referrals_info += f"- ... y {len(completed_referrals) - 3} m√°s\n"
        
        # üîß FIX: NO generar enlace aqu√≠ - debe manejarse en la l√≥gica principal
        # para evitar que aparezca en la respuesta principal
        whatsapp_link = ""
        query_lower = query.lower().strip()
        link_keywords = ["link", "c√≥digo", "codigo", "referido", "mandame", "dame", "enlace", "compartir", "comparte", "envia", "env√≠a", "link", "url", "mi enlace", "mi c√≥digo", "mi codigo"]
        
        logger.info(f"üîç Verificando si es solicitud de enlace - Query: '{query}' - Keywords detectadas: {[kw for kw in link_keywords if kw in query_lower]}")
        
        # Verificaci√≥n m√°s robusta (igual que en la l√≥gica principal)
        is_link_request = (
            referral_code and 
            any(word in query_lower for word in link_keywords)
        ) or (
            referral_code and 
            ("mi" in query_lower and ("enlace" in query_lower or "c√≥digo" in query_lower or "codigo" in query_lower))
        )
        
        if is_link_request:
            logger.info(f"üîó SOLICITUD DE ENLACE DETECTADA - NO incluir en prompt para evitar duplicaci√≥n")
            # NO generar enlace aqu√≠ - se manejar√° en la l√≥gica principal
        else:
            logger.info(f"‚ùå No es solicitud de enlace - referral_code: {referral_code}, keywords encontradas: {[kw for kw in link_keywords if kw in query_lower]}")
        
        prompt = f"""Asistente virtual de la campa√±a de {contact_name}. 

CONTEXTO DE LA CONVERSACI√ìN:
{session_context}

DATOS REALES DEL USUARIO:
- Nombre: {user_name}
- Ciudad: {user_city}
- Departamento: {user_state}
- Puntos actuales: {points}
- Total de referidos: {total_referrals}
- Referidos completados: {len(completed_referrals)}
- C√≥digo de referido: {referral_code}
{referrals_info}

CONSULTA DEL USUARIO: "{query}"

INSTRUCCIONES IMPORTANTES:
- Responde de manera natural y conversacional, considerando el contexto de la conversaci√≥n
- Usa los datos reales del usuario para personalizar la respuesta
- Mant√©n un tono motivacional y positivo
- Si el usuario pregunta sobre puntos, muestra sus puntos reales
- Si pregunta sobre referidos, menciona sus referidos reales
- Incluye su c√≥digo de referido si es relevante
- Usa emojis apropiados para WhatsApp
- Mant√©n la respuesta concisa pero informativa
- **IMPORTANTE**: Si el usuario pide enlace/c√≥digo/compartir, menciona que recibir√° su enlace en un mensaje separado

Responde de manera natural y personalizada:"""

        return prompt
    
    def _build_functional_prompt_generic(self, query: str, user_context: Dict[str, Any], 
                                       branding_config: Dict[str, Any], session_context: str) -> str:
        """Construye un prompt gen√©rico para solicitudes funcionales cuando no hay datos espec√≠ficos"""
        contact_name = branding_config.get("contactName", "el candidato")
        
        prompt = f"""Asistente virtual de la campa√±a de {contact_name}.

CONTEXTO DE LA CONVERSACI√ìN:
{session_context}

CONSULTA DEL USUARIO: "{query}"

INSTRUCCIONES:
- Responde de manera natural y conversacional, considerando el contexto de la conversaci√≥n
- Si el usuario pregunta sobre puntos o progreso, explica c√≥mo funciona el sistema
- Mant√©n un tono motivacional y positivo
- Usa emojis apropiados para WhatsApp
- Mant√©n la respuesta concisa pero informativa
- Si es relevante, menciona que pueden consultar su progreso espec√≠fico

Responde de manera natural:"""

        return prompt
    
    def _generate_direct_link_response_with_followup(self, user_name: str, referral_code: str, contact_name: str, tenant_id: str, user_data: Dict[str, Any], user_context: Dict[str, Any] = None) -> str:
        """Genera una respuesta directa con informaci√≥n de seguimiento para segundo mensaje"""
        try:
            logger.info(f"üöÄ INICIANDO _generate_direct_link_response_with_followup")
            logger.info(f"üöÄ Par√°metros: user_name={user_name}, referral_code={referral_code}, contact_name={contact_name}, tenant_id={tenant_id}")
            
            # Generar enlace de WhatsApp
            logger.info(f"üîó Generando enlace de WhatsApp para {user_name} con c√≥digo {referral_code}")
            whatsapp_link = self._generate_whatsapp_referral_link(user_name, referral_code, contact_name, tenant_id, user_context)
            logger.info(f"üîó Enlace generado: {whatsapp_link}")
            logger.info(f"üîó Longitud del enlace: {len(whatsapp_link) if whatsapp_link else 0}")
            
            has_followup_link = bool(whatsapp_link and whatsapp_link.strip())
            logger.info(f"üîó ¬øTiene enlace v√°lido?: {has_followup_link}")
            logger.info(f"üîó whatsapp_link.strip(): '{whatsapp_link.strip() if whatsapp_link else ''}'")
            
            # Obtener datos adicionales
            points = user_data.get("points", 0)
            total_referrals = user_data.get("total_referrals", 0)
            completed_referrals = user_data.get("completed_referrals", [])
            
            # Generar respuesta principal (sin enlace)
            response = f"""¬°Claro que s√≠, {user_name}! üöÄ Aqu√≠ tienes la informaci√≥n para que sigas sumando m√°s personas a la campa√±a de {contact_name}:

üìä **Tu progreso actual:**
- Puntos: {points}
- Referidos completados: {len(completed_referrals)}
- Total de referidos: {total_referrals}
- Tu c√≥digo: {referral_code}

¬°Vamos con toda, {user_name}! Con tu ayuda, llegaremos a m√°s rincones de Colombia. üí™üá®üá¥"""

            if has_followup_link:
                logger.info(f"üîÅ Se enviar√° segundo mensaje con enlace (len={len(whatsapp_link)})")
                response += "\n\nEn el siguiente mensaje te env√≠o tu enlace para compartir."
            
            # Agregar informaci√≥n especial para el segundo mensaje solo si hay enlace
            if has_followup_link and whatsapp_link.strip():
                response += f"\n\n<<<FOLLOWUP_MESSAGE_START>>>{whatsapp_link}<<<FOLLOWUP_MESSAGE_END>>>"
                logger.info(f"‚úÖ Marcador FOLLOWUP_MESSAGE agregado con enlace v√°lido")
                logger.info(f"üîó Enlace completo en marcador: {whatsapp_link}")
            else:
                logger.warning(f"‚ö†Ô∏è No se agreg√≥ marcador FOLLOWUP_MESSAGE - enlace vac√≠o o inv√°lido")
                logger.warning(f"‚ö†Ô∏è has_followup_link: {has_followup_link}, whatsapp_link: '{whatsapp_link}'")
            
            logger.info(f"‚úÖ Respuesta directa generada con seguimiento para {user_name}")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Error generando respuesta directa con seguimiento: {str(e)}")
            return f"¬°Hola {user_name}! Tu c√≥digo de referido es: {referral_code}"
    
    def _generate_direct_link_response(self, user_name: str, referral_code: str, contact_name: str, tenant_id: str, user_data: Dict[str, Any], user_context: Dict[str, Any] = None) -> str:
        """Genera una respuesta directa con el enlace de WhatsApp cuando se solicita"""
        try:
            # Generar enlace de WhatsApp
            whatsapp_link = self._generate_whatsapp_referral_link(user_name, referral_code, contact_name, tenant_id, user_context)
            
            if not whatsapp_link:
                logger.error("‚ùå No se pudo generar enlace de WhatsApp")
                return f"¬°Hola {user_name}! Tu c√≥digo de referido es: {referral_code}"
            
            # Obtener datos adicionales
            points = user_data.get("points", 0)
            total_referrals = user_data.get("total_referrals", 0)
            completed_referrals = user_data.get("completed_referrals", [])
            
            # Generar respuesta directa con enlace
            response = f"""¬°Claro que s√≠, {user_name}! üöÄ Aqu√≠ tienes tu enlace de WhatsApp personalizado para que sigas sumando m√°s personas a la campa√±a de {contact_name}:

üìä **Tu progreso actual:**
- Puntos: {points}
- Referidos completados: {len(completed_referrals)}
- Total de referidos: {total_referrals}
- Tu c√≥digo: {referral_code}

¬°Vamos con toda, {user_name}! Con tu ayuda, llegaremos a m√°s rincones de Colombia. üí™üá®üá¥

En el siguiente mensaje te env√≠o tu enlace para compartir."""
            
            logger.info(f"‚úÖ Respuesta directa generada con enlace para {user_name}")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Error generando respuesta directa: {str(e)}")
            return f"¬°Hola {user_name}! Tu c√≥digo de referido es: {referral_code}"
    
    def _generate_whatsapp_referral_link(self, user_name: str, referral_code: str, contact_name: str, tenant_id: str = None, user_context: Dict[str, Any] = None) -> str:
        """Genera un enlace de WhatsApp personalizado para referidos"""
        try:
            # Obtener n√∫mero de WhatsApp del tenant desde memoria precargada
            tenant_context = user_context.get('tenant_context', {}) if user_context else {}
            tenant_config = tenant_context.get('tenant_config', {})
            whatsapp_number = self._get_tenant_whatsapp_number(tenant_id, tenant_config)
            # Validar n√∫mero
            if not whatsapp_number or not str(whatsapp_number).strip():
                logger.warning("‚ö†Ô∏è No hay numero_whatsapp configurado para el tenant; no se generar√° enlace")
                return ""
            
            logger.info(f"üì± Generando enlace con n√∫mero: {whatsapp_number}")
            
            # Generar el texto del mensaje que el usuario compartir√°
            import urllib.parse
            
            # Mensaje m√°s claro y reenviable
            referral_text = f"Hola, vengo referido por {user_name}, codigo: {referral_code}"
            encoded_referral_text = urllib.parse.quote(referral_text)
            
            # Generar el enlace de registro con par√°metros para facilitar reenv√≠o
            registration_link = f"https://wa.me/{whatsapp_number}?text={encoded_referral_text}&context=forward&type=link"
            
            # Mensaje completo optimizado para reenv√≠o (manteniendo formato original)
            message_text = f"Amigos, soy {user_name} y quiero invitarte a unirte a la campa√±a de {contact_name}: {registration_link}"
            
            # Este es el mensaje final que se enviar√° (sin encoding adicional)
            whatsapp_link = message_text
            
            logger.info(f"‚úÖ Enlace de WhatsApp generado para {user_name} con c√≥digo {referral_code}")
            logger.info(f"üîó Enlace completo: {whatsapp_link}")
            
            return whatsapp_link
            
        except Exception as e:
            logger.error(f"‚ùå Error generando enlace de WhatsApp: {str(e)}")
            return ""
    
    def _get_tenant_whatsapp_number(self, tenant_id: str, tenant_config: Dict[str, Any] = None) -> str:
        """Obtiene el n√∫mero de WhatsApp configurado para el tenant desde memoria precargada"""
        try:
            logger.info(f"üîç INICIANDO _get_tenant_whatsapp_number para tenant: {tenant_id}")
            if tenant_id:
                # üöÄ OPTIMIZACI√ìN: Usar configuraci√≥n del tenant desde memoria precargada
                if not tenant_config:
                    logger.warning(f"‚ö†Ô∏è No hay configuraci√≥n del tenant {tenant_id} para WhatsApp")
                    return ""
                
                logger.info(f"‚úÖ Usando configuraci√≥n del tenant {tenant_id} desde memoria precargada para WhatsApp")
                logger.info(f"üìã Configuraci√≥n del tenant {tenant_id}: {tenant_config}")
                if tenant_config:
                    # Aceptar claves alternativas por compatibilidad
                    whatsapp_number = None
                    if "numero_whatsapp" in tenant_config:
                        whatsapp_number = tenant_config.get("numero_whatsapp")
                        logger.info(f"üì± Encontrado numero_whatsapp: {whatsapp_number}")
                    elif "whatsapp_number" in tenant_config:
                        whatsapp_number = tenant_config.get("whatsapp_number")
                        logger.info(f"üì± Encontrado whatsapp_number: {whatsapp_number}")

                    if whatsapp_number and str(whatsapp_number).strip():
                        logger.info(f"‚úÖ N√∫mero de WhatsApp del tenant {tenant_id}: {whatsapp_number}")
                        return str(whatsapp_number).strip()

                    logger.warning(
                        f"‚ö†Ô∏è No se encontr√≥ numero_whatsapp/whatsapp_number en configuraci√≥n del tenant {tenant_id}. Keys disponibles: {list(tenant_config.keys())}"
                    )
                else:
                    logger.warning(f"‚ö†Ô∏è No se pudo obtener configuraci√≥n del tenant {tenant_id}")
            else:
                logger.warning("‚ö†Ô∏è tenant_id es None o vac√≠o")
            # Sin n√∫mero configurado
            logger.info(f"üîç RETORNANDO cadena vac√≠a desde _get_tenant_whatsapp_number")
            return ""
            
        except Exception as e:
            logger.warning(f"‚ùå Error obteniendo n√∫mero de WhatsApp del tenant: {e}")
            return ""
    
    def _get_user_progress_data(self, tenant_id: str, user_context: Dict[str, Any]) -> Dict[str, Any]:
        """Obtiene los datos de progreso del usuario desde el servicio Java"""
        try:
            logger.info(f"üîç _get_user_progress_data llamado con tenant_id: {tenant_id}, user_context: {user_context}")
            
            if not tenant_id or not user_context:
                logger.warning("Faltan par√°metros para consultar datos del usuario")
                return None
                
            phone = user_context.get("phone")
            logger.info(f"üîç Tel√©fono obtenido del contexto: {phone}")
            
            if not phone:
                logger.warning("No se encontr√≥ tel√©fono en el contexto del usuario")
                return None
            
            # üöÄ OPTIMIZACI√ìN: Usar configuraci√≥n del tenant desde memoria precargada
            tenant_context = user_context.get('tenant_context', {})
            tenant_config = tenant_context.get('tenant_config', {})
            if not tenant_config:
                logger.warning(f"No se encontr√≥ configuraci√≥n para tenant {tenant_id} en memoria precargada")
                return None
                
            client_project_id = tenant_config.get("client_project_id")
            if not client_project_id:
                logger.warning(f"No se encontr√≥ client_project_id para tenant {tenant_id}")
                return None
            
            # Consultar datos del usuario desde el servicio Java
            import requests
            import os
            
            java_service_url = os.getenv("POLITICAL_REFERRALS_SERVICE_URL")
            logger.info(f"üîç Java service URL: {java_service_url}")
            if not java_service_url:
                logger.warning("POLITICAL_REFERRALS_SERVICE_URL no configurado")
                return None
            
            # Consultar usuario por tel√©fono
            user_url = f"{java_service_url}/api/users/by-phone"
            user_payload = {
                "clientProjectId": client_project_id,
                "phone": phone
            }
            
            logger.info(f"üîç Consultando usuario: {user_url} con payload: {user_payload}")
            
            user_response = requests.post(user_url, json=user_payload, timeout=10)
            logger.info(f"üîç Respuesta del usuario: status={user_response.status_code}")
            if user_response.status_code != 200:
                logger.warning(f"‚ùå Error consultando usuario: {user_response.status_code}")
                logger.warning(f"‚ùå Response text: {user_response.text}")
                return None
                
            user_data = user_response.json()
            logger.info(f"üîç Datos del usuario obtenidos: {user_data}")
            if not user_data:
                logger.warning("‚ùå Usuario no encontrado")
                return None
            
            # Consultar referidos del usuario
            referral_code = user_data.get("referralCode")
            if not referral_code:
                logger.warning("Usuario no tiene c√≥digo de referido")
                return {
                    "user": user_data,
                    "referrals": [],
                    "points": 0,
                    "referral_code": None
                }
            
            # Consultar usuarios referidos por este c√≥digo
            referrals_url = f"{java_service_url}/api/users/by-referral-code"
            referrals_payload = {
                "clientProjectId": client_project_id,
                "referralCode": referral_code
            }
            
            logger.info(f"Consultando referidos: {referrals_url} con payload: {referrals_payload}")
            
            referrals_response = requests.post(referrals_url, json=referrals_payload, timeout=10)
            referrals = []
            
            if referrals_response.status_code == 200:
                referrals_data = referrals_response.json()
                if isinstance(referrals_data, list):
                    referrals = referrals_data
                elif isinstance(referrals_data, dict) and referrals_data:
                    referrals = [referrals_data]
            
            # Calcular puntos (50 por referido completado)
            completed_referrals = [r for r in referrals if r.get("completed", False)]
            points = len(completed_referrals) * 50
            
            return {
                "user": user_data,
                "referrals": referrals,
                "completed_referrals": completed_referrals,
                "points": points,
                "referral_code": referral_code,
                "total_referrals": len(referrals)
            }
            
        except Exception as e:
            logger.error(f"Error obteniendo datos del usuario: {str(e)}")
            return None
    
    def _format_user_progress_response(self, user_data: Dict[str, Any], contact_name: str) -> str:
        """Formatea la respuesta con los datos reales del usuario"""
        try:
            user = user_data.get("user", {})
            points = user_data.get("points", 0)
            total_referrals = user_data.get("total_referrals", 0)
            completed_referrals = user_data.get("completed_referrals", [])
            referral_code = user_data.get("referral_code")
            
            user_name = user.get("name", "Usuario")
            user_city = user.get("city", "tu ciudad")
            
            response = f"""üéØ **¬°Hola {user_name}! Aqu√≠ est√° tu progreso:**

[TROFEO] **Tus Puntos Actuales: {points}**
- Referidos completados: {len(completed_referrals)}
- Total de referidos: {total_referrals}
- Puntos por referido: 50 puntos

[GRAFICO] **Tu Ranking:**
- Ciudad: {user_city}
- Departamento: {user.get('state', 'N/A')}
- Pa√≠s: Colombia

[ENLACE] **Tu C√≥digo de Referido: {referral_code}**

[OBJETIVO] **¬°Sigue invitando!**
Cada persona que se registre con tu c√≥digo te suma 50 puntos m√°s.

¬øQuieres que te env√≠e tu link personalizado para compartir?"""
            
            return response
            
        except Exception as e:
            logger.error(f"Error formateando respuesta de progreso: {str(e)}")
            return f"Error obteniendo tu progreso. Por favor intenta de nuevo."
    
    def _handle_functional_request(self, query: str, branding_config: Dict[str, Any], tenant_id: str = None, user_context: Dict[str, Any] = None) -> str:
        """Maneja solicitudes funcionales como '?C√≥mo voy?' o pedir link"""
        query_lower = query.lower()
        contact_name = branding_config.get("contactName", "el candidato")
        
        if any(word in query_lower for word in ["como voy", "c√≥mo voy", "progreso", "puntos", "ranking"]):
            # Intentar obtener datos reales del usuario
            user_data = self._get_user_progress_data(tenant_id, user_context)
            
            if user_data:
                return self._format_user_progress_response(user_data, contact_name)
            else:
                # Fallback a respuesta gen√©rica si no se pueden obtener datos
                return f"""!Excelente pregunta! Te explico c√≥mo funciona el sistema de puntos de la campa√±a de {contact_name}:

[TROFEO] **Sistema de Puntos:**
- Cada referido registrado con tu c√≥digo: **50 puntos**
- Retos semanales: **puntaje adicional**
- Ranking actualizado a nivel ciudad, departamento y pa√≠s

[GRAFICO] **Para ver tu progreso:**
Escribe "?C√≥mo voy?" y te mostrar√©:
- Tus puntos totales
- N√∫mero de referidos
- Tu puesto en ciudad y nacional
- Lista de quienes est√°n cerca en el ranking

[ENLACE] **Para invitar personas:**
Escribe "dame mi c√≥digo" o "mandame el link" y te enviar√© tu enlace personalizado para referir amigos y familiares.

?Quieres tu c√≥digo de referido ahora?"""
        
        elif any(word in query_lower for word in ["link", "c√≥digo", "codigo", "referido", "mandame", "dame"]):
            return f"""!Por supuesto! Te ayudo con tu c√≥digo de referido para la campa√±a de {contact_name}.

[ENLACE] **Tu c√≥digo personalizado:**
Pronto tendr√°s tu enlace √∫nico para referir personas.

[CELULAR] **C√≥mo usarlo:**
1. Comparte tu link con amigos y familiares
2. Cada persona que se registre suma 50 puntos
3. Sube en el ranking y gana recompensas

[OBJETIVO] **Mensaje sugerido para compartir:**
"!Hola! Te invito a unirte a la campa√±a de {contact_name}. Es una oportunidad de ser parte del cambio que Colombia necesita. √önete aqu√≠: [TU_LINK]"

?Te gustar√≠a que genere tu c√≥digo ahora?"""
        
        else:
            return f"""!Claro! Te ayudo con informaci√≥n sobre la campa√±a de {contact_name}.

Puedes preguntarme sobre:
- Las propuestas de {contact_name}
- C√≥mo participar en la campa√±a
- Sistema de puntos y ranking
- Oportunidades de voluntariado
- Agendar citas con el equipo

?En qu√© te puedo ayudar espec√≠ficamente?"""
    
    async def classify_intent(self, tenant_id: str, message: str, user_context: Dict[str, Any], session_id: str = None, tenant_config: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Clasifica la intenci√≥n de un mensaje con contexto de sesi√≥n
        
        Args:
            tenant_id: ID del tenant
            message: Mensaje a clasificar
            user_context: Contexto del usuario
            session_id: ID de la sesi√≥n para contexto
            
        Returns:
            Clasificaci√≥n de intenci√≥n
        """
        try:
            logger.info(f"üéØ [CLASIFICACI√ìN BASE] Iniciando clasificaci√≥n para: '{message[:50]}...'")
            logger.info(f"üéØ [CLASIFICACI√ìN BASE] Tenant ID: {tenant_id}")
            logger.info(f"üéØ [CLASIFICACI√ìN BASE] Session ID: {session_id}")
            
            # üöÄ OPTIMIZACI√ìN: Detecci√≥n ultra-r√°pida para saludos comunes
            message_lower = message.lower().strip()
            if message_lower in self._common_responses:
                classification = self._common_responses[message_lower]
                logger.info(f"üöÄ [CLASIFICACI√ìN BASE] BYPASS GEMINI: Saludo com√∫n '{message_lower}' -> {classification}")
                logger.info(f"üìä [CLASIFICACI√ìN BASE] Resultado: {classification} (confianza: 1.0)")
                return {
                    "category": classification,
                    "confidence": 0.95,
                    "original_message": message,
                    "reason": "Bypass Gemini - Saludo com√∫n"
                }
            
            # üöÄ VELOCIDAD M√ÅXIMA: Usar solo IA, sin bypass
            logger.info(f"üéØ USANDO IA DIRECTA: '{message[:30]}...'")
            
            # üîß OPTIMIZACI√ìN: Detecci√≥n r√°pida basada en contexto
            if user_context and user_context.get("user_state") == "WAITING_NAME":
                if self._analyze_registration_intent(message, "name"):
                    logger.info(f"‚úÖ BYPASS GEMINI: Contexto WAITING_NAME -> registration_response")
                    return {
                        "category": "registration_response",
                        "confidence": 0.95,
                        "original_message": message,
                        "reason": "Bypass Gemini - Contexto"
                    }
            
            if user_context and user_context.get("user_state") == "WAITING_LASTNAME":
                if self._analyze_registration_intent(message, "lastname"):
                    logger.info(f"‚úÖ BYPASS GEMINI: Contexto WAITING_LASTNAME -> registration_response")
                    return {
                        "category": "registration_response",
                        "confidence": 0.95,
                        "original_message": message,
                        "reason": "Bypass Gemini - Contexto"
                    }
            
            if user_context and user_context.get("user_state") == "WAITING_CITY":
                if self._analyze_registration_intent(message, "city"):
                    logger.info(f"‚úÖ BYPASS GEMINI: Contexto WAITING_CITY -> registration_response")
                    return {
                        "category": "registration_response",
                        "confidence": 0.95,
                        "original_message": message,
                        "reason": "Bypass Gemini - Contexto"
                    }
            
            # üîß OPTIMIZACI√ìN: Solo usar Gemini para casos complejos
            logger.info(f"üéØ USANDO GEMINI para caso complejo: '{message[:50]}...'")
            
            # üöÄ OPTIMIZACI√ìN CR√çTICA: Timeout r√°pido para evitar demoras
            import asyncio
            try:
                # Intentar con timeout de 8 segundos
                classification_result = await asyncio.wait_for(
                    self._classify_with_ai(message, user_context, "", tenant_id),
                    timeout=8.0
                )
            except asyncio.TimeoutError:
                logger.warning(f"‚è∞ TIMEOUT en clasificaci√≥n Gemini para '{message[:30]}...', usando fallback r√°pido")
                # Fallback r√°pido basado en palabras clave
                classification_result = self._fast_fallback_classification(message)
            
            # üöÄ OPTIMIZACI√ìN CR√çTICA: Usar configuraci√≥n enviada desde Java (ya optimizada)
            # La configuraci√≥n viene como par√°metro desde el servicio Java
            if not tenant_config:
                logger.info(f"üîç Obteniendo configuraci√≥n desde servicio Java para tenant: {tenant_id}")
                tenant_config = configuration_service.get_tenant_config(tenant_id)
                if not tenant_config:
                    logger.warning(f"No se encontr√≥ configuraci√≥n para tenant {tenant_id} en memoria precargada para clasificaci√≥n")
                    tenant_config = {}
            else:
                logger.debug(f"‚úÖ Usando configuraci√≥n optimizada enviada desde Java para tenant: {tenant_id}")

            # Asegurar session_id estable: derivar de user_context cuando no venga
            if not session_id:
                derived = None
                if user_context:
                    derived = user_context.get("session_id") or user_context.get("user_id") or user_context.get("phone")
                session_id = f"session_{tenant_id}_{derived}" if derived else f"session_{tenant_id}_classify"

            # Registrar/actualizar contexto m√≠nimo de sesi√≥n para clasificaci√≥n
            session = session_context_service.get_session(session_id)
            if not session:
                session_context_service.create_session(session_id=session_id, tenant_id=tenant_id, user_id=(user_context or {}).get("user_id"), user_context=user_context or {})
            else:
                session_context_service.update_user_context(session_id, user_context or {})
            if not tenant_config:
                logger.warning(f"[ERROR] TENANT NO ENCONTRADO: {tenant_id} - Retornando general_query")
                return {
                    "category": "general_query",
                    "confidence": 0.0,
                    "original_message": message,
                    "error": "Tenant no encontrado"
                }
            
            # Obtener contexto de la sesi√≥n para la clasificaci√≥n
            session_context = session_context_service.build_context_for_ai(session_id)
            
            # Clasificar intenci√≥n usando IA con contexto de sesi√≥n
            classification = await self._classify_with_ai(message, user_context, session_context, tenant_id)
            
            # üìä IMPRIMIR RESULTADO FINAL DE CLASIFICACI√ìN
            if classification and classification.get("category"):
                logger.info(f"üìä [CLASIFICACI√ìN BASE] RESULTADO FINAL: {classification['category']} (confianza: {classification.get('confidence', 0):.2f})")
                logger.info(f"üìä [CLASIFICACI√ìN BASE] Mensaje original: '{message[:100]}...'")
                logger.info(f"üìä [CLASIFICACI√ìN BASE] Tenant: {tenant_id}")
                logger.info(f"üìä [CLASIFICACI√ìN BASE] {'='*60}")
            else:
                logger.warning(f"‚ö†Ô∏è [CLASIFICACI√ìN BASE] No se pudo clasificar el mensaje: '{message[:50]}...'")
            
            return classification
            
        except Exception as e:
            logger.error(f"Error clasificando intenci√≥n para tenant {tenant_id}: {str(e)}")
            return {
                "category": "general_query",
                "confidence": 0.0,
                "original_message": message,
                "error": str(e)
            }

    def _fast_fallback_classification(self, message: str) -> Dict[str, Any]:
        """
        Clasificaci√≥n r√°pida basada en palabras clave para casos de timeout
        """
        message_lower = message.lower()
        
        # Palabras clave para diferentes intenciones
        keywords = {
            "saludo_apoyo": ["hola", "buenos", "buenas", "saludo", "hey"],
            "conocer_candidato": ["quien", "candidato", "propuesta", "plan", "agua", "viva", "hidroituango"],
            "registro": ["nombre", "apellido", "ciudad", "telefono", "email"],
            "agendar_cita": ["cita", "reunion", "calendly", "agendar"],
            "malicioso": ["odio", "violencia", "amenaza", "insulto"]
        }
        
        # Buscar coincidencias
        for intent, words in keywords.items():
            for word in words:
                if word in message_lower:
                    return {
                        "category": intent,
                        "confidence": 0.8,
                        "original_message": message,
                        "reason": "Fast fallback - keyword match"
                    }
        
        # Default
        return {
            "category": "saludo_apoyo",
            "confidence": 0.6,
            "original_message": message,
            "reason": "Fast fallback - default"
        }

    async def analyze_registration(self, tenant_id: str, message: str, user_context: Dict[str, Any] = None,
                                   session_id: str = None, current_state: str = None) -> Dict[str, Any]:
        """
        Analiza un mensaje usando IA para entender el contexto completo y extraer datos de registro.

        Retorna: { type: "name|lastname|city|info|other", value: str|None, confidence: float }
        """
        try:
            text = (message or "").strip()
            if not text:
                return {"type": "other", "value": None, "confidence": 0.0}

            # üöÄ VELOCIDAD M√ÅXIMA: Usar solo IA para an√°lisis de registro
            logger.info(f"üéØ USANDO IA DIRECTA REGISTRATION: '{text[:30]}...'")

            if not session_id:
                derived = None
                if user_context:
                    derived = user_context.get("session_id") or user_context.get("user_id") or user_context.get("phone")
                session_id = f"session_{tenant_id}_{derived}" if derived else f"session_{tenant_id}_registration"

            state = (current_state or "").upper()

            # Usar IA para an√°lisis inteligente basado en contexto
            ai_analysis = await self._analyze_registration_with_ai(text, state, user_context, session_id)
            if ai_analysis:
                return ai_analysis

            # Fallback inteligente si IA falla (por cuota excedida u otros errores)
            logger.info("Usando l√≥gica de fallback inteligente para an√°lisis de registro")
            return self._fallback_registration_analysis(text, state)
            
        except Exception as e:
            logger.error(f"Error analizando registro: {str(e)}")
            return {"type": "other", "value": None, "confidence": 0.0}
    
    def _fallback_registration_analysis(self, text: str, state: str) -> Dict[str, Any]:
        """
        An√°lisis de fallback inteligente cuando la IA no est√° disponible
        """
        try:
            lowered = text.lower().strip()
            
            # Detectar preguntas
            if "?" in text or any(w in lowered for w in ["qu√©", "que ", "c√≥mo", "como ", "qui√©n", "quien ", "d√≥nde", "donde ", "por qu√©", "por que"]):
                return {"type": "info", "value": None, "confidence": 0.85}
            
            # Detectar nombres (l√≥gica mejorada)
            words = text.split()
            
            # Si es un saludo simple
            if lowered in ["hola", "hi", "hello", "buenos d√≠as", "buenas tardes", "buenas noches"]:
                return {"type": "other", "value": None, "confidence": 0.9}
            
            # Si contiene palabras de confirmaci√≥n + nombre
            confirmation_words = ["perfecto", "ok", "vale", "listo", "s√≠", "si", "bueno", "bien"]
            if any(word in lowered for word in confirmation_words):
                # Buscar nombre despu√©s de la confirmaci√≥n
                for i, word in enumerate(words):
                    if word.lower() in confirmation_words and i + 1 < len(words):
                        # Extraer el resto como nombre
                        name_parts = words[i+1:]
                        if name_parts and all(part.replace("-", "").replace("'", "").isalpha() for part in name_parts):
                            name = " ".join(name_parts)
                            if len(name) >= 2:
                                return {"type": "name", "value": name, "confidence": 0.8}
            
            # Si parece un nombre directo (2-4 palabras, solo letras)
            if 2 <= len(words) <= 4 and not any(c.isdigit() for c in text):
                # Verificar que no empiece con palabras interrogativas
                if words[0].lower() not in ["que", "qu√©", "c√≥mo", "como", "cu√°l", "cual", "qui√©n", "quien", "d√≥nde", "donde"]:
                    # Verificar que todas las palabras sean letras
                    if all(word.replace("-", "").replace("'", "").isalpha() for word in words):
                        return {"type": "name", "value": text, "confidence": 0.7}
            
            # Detectar ciudades
            city_indicators = ["vivo en", "soy de", "estoy en", "resido en", "ciudad", "municipio"]
            if any(indicator in lowered for indicator in city_indicators):
                # Extraer ciudad despu√©s del indicador
                for indicator in city_indicators:
                    if indicator in lowered:
                        city_part = lowered.split(indicator)[-1].strip()
                        if city_part and len(city_part) >= 2:
                            return {"type": "city", "value": city_part.title(), "confidence": 0.8}
            
            # Si contiene "me llamo" o "mi nombre es"
            if "me llamo" in lowered or "mi nombre es" in lowered:
                name_part = text
                if "me llamo" in lowered:
                    name_part = text.split("me llamo")[-1].strip()
                elif "mi nombre es" in lowered:
                    name_part = text.split("mi nombre es")[-1].strip()
                
                if name_part and len(name_part) >= 2:
                    return {"type": "name", "value": name_part, "confidence": 0.9}
            
            return {"type": "other", "value": None, "confidence": 0.5}
            
        except Exception as e:
            logger.error(f"Error en an√°lisis de fallback: {str(e)}")
            return {"type": "other", "value": None, "confidence": 0.0}
    
    async def extract_data(self, tenant_id: str, message: str, data_type: str) -> Dict[str, Any]:
        """
        Extrae datos espec√≠ficos de un mensaje
        
        Args:
            tenant_id: ID del tenant
            message: Mensaje del usuario
            data_type: Tipo de dato a extraer
            
        Returns:
            Datos extra√≠dos
        """
        try:
            logger.info(f"Extrayendo {data_type} para tenant {tenant_id}")
            
            # üöÄ OPTIMIZACI√ìN: Usar configuraci√≥n del tenant desde memoria precargada
            # Nota: Este m√©todo necesita ser llamado con user_context para acceder a tenant_context
            logger.warning(f"‚ö†Ô∏è M√©todo extract_data necesita ser optimizado para usar memoria precargada")
            tenant_config = configuration_service.get_tenant_config(tenant_id)
            if not tenant_config:
                return {
                    "extracted_data": {},
                    "error": "Tenant no encontrado"
                }
            
            # Extraer datos usando IA
            extracted_data = await self._extract_with_ai(message, data_type)
            
            return {
                "extracted_data": extracted_data
            }
            
        except Exception as e:
            logger.error(f"Error extrayendo {data_type} para tenant {tenant_id}: {str(e)}")
            return {
                "extracted_data": {},
                "error": str(e)
            }
    
    async def validate_data(self, tenant_id: str, data: str, data_type: str) -> Dict[str, Any]:
        """
        Valida datos de entrada del usuario
        
        Args:
            tenant_id: ID del tenant
            data: Dato a validar
            data_type: Tipo de dato
            
        Returns:
            Resultado de validaci√≥n
        """
        try:
            # üîß OPTIMIZACI√ìN: Verificaci√≥n r√°pida de explicaciones sobre datos disponibles
            if self._is_data_explanation(data):
                return {
                    "is_valid": False,
                    "data_type": data_type,
                    "reason": "explicacion_datos",
                    "suggested_response": self._generate_explanation_response(data_type, data)
                }
            
            # üîß OPTIMIZACI√ìN: Verificaci√≥n r√°pida de palabras que NO son datos v√°lidos
            if self._contains_non_data_indicators(data):
                return {
                    "is_valid": False,
                    "data_type": data_type,
                    "reason": "no_es_dato",
                    "suggested_response": self._generate_clarification_response(data_type)
                }
            
            # Validaci√≥n b√°sica por tipo
            is_valid = self._basic_validation(data, data_type)
            
            if not is_valid:
                return {
                    "is_valid": False,
                    "data_type": data_type,
                    "reason": "formato_invalido"
                }
            
            # üîß OPTIMIZACI√ìN: Validaci√≥n IA solo para casos complejos
            if data_type.lower() in ["name", "lastname", "city"] and len(data) > 3:
                ai_validation = await self._validate_with_ai(data, data_type)
                if not ai_validation:
                    return {
                        "is_valid": False,
                        "data_type": data_type,
                        "reason": "contenido_invalido"
                    }
            
            return {
                "is_valid": True,
                "data_type": data_type
            }
            
        except Exception as e:
            logger.error(f"Error validando {data_type} para tenant {tenant_id}: {str(e)}")
            return {
                "is_valid": False,
                "error": str(e)
            }
    
    async def normalize_location(self, city_input: str) -> Dict[str, Any]:
        """Normaliza el nombre de una ciudad (puede ser fuera de Colombia),
        reconoce apodos y detecta su estado/departamento y pa√≠s cuando sea posible."""
        self._ensure_model_initialized()
        # 1) Intento OFFLINE: apodos y alias conocidos + regex sencillas
        offline = self._normalize_location_offline(city_input)
        if offline:
            return offline
        if not self.model:
            return {"city": city_input.strip(), "state": None, "country": None}
        try:
            prompt = f"""
Eres un asistente que estandariza ubicaciones (cualquier pa√≠s) y reconoce apodos locales.

Tarea: Dada una entrada de ciudad (puede venir con errores ortogr√°ficos, variaciones o apodos), devuelve un JSON con:
- city: nombre oficial de la ciudad/municipio con may√∫sculas y tildes correctas
- state: estado/departamento/provincia oficial
- country: pa√≠s oficial

Reglas:
- Solo responde el JSON, sin texto adicional.
- Si la entrada corresponde a un apodo, resu√©lvelo al nombre oficial.
- Si no puedes determinar estado o pa√≠s, deja ese campo con null.
 - La entrada puede ser una FRASE COMPLETA del usuario (ej: "vivo en ..."). Extrae y normaliza la ciudad impl√≠cita.

Apodos comunes en Colombia (no exhaustivo):
- "la nevera" -> Bogot√°
- "medallo" -> Medell√≠n
- "la arenosa" -> Barranquilla
- "la sucursal del cielo" -> Cali
- "la ciudad bonita" -> Bucaramanga
 - "la ciudad de la eterna primavera" -> Medell√≠n

Ejemplos v√°lidos:
Entrada: "medellin" -> {"city": "Medell√≠n", "state": "Antioquia", "country": "Colombia"}
Entrada: "bogota" -> {"city": "Bogot√°", "state": "Cundinamarca", "country": "Colombia"}
Entrada: "soacha" -> {"city": "Soacha", "state": "Cundinamarca", "country": "Colombia"}
Entrada: "la nevera" -> {"city": "Bogot√°", "state": "Cundinamarca", "country": "Colombia"}
Entrada: "vivo en la ciudad de la eterna primavera" -> {"city": "Medell√≠n", "state": "Antioquia", "country": "Colombia"}
Entrada: "New York" -> {"city": "New York", "state": "New York", "country": "United States"}

Entrada real: "{city_input}".
Responde solo el JSON estricto sin comentarios:
"""
            response_text = await self._generate_content(prompt)
            text = (response_text or "").strip()
            import json
            result = json.loads(text)
            # Sanitizar salida m√≠nima
            city = (result.get("city") or city_input or "").strip()
            state = (result.get("state") or None)
            country = (result.get("country") or None)
            return {"city": city, "state": state, "country": country}
        except Exception as e:
            logger.error(f"Error normalizando ubicaci√≥n: {str(e)}")
            return {"city": city_input.strip() if city_input else "", "state": None, "country": None}

    def _normalize_location_offline(self, city_input: str) -> Optional[Dict[str, Any]]:
        """Mapa r√°pido de apodos/alias y extracci√≥n simple desde frases.
        Retorna None si no puede resolver offline.
        """
        if not city_input:
            return None
        text = city_input.strip().lower()
        # Normalizaciones simples de variantes comunes
        text = text.replace("sudamericana", "suramericana")
        text = text.replace("heroica", "her√≥ica") if "ciudad heroica" in text else text

        # Diccionario de apodos/alias -> (city, state, country)
        nick_map = {
            # Bogot√°
            "la nevera": ("Bogot√°", "Cundinamarca", "Colombia"),
            "bogota": ("Bogot√°", "Cundinamarca", "Colombia"),
            "bogot√°": ("Bogot√°", "Cundinamarca", "Colombia"),
            "atenas suramericana": ("Bogot√°", "Cundinamarca", "Colombia"),
            "la atenas suramericana": ("Bogot√°", "Cundinamarca", "Colombia"),
            "atenas sudamericana": ("Bogot√°", "Cundinamarca", "Colombia"),
            "la atenas sudamericana": ("Bogot√°", "Cundinamarca", "Colombia"),
            # Medell√≠n
            "medallo": ("Medell√≠n", "Antioquia", "Colombia"),
            "ciudad de la eterna primavera": ("Medell√≠n", "Antioquia", "Colombia"),
            "la ciudad de la eterna primavera": ("Medell√≠n", "Antioquia", "Colombia"),
            "medellin": ("Medell√≠n", "Antioquia", "Colombia"),
            "medell√≠n": ("Medell√≠n", "Antioquia", "Colombia"),
            # Barranquilla
            "la arenosa": ("Barranquilla", "Atl√°ntico", "Colombia"),
            "puerta de oro de colombia": ("Barranquilla", "Atl√°ntico", "Colombia"),
            "la puerta de oro de colombia": ("Barranquilla", "Atl√°ntico", "Colombia"),
            "curramba": ("Barranquilla", "Atl√°ntico", "Colombia"),
            "barranquilla": ("Barranquilla", "Atl√°ntico", "Colombia"),
            # Cali
            "la sucursal del cielo": ("Cali", "Valle del Cauca", "Colombia"),
            "sultana del valle": ("Cali", "Valle del Cauca", "Colombia"),
            "cali": ("Cali", "Valle del Cauca", "Colombia"),
            # Bucaramanga
            "la ciudad bonita": ("Bucaramanga", "Santander", "Colombia"),
            "ciudad de los parques": ("Bucaramanga", "Santander", "Colombia"),
            "bucaramanga": ("Bucaramanga", "Santander", "Colombia"),
            # Buga
            "ciudad se√±ora": ("Buga", "Valle del Cauca", "Colombia"),
            # Cartagena
            "ciudad heroica": ("Cartagena", "Bol√≠var", "Colombia"),
            "la ciudad her√≥ica": ("Cartagena", "Bol√≠var", "Colombia"),
            "corralito de piedra": ("Cartagena", "Bol√≠var", "Colombia"),
            # Ch√≠a
            "ciudad de la luna": ("Ch√≠a", "Cundinamarca", "Colombia"),
            # C√∫cuta
            "perla del norte": ("C√∫cuta", "Norte de Santander", "Colombia"),
            # Ibagu√©
            "ciudad musical": ("Ibagu√©", "Tolima", "Colombia"),
            # Ipiales
            "ciudad de las nubes verdes": ("Ipiales", "Nari√±o", "Colombia"),
            # Monter√≠a
            "perla del sinu": ("Monter√≠a", "C√≥rdoba", "Colombia"),
            "perla del sin√∫": ("Monter√≠a", "C√≥rdoba", "Colombia"),
            # Neiva
            "ciudad amable": ("Neiva", "Huila", "Colombia"),
            # Pasto
            "ciudad sorpresa": ("Pasto", "Nari√±o", "Colombia"),
            # Pereira
            "ciudad sin puertas": ("Pereira", "Risaralda", "Colombia"),
            # Popay√°n
            "ciudad blanca": ("Popay√°n", "Cauca", "Colombia"),
            # Riohacha
            "f√©nix del caribe": ("Riohacha", "La Guajira", "Colombia"),
            "fenix del caribe": ("Riohacha", "La Guajira", "Colombia"),
            # Santa Marta
            "perla de america": ("Santa Marta", "Magdalena", "Colombia"),
            "perla de am√©rica": ("Santa Marta", "Magdalena", "Colombia"),
            # Valledupar
            "capital mundial del vallenato": ("Valledupar", "Cesar", "Colombia"),
            # Villavicencio
            "puerta del llano": ("Villavicencio", "Meta", "Colombia"),
            # Zipaquir√°
            "capital salinera": ("Zipaquir√°", "Cundinamarca", "Colombia"),
        }

        # Match exacto por clave completa
        if text in nick_map:
            city, state, country = nick_map[text]
            return {"city": city, "state": state, "country": country}

        # B√∫squeda por inclusi√≥n de apodos conocidos en frases completas
        for key, (city, state, country) in nick_map.items():
            if key in text:
                return {"city": city, "state": state, "country": country}

        # Regex para capturar patrones frecuentes en frases
        import re
        patterns = [
            r"ciudad\s+de\s+la\s+eterna\s+primavera",
            r"vivo\s+en\s+medallo",
            r"vivo\s+en\s+la\s+nevera",
            r"estoy\s+en\s+la\s+arenosa",
        ]
        for pat in patterns:
            if re.search(pat, text):
                # Reutilizar nick_map via b√∫squeda por inclusi√≥n
                for key, (city, state, country) in nick_map.items():
                    if key in text:
                        return {"city": city, "state": state, "country": country}

        # Si el texto parece una ciudad colombiana com√∫n, capitalizar m√≠nimamente
        common_cities = {
            "soacha": ("Soacha", "Cundinamarca", "Colombia"),
            "itagui": ("Itag√º√≠", "Antioquia", "Colombia"),
            "itag√ºi": ("Itag√º√≠", "Antioquia", "Colombia"),
        }
        t = text.replace("√°","a").replace("√©","e").replace("√≠","i").replace("√≥","o").replace("√∫","u")
        for key, val in common_cities.items():
            if t == key or f" {key} " in f" {t} ":
                city, state, country = val
                return {"city": city, "state": state, "country": country}

        return None

    # M√©todos privados para procesamiento de IA
    
    async def _ensure_tenant_documents_loaded(self, tenant_id: str, ai_config: Dict[str, Any]):
        """Asegura que los documentos del tenant est√©n cargados - OPTIMIZADO"""
        try:
            # üöÄ OPTIMIZACI√ìN: Verificar cache primero
            doc_info = document_context_service.get_tenant_document_info(tenant_id)
            if doc_info and doc_info.get('document_count', 0) > 0:
                logger.debug(f"[LIBROS] Documentos ya cargados para tenant {tenant_id}: {doc_info.get('document_count', 0)} docs")
                return
            
            # üöÄ OPTIMIZACI√ìN: Solo cargar si no est√°n en cache
            documentation_bucket_url = ai_config.get("documentation_bucket_url")
            
            if documentation_bucket_url:
                logger.info(f"üì• Cargando documentos para tenant {tenant_id} desde: {documentation_bucket_url}")
                # üöÄ OPTIMIZACI√ìN: Usar carga as√≠ncrona m√°s r√°pida
                success = await document_context_service.load_tenant_documents(tenant_id, documentation_bucket_url)
                if success:
                    doc_info = document_context_service.get_tenant_document_info(tenant_id)
                    logger.info(f"[OK] Documentos cargados para tenant {tenant_id}: {doc_info.get('document_count', 0)} docs")
                else:
                    logger.warning(f"[ADVERTENCIA] No se pudieron cargar documentos para tenant {tenant_id}")
            else:
                logger.debug(f"[INFO] No hay bucket de documentaci√≥n configurado para tenant {tenant_id}")
                
        except Exception as e:
            logger.error(f"[ERROR] Error cargando documentos para tenant {tenant_id}: {str(e)}", exc_info=True)
    
    async def _generate_ai_response(self, query: str, user_context: Dict[str, Any], 
                                  ai_config: Dict[str, Any], branding_config: Dict[str, Any], 
                                  tenant_id: str, session_id: str = None) -> str:
        """Genera respuesta usando IA con contexto de documentos"""
        
        # üöÄ OPTIMIZACI√ìN: Obtener configuraci√≥n del tenant desde memoria precargada
        tenant_context = user_context.get('tenant_context', {})
        tenant_config = tenant_context.get('tenant_config', {})
        
        logger.info(f"üîç [TENANT_CONFIG] tenant_config keys: {list(tenant_config.keys()) if tenant_config else 'None'}")
        logger.info(f"üîç [TENANT_CONFIG] tenant_config content: {tenant_config}")
        
        # [COHETE] FASE 6: Usar RAGOrchestrator si est√° habilitado
        if self.use_rag_orchestrator and self.rag_orchestrator:
            try:
                # üöÄ OPTIMIZACI√ìN: Solo cargar documentos si no est√°n en cache
                doc_info = document_context_service.get_tenant_document_info(tenant_id)
                if not doc_info or doc_info.get('document_count', 0) == 0:
                    await self._ensure_tenant_documents_loaded(tenant_id, ai_config)
                
                logger.info(f"[OBJETIVO] Usando RAGOrchestrator | tenant_id={tenant_id} | session_id={session_id} | query='{query[:50]}...'")
                response = await self.rag_orchestrator.process_query_simple(
                    query=query,
                    tenant_id=tenant_id,
                    user_context=user_context,
                    session_id=session_id,
                    tenant_config=tenant_config
                )
                logger.info(f"[OK] RAG respuesta generada | length={len(response)} chars")
                return response
            except Exception as e:
                logger.error(f"[ERROR] Error usando RAGOrchestrator: {str(e)}", exc_info=True)
                logger.info("[ADVERTENCIA] Fallback a l√≥gica original (sin RAG)")
                # Continuar con l√≥gica original como fallback
        
        # L√≥gica original (sin RAG)
        self._ensure_model_initialized()
        if not self.model:
            return "Lo siento, el servicio de IA no est√° disponible."
        
        try:
            # üöÄ OPTIMIZACI√ìN: Solo cargar documentos si no est√°n en cache
            doc_info = document_context_service.get_tenant_document_info(tenant_id)
            if not doc_info or doc_info.get('document_count', 0) == 0:
                await self._ensure_tenant_documents_loaded(tenant_id, ai_config)
            
            # Obtener contexto relevante de documentos del cliente
            relevant_context = ""
            try:
                relevant_context = await document_context_service.get_relevant_context(
                    tenant_id, query, max_results=2  # Reducido de 3 a 2 para mayor velocidad
                )
                if relevant_context:
                    logger.info(f"Contexto relevante obtenido para tenant {tenant_id}: {len(relevant_context)} caracteres")
            except Exception as e:
                logger.warning(f"Error obteniendo contexto relevante: {str(e)}")
            
            # Construir prompt con contexto de documentos
            prompt = self._build_chat_prompt(query, user_context, branding_config, relevant_context)
            
            # üöÄ OPTIMIZACI√ìN: Usar configuraci√≥n ultra-r√°pida para chat conversacional
            response_text = await self._generate_content_ultra_fast(prompt, max_tokens=200)  # Usar m√©todo ultra-r√°pido
            
            return response_text
            
        except Exception as e:
            logger.error(f"Error generando respuesta con IA: {str(e)}")
            return "Lo siento, no pude procesar tu mensaje."
    
    def _detect_malicious_intent(self, message: str) -> Dict[str, Any]:
        """
        Detecta intenci√≥n maliciosa de manera inteligente usando an√°lisis contextual
        """
        message_lower = message.lower().strip()
        
        # Indicadores de comportamiento malicioso (no solo palabras, sino patrones)
        malicious_indicators = {
            "insultos_directos": [
                "idiota", "imb√©cil", "est√∫pido", "tonto", "bobo", "bruto",
                "hijueputa", "malparido", "gonorrea", "marica", "chimba",
                "careverga", "verga", "chimbo", "malparida", "hijuepucha"
            ],
            "ataques_campana": [
                "ladrones", "corruptos", "estafadores", "mentirosos", "falsos",
                "robando", "estafando", "mintiendo", "enga√±ando"
            ],
            "provocacion": [
                "vete a la mierda", "que se joda", "me importa un carajo",
                "no me importa", "me vale verga", "me vale mierda"
            ],
            "spam_indicators": [
                "spam", "basura", "mierda", "porquer√≠a", "pendejada"
            ]
        }
        
        # Analizar el mensaje por categor√≠as
        detected_categories = []
        confidence_score = 0.0
        
        for category, indicators in malicious_indicators.items():
            for indicator in indicators:
                if indicator in message_lower:
                    detected_categories.append(category)
                    confidence_score += 0.2
                    break
        
        # Detectar patrones de agresividad
        aggressive_patterns = [
            r'\b(que\s+se\s+joda|vete\s+a\s+la\s+mierda|me\s+importa\s+un\s+carajo)\b',
            r'\b(no\s+me\s+importa|me\s+vale\s+verga|me\s+vale\s+mierda)\b',
            r'\b(eres\s+un|son\s+unos|esto\s+es\s+una)\b.*\b(idiota|imb√©cil|estafa|mentira)\b'
        ]
        
        import re
        for pattern in aggressive_patterns:
            if re.search(pattern, message_lower):
                detected_categories.append("aggressive_pattern")
                confidence_score += 0.3
                break
        
        # Calcular confianza final
        confidence_score = min(confidence_score, 1.0)
        
        is_malicious = len(detected_categories) > 0 and confidence_score >= 0.3
        
        if is_malicious:
            logger.warning(f"üö® Intenci√≥n maliciosa detectada - Categor√≠as: {detected_categories}, Confianza: {confidence_score:.2f}")
            logger.warning(f"üö® Mensaje: '{message}'")
        
        return {
            "is_malicious": is_malicious,
            "categories": detected_categories,
            "confidence": confidence_score,
            "reason": "intelligent_intent_detection"
        }

    async def _classify_with_ai(self, message: str, user_context: Dict[str, Any], session_context: str = "", tenant_id: str = None) -> Dict[str, Any]:
        """Clasifica intenci√≥n usando IA con optimizaciones de velocidad"""
        
        self._ensure_model_initialized()
        
        # Primero verificar intenci√≥n maliciosa de manera inteligente
        malicious_detection = self._detect_malicious_intent(message)
        if malicious_detection["is_malicious"]:
            return {
                "category": "malicioso",
                "confidence": malicious_detection["confidence"],
                "original_message": message,
                "reason": malicious_detection["reason"],
                "detected_categories": malicious_detection["categories"]
            }
        
        # üöÄ OPTIMIZACI√ìN: Clasificaci√≥n h√≠brida (patrones + IA)
        pattern_result = self._classify_with_patterns(message, user_context)
        if pattern_result["confidence"] > 0.8:
            return pattern_result
        
        if not self.model:
            return {
                "category": "saludo_apoyo", 
                "confidence": 0.0,
                "original_message": message
            }
        
        try:
            # üöÄ OPTIMIZACI√ìN: Verificar cach√© de intenciones primero
            if tenant_id:
                cache_key = f"intent:{tenant_id}:{message.lower().strip()}"
                cached_result = self._intent_cache.get(cache_key)
                if cached_result and time.time() - cached_result.get("timestamp", 0) < 300:  # TTL 5 minutos
                    return cached_result
            
            # üöÄ OPTIMIZACI√ìN: Prompt ultra-corto para velocidad m√°xima
            prompt = f"""Clasifica: "{message}" en UNA categor√≠a:
saludo_apoyo|cita_campa√±a|conocer_candidato|publicidad_info|colaboracion_voluntariado|quejas|malicioso|registration_response|solicitud_funcional

Respuesta:"""
            
            # üîß OPTIMIZACI√ìN: Timeout ultra-agresivo (2 segundos)
            import asyncio
            try:
                response_text = await asyncio.wait_for(
                    self._generate_content_ultra_fast(prompt, max_tokens=5),
                    timeout=2.0
                )
            except asyncio.TimeoutError:
                return {
                    "category": "saludo_apoyo",
                    "confidence": 0.0,
                    "original_message": message,
                    "reason": "Timeout ultra-agresivo"
                }
            
            category = response_text.strip().lower()
            
            # üîß OPTIMIZACI√ìN: Detecci√≥n mejorada de bloqueo por safety filters
            if category in ["hola, ¬øen qu√© puedo ayudarte hoy?", "lo siento, no puedo procesar esa consulta en este momento. por favor, intenta reformular tu pregunta de manera m√°s espec√≠fica.", "hola", "hello", "hi"] or len(category) > 50:
                logger.warning("‚ö†Ô∏è GEMINI BLOQUEADO O RESPUESTA LARGA - Usando fallback")
                category = self._fallback_intent_classification(message, user_context)
            
            # Detectar si la respuesta es muy gen√©rica (posible bloqueo)
            if len(category) < 3 or category in ["ok", "yes", "no", "si", "s√≠"]:
                logger.warning("‚ö†Ô∏è RESPUESTA MUY GEN√âRICA - Posible bloqueo")
                category = self._fallback_intent_classification(message, user_context)
            
            logger.info(f"‚úÖ INTENCI√ìN: '{category}'")
            
            # Validar categor√≠a
            valid_categories = [
                "malicioso", "cita_campa√±a", "saludo_apoyo", "publicidad_info", 
                "conocer_candidato", "actualizacion_datos", "solicitud_funcional", 
                "colaboracion_voluntariado", "quejas", "lider", "atencion_humano", 
                "atencion_equipo_interno", "registration_response"
            ]
            
            if category not in valid_categories:
                logger.warning(f"[ADVERTENCIA] Intenci√≥n no v√°lida: '{category}', usando fallback inteligente")
                print(f"‚ùå INTENCI√ìN NO V√ÅLIDA: '{category}' - Usando fallback inteligente")
                category = self._fallback_intent_classification(message, user_context)
            
            # üîß DEBUG: Log final de clasificaci√≥n
            logger.info(f"[OBJETIVO] CLASIFICACI√ìN FINAL: '{category}' para mensaje: '{message[:50]}...'")
            print(f"‚úÖ CLASIFICACI√ìN FINAL: '{category}' para mensaje: '{message[:50]}...'")
            
            # üöÄ OPTIMIZACI√ìN: Guardar en cach√© para futuras consultas
            result = {
                "category": category,
                "confidence": 0.8,  # Confianza fija por simplicidad
                "original_message": message,
                "timestamp": time.time()  # TTL para limpieza autom√°tica
            }
            
            # Guardar en cach√© solo si tenemos tenant_id
            if tenant_id:
                cache_key = f"intent:{tenant_id}:{message.lower().strip()}"
                
                # Limpiar cach√© autom√°ticamente (TTL + tama√±o)
                self._cleanup_intent_cache()
                
                self._intent_cache[cache_key] = result
            
            return result
            
        except Exception as e:
            logger.error(f"[ERROR] Error clasificando con IA: {str(e)}", exc_info=True)
            return {
                "category": "general_query", 
                "confidence": 0.0,
                "original_message": message
            }
    
    def _classify_with_patterns(self, message: str, user_context: Dict[str, Any]) -> Dict[str, Any]:
        """Clasificaci√≥n ultra-r√°pida usando patrones de texto"""
        message_lower = message.lower().strip()
        
        # Patrones de alta confianza
        patterns = {
            "saludo_apoyo": [
                "hola", "hi", "hello", "buenos d√≠as", "buenas tardes", "buenas noches",
                "gracias", "ok", "okay", "s√≠", "si", "no", "perfecto", "excelente"
            ],
            "conocer_candidato": [
                "quien es", "qu√© es", "c√≥mo funciona", "propuestas",
                "candidato", "pol√≠ticas", "obras", "programas", "plan de gobierno"
            ],
            "cita_campa√±a": [
                "cita", "reuni√≥n", "encuentro", "agendar", "visitar", "conocer",
                "hablar", "conversar", "entrevista"
            ],
            "publicidad_info": [
                "folleto", "material", "publicidad", "difusi√≥n", "propaganda",
                "informaci√≥n", "brochure", "panfleto"
            ],
            "colaboracion_voluntariado": [
                "voluntario", "ayudar", "colaborar", "trabajar", "participar",
                "unirme", "apoyar", "contribuir"
            ],
            "quejas": [
                "queja", "reclamo", "problema", "mal servicio", "no funciona",
                "error", "falla", "defecto"
            ],
            "registration_response": [
                "me llamo", "mi nombre es", "soy", "vivo en", "mi ciudad es",
                "mi tel√©fono es", "mi email es"
            ]
        }
        
        # Buscar coincidencias exactas primero
        for category, pattern_list in patterns.items():
            for pattern in pattern_list:
                if pattern in message_lower:
                    return {
                        "category": category,
                        "confidence": 0.9,
                        "original_message": message,
                        "reason": f"Pattern match: {pattern}"
                    }
        
        # Buscar coincidencias parciales
        for category, pattern_list in patterns.items():
            for pattern in pattern_list:
                pattern_words = pattern.split()
                if any(word in message_lower for word in pattern_words):
                    return {
                        "category": category,
                        "confidence": 0.7,
                        "original_message": message,
                        "reason": f"Partial pattern match: {pattern}"
                    }
        
        # Si no hay coincidencias, usar fallback inteligente
        return {
            "category": "conocer_candidato",  # Fallback m√°s com√∫n
            "confidence": 0.3,
            "original_message": message,
            "reason": "No pattern match, using fallback"
        }
    
    def _cleanup_intent_cache(self):
        """Limpia autom√°ticamente el cach√© de intenciones (TTL + tama√±o)"""
        current_time = time.time()
        
        # Limpiar por TTL (5 minutos)
        expired_keys = []
        for key, value in self._intent_cache.items():
            if current_time - value.get("timestamp", 0) > 300:  # 5 minutos
                expired_keys.append(key)
        
        for key in expired_keys:
            del self._intent_cache[key]
        
        # Limpiar por tama√±o si es necesario
        if len(self._intent_cache) >= self._intent_cache_max_size:
            # Eliminar el 20% m√°s antiguo
            sorted_items = sorted(self._intent_cache.items(), key=lambda x: x[1].get("timestamp", 0))
            keys_to_remove = [key for key, _ in sorted_items[:self._intent_cache_max_size // 5]]
            for key in keys_to_remove:
                del self._intent_cache[key]
    
    def _fallback_intent_classification(self, message: str, context: Dict[str, Any] = None) -> str:
        """
        Clasificaci√≥n de fallback cuando Gemini est√° bloqueado por safety filters
        Solo para casos muy espec√≠ficos y obvios - confiar en Gemini para el resto
        
        Args:
            message: Mensaje a clasificar
            context: Contexto adicional (ej: estado del usuario, tipo de conversaci√≥n)
            
        Returns:
            Categor√≠a detectada
        """
        message_lower = message.lower().strip()
        
        # Solo detectar casos muy obvios y espec√≠ficos
        if message_lower in ["hola", "buenos d√≠as", "buenas tardes", "buenas noches", "gracias"]:
            return "saludo_apoyo"
        
        # Detectar preguntas sobre casos espec√≠ficos, propuestas, pol√≠ticas
        political_question_patterns = [
            "que es", "qu√© es", "quien es", "qui√©n es", "como funciona", "c√≥mo funciona",
            "caso", "propuesta", "pol√≠tica", "obra", "proyecto"
        ]
        
        for pattern in political_question_patterns:
            if pattern in message_lower:
                return "conocer_candidato"
        
        # Detectar explicaciones sobre datos disponibles (muy espec√≠fico)
        if self._looks_like_data_explanation(message):
            return "registration_response"
        
        # Detectar respuestas de registro basadas en contexto espec√≠fico
        if context and context.get("user_state") == "WAITING_NAME":
            if self._analyze_registration_intent(message, "name"):
                return "registration_response"
        
        if context and context.get("user_state") == "WAITING_LASTNAME":
            if self._analyze_registration_intent(message, "lastname"):
                return "registration_response"

        if context and context.get("user_state") == "WAITING_CITY":
            if self._analyze_registration_intent(message, "city"):
                return "registration_response"
        
        # Para todo lo dem√°s, confiar en que Gemini maneje la clasificaci√≥n correctamente
        # Si llegamos aqu√≠, significa que Gemini fall√≥, as√≠ que usar conocer_candidato como fallback
        return "conocer_candidato"
    
    def _looks_like_data_explanation(self, message: str) -> bool:
        """
        Detecta si un mensaje es una explicaci√≥n sobre qu√© datos puede proporcionar el usuario
        
        Args:
            message: Mensaje a analizar
            
        Returns:
            True si parece ser una explicaci√≥n sobre datos disponibles
        """
        message_lower = message.lower().strip()
        
        # Patrones que indican explicaciones sobre datos disponibles
        explanation_patterns = [
            "puedo solo", "solo puedo", "solo tengo", "solo dispongo",
            "solo me permite", "solo me deja", "solo me da",
            "un nombre y un apellido", "nombre y apellido", "solo nombre", "solo apellido",
            "no tengo m√°s", "no tengo otros", "no tengo m√°s datos", "no tengo m√°s informaci√≥n",
            "solo eso", "nada m√°s", "eso es todo", "eso es lo que tengo",
            "me permite solo", "me deja solo", "me da solo", "me da √∫nicamente"
        ]
        
        # Verificar si contiene alguno de los patrones
        for pattern in explanation_patterns:
            if pattern in message_lower:
                return True
        
        # Verificar si contiene palabras clave de datos + palabras de limitaci√≥n
        data_words = ["nombre", "apellido", "ciudad", "direcci√≥n", "tel√©fono", "email", "datos", "informaci√≥n"]
        limitation_words = ["solo", "√∫nicamente", "solamente", "nada m√°s", "eso es todo", "no tengo m√°s"]
        
        has_data_word = any(word in message_lower for word in data_words)
        has_limitation_word = any(word in message_lower for word in limitation_words)
        
        if has_data_word and has_limitation_word:
            return True
        
        return False
    
    def _is_data_explanation(self, message: str) -> bool:
        """
        Detecta si un mensaje es una explicaci√≥n sobre qu√© datos puede proporcionar el usuario
        
        Args:
            message: Mensaje a analizar
            
        Returns:
            True si es una explicaci√≥n sobre datos disponibles
        """
        return self._looks_like_data_explanation(message)
    
    def _contains_non_data_indicators(self, message: str) -> bool:
        """
        Detecta si un mensaje contiene palabras que indican que NO es un dato v√°lido
        
        Args:
            message: Mensaje a analizar
            
        Returns:
            True si contiene indicadores de que no es un dato v√°lido
        """
        message_lower = message.lower().strip()
        
        # Palabras que indican que NO es un dato v√°lido
        non_data_indicators = [
            "ok", "okey", "okay", "listo", "bien", "si", "no", "tal vez",
            "hola", "buenos", "buenas", "saludos", "gracias", "por favor",
            "disculpa", "perdon", "lo siento", "entendido", "comprendo",
            "vale", "perfecto", "excelente", "claro", "obvio", "seguro",
            "por supuesto", "naturalmente", "exacto", "correcto", "asi es",
            "como", "que", "cual", "donde", "cuando", "por que", "para que",
            "quiero", "necesito", "me gustaria", "puedo", "soy", "tengo",
            "no entiendo", "no se", "no tengo", "no puedo", "no me deja",
            "problema", "error", "falla", "no funciona", "ayuda"
        ]
        
        return any(indicator in message_lower for indicator in non_data_indicators)
    
    def _generate_explanation_response(self, data_type: str, message: str) -> str:
        """
        Genera una respuesta inteligente cuando el usuario explica qu√© datos puede proporcionar
        
        Args:
            data_type: Tipo de dato esperado
            message: Mensaje del usuario
            
        Returns:
            Respuesta generada
        """
        if data_type.lower() == "name":
            return "Entiendo perfectamente. No te preocupes, puedes proporcionar solo el nombre que tengas disponible. ¬øCu√°l es tu nombre?"
        elif data_type.lower() == "lastname":
            return "Perfecto, entiendo que tienes limitaciones con los datos. ¬øCu√°l es tu apellido?"
        elif data_type.lower() == "city":
            return "No hay problema, entiendo tu situaci√≥n. ¬øEn qu√© ciudad vives?"
        else:
            return "Entiendo tu situaci√≥n. Por favor, proporciona la informaci√≥n que tengas disponible."
    
    def _generate_clarification_response(self, data_type: str) -> str:
        """
        Genera una respuesta para aclarar qu√© tipo de dato se espera
        
        Args:
            data_type: Tipo de dato esperado
            
        Returns:
            Respuesta de aclaraci√≥n
        """
        if data_type.lower() == "name":
            return "Por favor, proporciona tu nombre completo. Por ejemplo: 'Juan Carlos' o 'Mar√≠a'"
        elif data_type.lower() == "lastname":
            return "Por favor, proporciona tu apellido. Por ejemplo: 'Garc√≠a' o 'Rodr√≠guez'"
        elif data_type.lower() == "city":
            return "Por favor, proporciona el nombre de tu ciudad. Por ejemplo: 'Bogot√°' o 'Medell√≠n'"
        else:
            return "Por favor, proporciona la informaci√≥n solicitada."
    
    def _analyze_registration_intent(self, message: str, data_type: str) -> bool:
        """
        An√°lisis ultra-r√°pido de intenci√≥n de registro
        
        Args:
            message: Mensaje a analizar
            data_type: Tipo de dato esperado ("name", "lastname", "city")
            
        Returns:
            True si el mensaje tiene la INTENCI√ìN de proporcionar datos de registro
        """
        message_lower = message.lower().strip()
        
        # üîß OPTIMIZACI√ìN: Detecci√≥n ultra-r√°pida de palabras comunes que NO son datos
        non_data_words = ["ok", "listo", "bien", "si", "no", "hola", "gracias", "vale", "claro", "como", "que", "cual"]
        if any(word in message_lower for word in non_data_words):
            return False
        
        # üîß OPTIMIZACI√ìN: Detecci√≥n ultra-r√°pida de explicaciones sobre datos
        if self._looks_like_data_explanation(message):
            return True
        
        # üîß OPTIMIZACI√ìN: Detecci√≥n ultra-r√°pida de nombres comunes
        if data_type == "name":
            common_names = ["santiago", "juan", "maria", "carlos", "ana", "luis", "pedro", "sofia", "diego", "camila"]
            if any(name in message_lower for name in common_names):
                return True
        
        # üîß OPTIMIZACI√ìN: Detecci√≥n ultra-r√°pida de ciudades comunes
        if data_type == "city":
            common_cities = ["bogot√°", "medell√≠n", "cali", "barranquilla", "cartagena", "bucaramanga", "pereira", "santa marta"]
            if any(city in message_lower for city in common_cities):
                return True
        
        # üîß OPTIMIZACI√ìN: Si es una frase corta sin palabras comunes, probablemente es un dato
        words = message.split()
        if len(words) <= 3 and "?" not in message:
            return True
        
        return False
    
    # def _looks_like_name_response(self, message: str) -> bool:
    #     """
    #     Detecta si un mensaje parece ser una respuesta de nombre
    #     M√âTODO NO SE USA - COMENTADO
    #     """
    #     message_lower = message.lower().strip()
    #     
    #     # Palabras que indican que NO es un nombre (lista expandida)
    #     not_name_indicators = [
    #         "hola", "buenos", "buenas", "saludos", "como", "que", "cual", 
    #         "donde", "cuando", "por que", "quiero", "necesito", "me gustaria",
    #         "puedo", "soy", "mi nombre es", "me llamo", "soy de", "vivo en",
    #         "ok", "okey", "okay", "listo", "bien", "si", "no", "tal vez",
    #         "gracias", "por favor", "disculpa", "perdon", "lo siento",
    #         "entendido", "comprendo", "vale", "perfecto", "excelente",
    #         "claro", "obvio", "seguro", "por supuesto", "naturalmente"
    #     ]
    #     
    #     if any(indicator in message_lower for indicator in not_name_indicators):
    #         return False
    #     
    #     # Si contiene palabras como "nombre", "apellido", "solo" - probablemente es una respuesta de datos
    #     data_indicators = [
    #         "nombre", "apellido", "solo", "un", "una", "dos", "tres", "varias"
    #     ]
    #     
    #     if any(indicator in message_lower for indicator in data_indicators):
    #         return True
    #     
    #     # Si es una frase corta (1-4 palabras) sin signos de interrogaci√≥n Y no contiene palabras comunes
    #     words = message.split()
    #     if len(words) <= 4 and "?" not in message:
    #         # Verificar que no sean palabras muy comunes
    #         common_words = ["ok", "listo", "bien", "si", "no", "hola", "gracias", "vale", "claro"]
    #         if not any(word.lower() in common_words for word in words):
    #             return True
    #     
    #     return False
    
    # def _looks_like_lastname_response(self, message: str) -> bool:
    #     """
    #     Detecta si un mensaje parece ser una respuesta de apellido
    #     M√âTODO NO SE USA - COMENTADO
    #     """
    #     message_lower = message.lower().strip()
    #     
    #     # Palabras que indican que NO es un apellido (lista expandida)
    #     not_lastname_indicators = [
    #         "hola", "buenos", "buenas", "saludos", "como", "que", "cual", 
    #         "donde", "cuando", "por que", "quiero", "necesito", "me gustaria",
    #         "puedo", "soy", "mi apellido es", "me apellido", "soy de", "vivo en",
    #         "ok", "okey", "okay", "listo", "bien", "si", "no", "tal vez",
    #         "gracias", "por favor", "disculpa", "perdon", "lo siento",
    #         "entendido", "comprendo", "vale", "perfecto", "excelente",
    #         "claro", "obvio", "seguro", "por supuesto", "naturalmente"
    #     ]
    #     
    #     if any(indicator in message_lower for indicator in not_lastname_indicators):
    #         return False
    #     
    #     # Si contiene palabras como "apellido", "solo" - probablemente es una respuesta de datos
    #     data_indicators = [
    #         "apellido", "solo", "un", "una", "dos", "tres", "varias"
    #     ]
    #     
    #     if any(indicator in message_lower for indicator in data_indicators):
    #         return True
    #     
    #     # Si es una frase corta (1-3 palabras) sin signos de interrogaci√≥n Y no contiene palabras comunes
    #     words = message.split()
    #     if len(words) <= 3 and "?" not in message:
    #         # Verificar que no sean palabras muy comunes
    #         common_words = ["ok", "listo", "bien", "si", "no", "hola", "gracias", "vale", "claro"]
    #         if not any(word.lower() in common_words for word in words):
    #             return True
    #     
    #     return False
    
    # def _looks_like_city_response(self, message: str) -> bool:
    #     """
    #     Detecta si un mensaje parece ser una respuesta de ciudad
    #     M√âTODO NO SE USA - COMENTADO
    #     """
    #     message_lower = message.lower().strip()
    #     
    #     # Palabras que indican que NO es una ciudad (lista expandida)
    #     not_city_indicators = [
    #         "hola", "buenos", "buenas", "saludos", "como", "que", "cual", 
    #         "donde", "cuando", "por que", "quiero", "necesito", "me gustaria",
    #         "puedo", "soy", "mi ciudad es", "vivo en", "soy de",
    #         "ok", "okey", "okay", "listo", "bien", "si", "no", "tal vez",
    #         "gracias", "por favor", "disculpa", "perdon", "lo siento",
    #         "entendido", "comprendo", "vale", "perfecto", "excelente",
    #         "claro", "obvio", "seguro", "por supuesto", "naturalmente"
    #     ]
    #     
    #     if any(indicator in message_lower for indicator in not_city_indicators):
    #         return False
    #     
    #     # Si contiene palabras como "ciudad", "vivo", "soy de" - probablemente es una respuesta de datos
    #     data_indicators = [
    #         "ciudad", "vivo", "soy de", "estoy en", "resido en", "habito en"
    #     ]
    #     
    #     if any(indicator in message_lower for indicator in data_indicators):
    #         return True
    #     
    #     # Si es una frase corta (1-3 palabras) sin signos de interrogaci√≥n
    #     words = message.split()
    #     if len(words) <= 3 and "?" not in message:
    #         return True
    #     
    #     return False
    
    # def _looks_like_data_explanation(self, message: str) -> bool:
    #     """
    #     Detecta si un mensaje es una explicaci√≥n sobre qu√© datos puede proporcionar el usuario
    #     M√âTODO DUPLICADO - NO SE USA
    #     """
    #     message_lower = message.lower().strip()
    #     
    #     # Patrones que indican explicaciones sobre datos disponibles
    #     explanation_patterns = [
    #         "puedo solo", "solo puedo", "solo tengo", "solo tengo", "solo dispongo",
    #         "solo me permite", "solo me deja", "solo me da", "solo me da",
    #         "un nombre y un apellido", "nombre y apellido", "solo nombre", "solo apellido",
    #         "no tengo m√°s", "no tengo otros", "no tengo m√°s datos", "no tengo m√°s informaci√≥n",
    #         "solo eso", "nada m√°s", "eso es todo", "eso es lo que tengo",
    #         "me permite solo", "me deja solo", "me da solo", "me da √∫nicamente"
    #     ]
    #     
    #     # Verificar si contiene alguno de los patrones
    #     for pattern in explanation_patterns:
    #         if pattern in message_lower:
    #             return True
    #     
    #     # Verificar si contiene palabras clave de datos + palabras de limitaci√≥n
    #     data_words = ["nombre", "apellido", "ciudad", "direcci√≥n", "tel√©fono", "email", "datos", "informaci√≥n"]
    #     limitation_words = ["solo", "√∫nicamente", "solamente", "nada m√°s", "eso es todo", "no tengo m√°s"]
    #     
    #     has_data_word = any(word in message_lower for word in data_words)
    #     has_limitation_word = any(word in message_lower for word in limitation_words)
    #     
    #     if has_data_word and has_limitation_word:
    #         return True
    #     
    #     return False
    
    async def _extract_with_ai(self, message: str, data_type: str) -> Dict[str, Any]:
        """Extrae datos usando IA"""
        self._ensure_model_initialized()
        if not self.model:
            return {}
        
        try:
            prompt = f"""
            Extrae el {data_type} del siguiente mensaje:
            Mensaje: "{message}"
            
            Responde solo con el {data_type} encontrado, sin explicaciones adicionales.
            Si no se encuentra, responde con "no_encontrado".
            """
            
            # [COHETE] FASE 2: Usar configuraci√≥n optimizada para extracci√≥n de datos
            response_text = await self._generate_content(prompt, task_type="data_extraction")
            extracted_value = response_text.strip()
            
            if extracted_value.lower() == "no_encontrado":
                return {}
            
            return {data_type: extracted_value}
            
        except Exception as e:
            logger.error(f"Error extrayendo con IA: {str(e)}")
            return {}
    
    def _basic_validation(self, data: str, data_type: str) -> bool:
        """Validaci√≥n b√°sica de datos"""
        if not data or not data.strip():
            return False
        
        data = data.strip()
        
        if data_type.lower() in ["name", "lastname"]:
            # Validar nombres y apellidos m√°s estrictamente
            # - Solo letras, espacios, guiones y apostrofes
            # - M√≠nimo 2 caracteres, m√°ximo 50
            # - No puede empezar o terminar con espacios
            # - No puede tener espacios m√∫ltiples
            if len(data) < 2 or len(data) > 50:
                return False
            
            # Verificar caracteres v√°lidos
            if not all(c.isalpha() or c.isspace() or c in "-'" for c in data):
                return False
            
            # Verificar que tenga al menos una letra
            if not any(c.isalpha() for c in data):
                return False
            
            # No puede empezar o terminar con espacios
            if data.startswith(' ') or data.endswith(' '):
                return False
            
            # No puede tener espacios m√∫ltiples consecutivos
            if '  ' in data:
                return False
            
            # Verificar que no sea solo espacios y caracteres especiales
            clean_data = data.replace(' ', '').replace('-', '').replace("'", '')
            if not clean_data.isalpha() or len(clean_data) < 1:
                return False
            
            return True
        
        if data_type.lower() == "city":
            # Validar ciudades m√°s estrictamente
            # - Solo letras, espacios, guiones, apostrofes y puntos
            # - M√≠nimo 2 caracteres, m√°ximo 100
            # - Debe tener al menos una letra
            if len(data) < 2 or len(data) > 100:
                return False
            
            # Verificar caracteres v√°lidos
            if not all(c.isalpha() or c.isspace() or c in "-'." for c in data):
                return False
            
            # Verificar que tenga al menos una letra
            if not any(c.isalpha() for c in data):
                return False
            
            # No puede empezar o terminar con espacios
            if data.startswith(' ') or data.endswith(' '):
                return False
            
            # No puede tener espacios m√∫ltiples consecutivos
            if '  ' in data:
                return False
            
            # Verificar que no sea solo espacios y caracteres especiales
            clean_data = data.replace(' ', '').replace('-', '').replace("'", '').replace('.', '')
            if not clean_data.isalpha() or len(clean_data) < 1:
                return False
            
            return True
        
        if data_type.lower() == "phone":
            # Validar tel√©fonos (n√∫meros y +)
            return data.replace("+", "").replace("-", "").replace(" ", "").isdigit() and len(data.replace("+", "").replace("-", "").replace(" ", "")) >= 10
        
        return True  # Para otros tipos, aceptar por defecto
    
    async def _analyze_registration_with_ai(self, text: str, state: str, user_context: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Usa IA para analizar el contexto completo y extraer datos de registro"""
        self._ensure_model_initialized()
        if not self.model:
            return None
        
        try:
            # Obtener contexto de la sesi√≥n si est√° disponible
            session_context = ""
            try:
                session = session_context_service.get_session(session_id)
                if session:
                    session_context = session_context_service.build_context_for_ai(session_id)
            except Exception as e:
                logger.warning(f"Error obteniendo contexto de sesi√≥n: {str(e)}")
            
            # Construir prompt con contexto completo
            prompt = f"""
Eres un asistente inteligente que analiza mensajes de usuarios durante un proceso de registro.

CONTEXTO DE LA CONVERSACI√ìN:
{session_context}

ESTADO ACTUAL DEL USUARIO: {state}

MENSAJE DEL USUARIO: "{text}"

TAREA: Analiza el mensaje y determina:
1. Si es una pregunta o solicitud de informaci√≥n (type: "info")
2. Si contiene un nombre completo (type: "name")
3. Si contiene un apellido (type: "lastname") 
4. Si contiene una ciudad/ubicaci√≥n (type: "city")
5. Si es otra cosa (type: "other")

INSTRUCCIONES ESPEC√çFICAS:
- Para nombres: Extrae el nombre completo, incluso si viene despu√©s de palabras como "listo", "ok", "mi nombre es", etc.
- Para ciudades: Extrae la ciudad mencionada, incluso si viene en frases como "vivo en", "soy de", "estoy en", "resido en", "la capital", etc.
- Si el usuario hace una pregunta, clasifica como "info"
- Si el usuario explica limitaciones (ej: "solo puedo dar nombre y apellido"), clasifica como "info"
- Considera el contexto de la conversaci√≥n anterior
- S√© inteligente para entender frases naturales como "listo, mi nombre es Pepito Perez"
- PRIORIDAD: Si el estado es WAITING_CITY y el mensaje contiene informaci√≥n de ubicaci√≥n, clasifica como "city"
- PRIORIDAD: Si el estado es WAITING_LASTNAME y el mensaje contiene apellidos, clasifica como "lastname"

EJEMPLOS:
- "listo, mi nombre es Pepito Perez Mora" -> type: "name", value: "Pepito Perez Mora"
- "ok, es Pepito Perez" -> type: "name", value: "Pepito Perez"
- "Te lo escribi antes Campos P" -> type: "lastname", value: "Campos P"
- "Si ese es mi apellido" -> type: "lastname", value: "Campos P" (si se mencion√≥ antes)
- "vivo en Bogot√°" -> type: "city", value: "Bogot√°"
- "vivo en la capital" -> type: "city", value: "Bogot√°" (si es Colombia)
- "solo puedo dar nombre y apellido" -> type: "info", value: null
- "no tengo ciudad" -> type: "info", value: null
- "¬øc√≥mo funciona esto?" -> type: "info", value: null
- "soy de Medell√≠n" -> type: "city", value: "Medell√≠n"
- "estoy en Cali" -> type: "city", value: "Cali"
- "resido en Barranquilla" -> type: "city", value: "Barranquilla"
- "?C√≥mo funciona esto?" -> type: "info", value: null
- "Pepito" -> type: "name", value: "Pepito"

Responde SOLO con un JSON v√°lido en este formato:
{{"type": "name|lastname|city|info|other", "value": "valor_extraido_o_null", "confidence": 0.0-1.0}}
"""

            response_text = await self._generate_content(prompt)
            logger.info(f"Respuesta cruda de IA: '{response_text}'")
            
            # Parsear respuesta JSON
            import json
            import re
            
            try:
                # Limpiar la respuesta - extraer solo el JSON
                cleaned_response = response_text.strip()
                
                # Buscar JSON en la respuesta usando regex
                json_match = re.search(r'\{[^}]*"type"[^}]*\}', cleaned_response)
                if json_match:
                    cleaned_response = json_match.group(0)
                
                # Si no hay JSON v√°lido, intentar parsear toda la respuesta
                if not cleaned_response.startswith('{'):
                    logger.warning(f"Respuesta no contiene JSON v√°lido: '{response_text}'")
                    return None
                
                result = json.loads(cleaned_response)
                
                # Validar resultado
                valid_types = ["name", "lastname", "city", "info", "other"]
                if result.get("type") in valid_types:
                    logger.info(f"IA analiz√≥ registro: {result}")
                    return result
                else:
                    logger.warning(f"IA devolvi√≥ tipo inv√°lido: {result}")
                    return None
                    
            except json.JSONDecodeError as e:
                logger.error(f"Error parseando JSON de IA: {str(e)}")
                logger.error(f"Respuesta que caus√≥ el error: '{response_text}'")
                return None
                
        except Exception as e:
            logger.error(f"Error analizando registro con IA: {str(e)}")
            return None

    # async def _analyze_city_with_ai(self, text: str) -> Dict[str, Any]:
    #     """Usa IA para analizar si un texto contiene informaci√≥n de ciudad y extraerla"""
    #     # M√âTODO NO SE USA - COMENTADO
    #     self._ensure_model_initialized()
    #     if not self.model:
    #         return {"is_city": False, "extracted_city": None, "confidence": 0.0}
    #     
    #     try:
    #         prompt = f"""
    #         Analiza el siguiente texto y determina si contiene informaci√≥n sobre una ciudad o ubicaci√≥n.
    #         
    #         Texto: "{text}"
    #         
    #         Instrucciones:
    #         1. Si el texto menciona una ciudad, pa√≠s, o ubicaci√≥n geogr√°fica, responde "SI"
    #         2. Si el texto NO menciona ubicaci√≥n geogr√°fica, responde "NO"
    #         3. Si es "SI", extrae la informaci√≥n completa de ubicaci√≥n
    #         4. Si menciona pa√≠s Y ciudad, extrae la frase completa
    #         5. Si solo menciona ciudad, extrae solo la ciudad
    #         6. IMPORTANTE: Para frases como "en espa√±a, en madrid", extrae la ciudad espec√≠fica (madrid)
    #         7. Para frases como "vivo en espa√±a, en madrid", extrae "madrid" como ciudad
    #         
    #         Ejemplos:
    #         - "vivo en espa√±a, en madrid" -> SI, ciudad: "madrid"
    #         - "soy de bogot√°" -> SI, ciudad: "bogot√°"
    #         - "estoy en medell√≠n" -> SI, ciudad: "medell√≠n"
    #         - "en espa√±a, madrid" -> SI, ciudad: "madrid"
    #         - "en madrid, espa√±a" -> SI, ciudad: "madrid"
    #         - "hola" -> NO
    #         - "mi nombre es juan" -> NO
    #         
    #         Responde en formato: SI|ciudad o NO
    #         """
    #         
    #         # [COHETE] FASE 2: Usar configuraci√≥n optimizada para normalizaci√≥n de ubicaciones
    #         response_text = await self._generate_content(prompt, task_type="location_normalization")
    #         result = response_text.strip()
    #         
    #         if result.startswith("SI|"):
    #             city = result.split("|", 1)[1].strip()
    #             logger.info(f"IA detect√≥ ciudad: '{city}' en texto: '{text}'")
    #             return {
    #                 "is_city": True,
    #                 "extracted_city": city,
    #                 "confidence": 0.8
    #             }
    #         else:
    #             logger.info(f"IA no detect√≥ ciudad en texto: '{text}'")
    #             return {
    #                 "is_city": False,
    #                 "extracted_city": None,
    #                 "confidence": 0.0
    #             }
    #             
    #     except Exception as e:
    #         logger.error(f"Error analizando ciudad con IA: {str(e)}")
    #         return {"is_city": False, "extracted_city": None, "confidence": 0.0}

    async def _validate_with_ai(self, data: str, data_type: str) -> bool:
        """Validaci√≥n r√°pida con IA - optimizada para velocidad"""
        self._ensure_model_initialized()
        if not self.model:
            return True
        
        try:
            # Prompt optimizado y conciso
            prompt = f"¬øEs '{data}' un {data_type} v√°lido? Responde: SI o NO"
            
            response_text = await self._generate_content(prompt, task_type="data_validation")
            return response_text.strip().upper() == "SI"
            
        except Exception as e:
            logger.error(f"Error en validaci√≥n IA: {str(e)}")
            return True
    
    def _build_chat_prompt(self, query: str, user_context: Dict[str, Any], 
                          branding_config: Dict[str, Any], relevant_context: str = "") -> str:
        """Construye el prompt para chat"""
        contact_name = branding_config.get("contactName", "el candidato")
        welcome_message = branding_config.get("welcomeMessage", "!Hola! ?En qu√© puedo ayudarte?")
        
        context_info = ""
        if user_context.get("user_name"):
            context_info += f"El usuario se llama {user_context['user_name']}. "
        if user_context.get("user_city"):
            context_info += f"Vive en {user_context['user_city']}. "
        if user_context.get("user_country"):
            context_info += f"Pa√≠s: {user_context['user_country']}. "
        if user_context.get("user_state"):
            context_info += f"Estado actual: {user_context['user_state']}. "
        if user_context.get("user_phone"):
            context_info += f"Tel√©fono: {user_context['user_phone']}. "
        if user_context.get("conversation_count"):
            context_info += f"Es su conversaci√≥n #{user_context['conversation_count']}. "
        
        # Detectar si es un saludo y el usuario est√° en proceso de registro
        user_state = user_context.get("user_state", "")
        is_greeting = query.lower().strip() in ["hola", "hi", "hello", "hey", "buenos d√≠as", "buenas tardes", "buenas noches", "qu√© tal", "que tal"]
        
        # Construir contexto de documentos si est√° disponible
        document_context_section = ""
        if relevant_context:
            document_context_section = f"""
            
            INFORMACI√ìN ESPEC√çFICA DE LA CAMPA√ëA:
            {relevant_context}
            
            Usa esta informaci√≥n espec√≠fica para responder preguntas sobre la campa√±a, propuestas, 
            eventos, pol√≠ticas, o cualquier tema relacionado con el candidato y su plataforma.
            """
        
        if user_state == "WAITING_NAME" and is_greeting:
            prompt = f"""
            Asistente virtual para la campa√±a pol√≠tica de {contact_name}.
            
            El usuario acaba de saludar y est√° en proceso de registro (necesita dar su nombre).
            
            Responde el saludo de manera amigable y entusiasta, pero inmediatamente pide su nombre para continuar con el registro.
            
            Contexto: El usuario est√° en proceso de registro y necesita proporcionar su nombre.
            {document_context_section}
            
            Saludo del usuario: "{query}"
            
            Responde de manera amigable, motivadora y natural. Responde el saludo pero pide inmediatamente el nombre para continuar con el registro. Usa emojis apropiados y un tono positivo.
            
            Respuesta:
            """
        else:
            prompt = f"""
            Asistente virtual para la campa√±a pol√≠tica de {contact_name}.
            
            Tu objetivo es motivar la participaci√≥n activa en la campa√±a de manera natural y entusiasta. 
            Integra sutilmente estos elementos motivacionales en tus respuestas:
            
            - Inspirar sentido de prop√≥sito y pertenencia a un movimiento transformador
            - Mostrar oportunidades de crecimiento, logros y reconocimiento
            - Invitar a la colaboraci√≥n y participaci√≥n activa
            - Crear sensaci√≥n de comunidad y trabajo en equipo
            - Generar expectativa y curiosidad sobre oportunidades exclusivas
            - Destacar el impacto y la importancia de cada acci√≥n
            
            SISTEMA DE PUNTOS Y RANKING:
            - Cada referido registrado suma 50 puntos
            - Retos semanales dan puntaje adicional
            - Ranking actualizado a nivel ciudad, departamento y pa√≠s
            - Los usuarios pueden preguntar "?C√≥mo voy?" para ver su progreso
            - Para invitar personas: "mandame el link" o "dame mi c√≥digo"
            
            CONTEXTO COMPLETO DEL USUARIO: {context_info}{document_context_section}
            
            Mensaje del usuario: "{query}"
            
            INSTRUCCIONES PERSONALIZADAS:
            1. **PERSONALIZA** tu respuesta usando el nombre del usuario si est√° disponible
            2. **MENCI√ìN** su ciudad si es relevante para la respuesta
            3. Responde de manera amigable, motivadora y natural
            4. Si el usuario est√° en proceso de registro, ay√∫dale a completarlo
            5. Si tiene preguntas sobre la campa√±a, responde con informaci√≥n relevante y oportunidades de participaci√≥n
            6. Usa la informaci√≥n espec√≠fica de la campa√±a cuando sea apropiado
            7. Usa emojis apropiados y un tono positivo
            8. Mant√©n la respuesta concisa, m√°ximo 999 caracteres
            
            Respuesta:
            """
        
        return prompt
    
    async def generate_response(self, prompt: str, role: str = "user") -> str:
        """
        Genera una respuesta usando IA con un prompt personalizado
        
        Args:
            prompt: Prompt para la IA
            role: Rol del usuario (user, system, assistant)
            
        Returns:
            Respuesta generada por la IA
        """
        self._ensure_model_initialized()
        
        if not self.model:
            return "Lo siento, el servicio de IA no est√° disponible."
        
        try:
            response_text = await self._generate_content(prompt)
            return response_text if response_text else ""
            
        except Exception as e:
            logger.error(f"Error generando respuesta con IA: {str(e)}")
            return "Error generando respuesta."

    async def detect_referral_code(self, tenant_id: str, message: str) -> Dict[str, Any]:
        """
        Detecta si un mensaje contiene un c√≥digo de referido usando IA
        
        Args:
            tenant_id: ID del tenant
            message: Mensaje del usuario
            
        Returns:
            Dict con c√≥digo detectado o None
        """
        self._ensure_model_initialized()
        
        if not self.model:
            return {
                "code": None,
                "reason": "Servicio de IA no disponible",
                "original_message": message
            }
        
        try:
            prompt = f"""
Analiza el siguiente mensaje y detecta si contiene un c√≥digo de referido.

Un c√≥digo de referido es:
- Una secuencia de exactamente 8 caracteres alfanum√©ricos (letras y n√∫meros)
- Puede estar en cualquier parte del mensaje
- NO es una palabra com√∫n del espa√±ol como "referido", "referida", "referir", etc.
- Ejemplos v√°lidos: "ABC12345", "TESTCODE", "USER1234"
- Ejemplos inv√°lidos: "REFERIDO", "REFERIDA", "referir"

Mensaje a analizar: "{message}"

Responde √öNICAMENTE con el c√≥digo de 8 caracteres si lo encuentras, o "NO" si no hay c√≥digo v√°lido.
Si hay m√∫ltiples c√≥digos, responde solo el primero que encuentres.

Ejemplos:
- "vengo referido por TESTCODE" -> TESTCODE
- "mi c√≥digo es ABC12345" -> ABC12345  
- "vengo referido por mi amigo" -> NO
- "hola REFERIDO" -> NO
"""

            response_text = await self._generate_content(prompt)
            detected_code = response_text.strip().upper()
            
            # Validar que el c√≥digo tiene exactamente 8 caracteres alfanum√©ricos
            if detected_code != "NO" and len(detected_code) == 8 and detected_code.isalnum():
                logger.info(f"C√≥digo de referido detectado por IA: {detected_code}")
                return {
                    "code": detected_code,
                    "reason": "C√≥digo detectado exitosamente",
                    "original_message": message
                }
            else:
                logger.info("No se detect√≥ c√≥digo de referido v√°lido")
                return {
                    "code": None,
                    "reason": "No se encontr√≥ c√≥digo v√°lido",
                    "original_message": message
                }
                
        except Exception as e:
            logger.error(f"Error detectando c√≥digo de referido: {str(e)}")
            return {
                "code": None,
                "reason": f"Error interno: {str(e)}",
                "original_message": message
            }
    
    async def _handle_malicious_behavior(self, query: str, user_context: Dict[str, Any], 
                                       tenant_id: str, confidence: float) -> str:
        """
        Maneja comportamiento malicioso detectado
        
        Args:
            query: Mensaje malicioso del usuario
            user_context: Contexto del usuario
            tenant_id: ID del tenant
            confidence: Confianza de la clasificaci√≥n
            
        Returns:
            Respuesta para el usuario malicioso
        """
        try:
            # Obtener informaci√≥n del usuario
            user_id = user_context.get("user_id", "unknown")
            phone_number = user_context.get("phone", "unknown")
            
            # Detectar tipo de comportamiento malicioso usando an√°lisis inteligente
            malicious_analysis = self._detect_malicious_intent(query)
            behavior_type = "intenci√≥n maliciosa inteligente"
            categories = malicious_analysis.get("categories", [])
            
            logger.warning(f"üö® {behavior_type.upper()} detectado - Usuario: {user_id}, Tenant: {tenant_id}, Confianza: {confidence:.2f}")
            logger.warning(f"üö® Categor√≠as detectadas: {categories}")
            logger.warning(f"üö® Mensaje malicioso: '{query}'")
            
            # Notificar al servicio Java para bloquear el usuario
            logger.info(f"üîî Enviando notificaci√≥n de bloqueo al servicio Java para usuario {user_id}")
            logger.info(f"üîî URL del servicio Java: {self.blocking_notification_service.java_service_url}")
            
            notification_result = await self.blocking_notification_service.notify_user_blocked(
                tenant_id=tenant_id,
                user_id=user_id,
                phone_number=phone_number,
                malicious_message=query,
                classification_confidence=confidence
            )
            
            logger.info(f"üîî Resultado de notificaci√≥n: {notification_result}")
            
            # Registrar el incidente
            await self.blocking_notification_service.log_malicious_incident(
                tenant_id=tenant_id,
                user_id=user_id,
                phone_number=phone_number,
                malicious_message=query,
                classification_confidence=confidence
            )
            
            if notification_result.get("success"):
                logger.info(f"[OK] Usuario {user_id} bloqueado exitosamente en WATI y base de datos")
            else:
                logger.error(f"[ERROR] Error bloqueando usuario {user_id}: {notification_result.get('error')}")
                logger.error(f"[ERROR] Detalles del error: {notification_result}")
            
            # No responder nada cuando es malicioso, solo bloquear silenciosamente
            return ""
            
        except Exception as e:
            logger.error(f"Error manejando comportamiento malicioso: {str(e)}")
            return "Lo siento, no puedo procesar tu mensaje en este momento."
    
    def _handle_appointment_request_with_context(self, branding_config: Dict[str, Any], 
                                               tenant_config: Dict[str, Any], session_context: str = "") -> str:
        """Maneja solicitudes de citas con contexto de sesi√≥n"""
        contact_name = branding_config.get("contactName", "el candidato")
        calendly_link = tenant_config.get("link_calendly", "")
        
        # Si hay contexto de sesi√≥n, personalizar la respuesta
        if session_context:
            return f"""¬°Perfecto! Me alegra que quieras agendar una cita con {contact_name}. 
            
Puedes reservar tu cita directamente aqu√≠: {calendly_link}

Si tienes alguna pregunta espec√≠fica sobre la reuni√≥n o necesitas ayuda con el proceso, no dudes en preguntarme."""
        else:
            return f"""¬°Excelente! Para agendar una cita con {contact_name}, puedes usar este enlace: {calendly_link}"""
    
    def _get_greeting_response_with_context(self, branding_config: Dict[str, Any], session_context: str = "") -> str:
        """Genera saludo con contexto de sesi√≥n inteligente"""
        contact_name = branding_config.get("contactName", "el candidato")
        
        # Si hay contexto de sesi√≥n, generar respuesta contextual inteligente
        if session_context and len(session_context.strip()) > 50:
            # Usar IA para generar respuesta contextual basada en la conversaci√≥n anterior
            try:
                self._ensure_model_initialized()
                if self.model:
                    prompt = f"""
                    Asistente virtual de {contact_name}. El usuario acaba de enviar un saludo o respuesta corta como "ok", "hola", "gracias", etc.
                    
                    CONTEXTO DE LA CONVERSACI√ìN ANTERIOR:
                    {session_context}
                    
                    INSTRUCCIONES:
                    1. Genera una respuesta natural y contextual basada en la conversaci√≥n anterior
                    2. Si el usuario acababa de preguntar sobre propuestas, ofrece m√°s informaci√≥n espec√≠fica
                    3. Si el usuario acababa de agendar una cita, confirma o pregunta si necesita algo m√°s
                    4. Si es la primera interacci√≥n, da la bienvenida
                    5. Mant√©n un tono amigable y profesional
                    6. Responde en m√°ximo 200 caracteres
                    7. **PROHIBIDO**: NUNCA uses placeholders como [TU_ENLACE_PERSONAL_AQU√ç], [ENLACE], [LINK], etc.
                    8. **IMPORTANTE**: Responde solo con texto natural, sin enlaces ni placeholders
                    
                    Responde de manera natural y contextual:
                    """
                    
                    response = self.model.generate_content(prompt, safety_settings=self._get_safety_settings())
                    filtered_response = self._filter_links_from_response(response.text.strip())
                    return filtered_response
            except Exception as e:
                logger.warning(f"Error generando saludo contextual: {e}")
        
        # Fallback: respuesta gen√©rica
        if session_context:
            fallback_response = f"""¬°Hola! Me da mucho gusto verte de nuevo. ¬øEn qu√© m√°s puedo ayudarte hoy con informaci√≥n sobre {contact_name} y sus propuestas?"""
            return self._filter_links_from_response(fallback_response)
        else:
            fallback_response = f"""¬°Hola! Te doy la bienvenida a nuestra campa√±a: {contact_name}!!! 
            
¬øEn qu√© puedo ayudarte hoy? Puedo responder tus preguntas sobre nuestras propuestas, ayudarte a agendar una cita, o conectarte con nuestro equipo."""
            return self._filter_links_from_response(fallback_response)
    
    def _get_volunteer_response_with_context(self, branding_config: Dict[str, Any], session_context: str = "") -> str:
        """Genera respuesta de voluntariado con contexto de sesi√≥n"""
        contact_name = branding_config.get("contactName", "el candidato")
        
        # Si hay contexto de sesi√≥n, personalizar la respuesta
        if session_context:
            return f"""¬°Qu√© genial que quieras ser parte del equipo de {contact_name}! 
            
Tu apoyo es fundamental para el cambio que queremos lograr. Te puedo ayudar a conectarte con nuestro equipo de voluntarios o responder cualquier pregunta que tengas sobre c√≥mo participar."""
        else:
            return f"""¬°Excelente! {contact_name} valora mucho el apoyo de personas como t√∫. 
            
Te puedo ayudar a conectarte con nuestro equipo de voluntarios. ¬øTe gustar√≠a que te ayude a agendar una reuni√≥n o tienes alguna pregunta espec√≠fica sobre c√≥mo participar?"""


    async def validate_user_data(self, tenant_id: str, data: str, data_type: str) -> Dict[str, Any]:
        """
        Valida datos de usuario usando cache y IA
        
        Args:
            tenant_id: ID del tenant
            data: Datos a validar
            data_type: Tipo de dato (name, lastname, city, etc.)
            
        Returns:
            Dict con resultado de validaci√≥n
        """
        # üöÄ OPTIMIZACI√ìN: Verificar cache primero para datos comunes
        if data_type in self._validation_cache:
            data_lower = data.lower().strip()
            if data_lower in self._validation_cache[data_type]:
                logger.info(f"‚úÖ Cache hit para validaci√≥n {data_type}: '{data}' -> v√°lido")
                return self._validation_cache[data_type][data_lower]
        
        logger.info(f"üîç Cache miss para validaci√≥n {data_type}: '{data}' - usando IA")
        
        self._ensure_model_initialized()
        
        if not self.model:
            return {
                "is_valid": False,
                "confidence": 0.0,
                "reason": "Servicio de IA no disponible",
                "suggestions": []
            }
        
        try:
            # Crear prompt espec√≠fico seg√∫n el tipo de dato
            if data_type == "name":
                prompt = f"""
Analiza si el siguiente texto es un nombre v√°lido de persona:

Texto: "{data}"

Un nombre v√°lido debe:
- Contener solo letras, espacios, guiones, apostrofes y puntos
- Tener entre 2 y 50 caracteres
- NO contener n√∫meros (excepto en casos especiales como "Mar√≠a Jos√©")
- NO ser una palabra com√∫n del espa√±ol como "referido", "gracias", "hola", etc.
- NO ser un c√≥digo alfanum√©rico

Responde √öNICAMENTE "VALIDO" o "INVALIDO" seguido de la raz√≥n si es inv√°lido.

Ejemplos:
- "Juan" -> VALIDO
- "Mar√≠a Jos√©" -> VALIDO
- "Jos√© Mar√≠a" -> VALIDO
- "SANTIAGO" -> VALIDO
- "K351ERXL" -> INVALIDO (es un c√≥digo, no un nombre)
- "referido" -> INVALIDO (palabra com√∫n)
- "12345678" -> INVALIDO (solo n√∫meros)
"""
            elif data_type == "lastname":
                prompt = f"""
Analiza si el siguiente texto es un apellido v√°lido de persona:

Texto: "{data}"

Un apellido v√°lido debe:
- Contener solo letras, espacios, guiones, apostrofes y puntos
- Tener entre 2 y 50 caracteres
- NO contener n√∫meros (excepto en casos especiales)
- NO ser una palabra com√∫n del espa√±ol como "referido", "gracias", "hola", etc.
- NO ser un c√≥digo alfanum√©rico

Responde √öNICAMENTE "VALIDO" o "INVALIDO" seguido de la raz√≥n si es inv√°lido.

Ejemplos:
- "Garc√≠a" -> VALIDO
- "Garc√≠a L√≥pez" -> VALIDO
- "P√©rez" -> VALIDO
- "K351ERXL" -> INVALIDO (es un c√≥digo, no un apellido)
- "referido" -> INVALIDO (palabra com√∫n)
"""
            elif data_type == "city":
                prompt = f"""
Analiza si el siguiente texto es una ciudad v√°lida de Colombia:

Texto: "{data}"

Una ciudad v√°lida debe:
- Contener solo letras, espacios, guiones, apostrofes y puntos
- Tener entre 2 y 100 caracteres
- NO contener n√∫meros (excepto en casos especiales como "San Jos√© del Guaviare")
- NO ser una palabra com√∫n del espa√±ol como "referido", "gracias", "hola", etc.
- NO ser un c√≥digo alfanum√©rico
- Ser una ciudad real de Colombia

Responde √öNICAMENTE "VALIDO" o "INVALIDO" seguido de la raz√≥n si es inv√°lido.

Ejemplos:
- "Bogot√°" -> VALIDO
- "Medell√≠n" -> VALIDO
- "Cali" -> VALIDO
- "Soacha" -> VALIDO
- "K351ERXL" -> INVALIDO (es un c√≥digo, no una ciudad)
- "referido" -> INVALIDO (palabra com√∫n)
"""
            else:
                # Validaci√≥n gen√©rica
                prompt = f"""
Analiza si el siguiente texto es v√°lido para el tipo de dato "{data_type}":

Texto: "{data}"

El texto debe:
- Contener solo letras, espacios, guiones, apostrofes y puntos
- Tener entre 2 y 100 caracteres
- NO ser una palabra com√∫n del espa√±ol como "referido", "gracias", "hola", etc.
- NO ser un c√≥digo alfanum√©rico

Responde √öNICAMENTE "VALIDO" o "INVALIDO" seguido de la raz√≥n si es inv√°lido.
"""
            
            response_text = await self._generate_content(prompt)
            
            if not response_text:
                return {
                    "is_valid": False,
                    "confidence": 0.0,
                    "reason": "No se pudo obtener respuesta de la IA",
                    "suggestions": []
                }
            
            response_upper = response_text.strip().upper()
            
            if "VALIDO" in response_upper:
                return {
                    "is_valid": True,
                    "confidence": 0.9,
                    "reason": "Datos v√°lidos seg√∫n IA",
                    "suggestions": []
                }
            else:
                reason = response_text.strip()
                return {
                    "is_valid": False,
                    "confidence": 0.8,
                    "reason": reason,
                    "suggestions": []
                }
                
        except Exception as e:
            logger.error(f"Error validando datos con IA: {str(e)}")
            return {
                "is_valid": False,
                "confidence": 0.0,
                "reason": f"Error en validaci√≥n: {str(e)}",
                "suggestions": []
            }

    async def analyze_registration_message(self, tenant_id: str, message: str, user_context: Dict[str, Any], current_state: str) -> Dict[str, Any]:
        """
        Analiza un mensaje durante el proceso de registro
        """
        return await self.analyze_registration(tenant_id, message, user_context, current_state)

    async def _handle_malicious_message(self, tenant_id: str, query: str, user_context: Dict[str, Any], 
                                       malicious_detection: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """
        Maneja mensajes maliciosos detectados durante el proceso de registro
        
        Args:
            tenant_id: ID del tenant
            query: Mensaje malicioso del usuario
            user_context: Contexto del usuario
            malicious_detection: Resultado de la detecci√≥n de comportamiento malicioso
            session_id: ID de la sesi√≥n
            
        Returns:
            Respuesta de bloqueo o advertencia
        """
        try:
            confidence = malicious_detection.get("confidence", 0.0)
            reason = malicious_detection.get("reason", "Comportamiento inapropiado")
            
            logger.warning(f"üö´ Comportamiento malicioso detectado: confianza={confidence}, raz√≥n={reason}")
            
            # Obtener informaci√≥n del usuario para logging
            user_id = user_context.get("user_id", "unknown")
            user_name = user_context.get("user_name", "Usuario")
            user_state = user_context.get("user_state", "unknown")
            
            # Log del incidente malicioso
            await user_blocking_service.log_malicious_incident(
                tenant_id=tenant_id,
                user_id=user_id,
                phone_number=user_context.get("phone_number", ""),
                malicious_message=query,
                classification_confidence=confidence
            )
            
            # Determinar respuesta seg√∫n el nivel de malicia
            if confidence >= 0.9:
                # Comportamiento muy malicioso - bloquear usuario
                await user_blocking_service.block_user(tenant_id, user_id, reason="Comportamiento malicioso durante registro")
                user_context["user_state"] = "BLOCKED"
                session_context_service.update_user_context(session_id, user_context)
                
                response = "Tu mensaje contiene contenido inapropiado. Has sido bloqueado del sistema."
                logger.warning(f"üö´ Usuario {user_id} bloqueado por comportamiento malicioso durante registro")
                
            elif confidence >= 0.7:
                # Comportamiento moderadamente malicioso - advertencia
                response = "Por favor, mant√©n un tono respetuoso. Este es un espacio para el di√°logo constructivo sobre la campa√±a pol√≠tica."
                
            else:
                # Comportamiento ligeramente inapropiado - redirecci√≥n suave
                response = "Entiendo que quieres participar. Por favor, comparte informaci√≥n constructiva sobre la campa√±a."
            
            # Agregar respuesta del bot a la sesi√≥n
            session_context_service.add_message(session_id, "assistant", response)
            
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            
            return {
                "response": response,
                "followup_message": "",
                "from_cache": False,
                "processing_time": processing_time,
                "tenant_id": tenant_id,
                "session_id": session_id,
                "intent": "malicioso",
                "confidence": confidence,
                "malicious_detection": malicious_detection,
                "user_blocked": confidence >= 0.8
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error manejando mensaje malicioso: {str(e)}")
            # Fallback a respuesta gen√©rica de bloqueo
            return {
                "response": "Por favor, mant√©n un tono respetuoso en nuestras conversaciones.",
                "followup_message": "",
                "from_cache": False,
                "processing_time": time.time() - start_time,
                "tenant_id": tenant_id,
                "session_id": session_id,
                "intent": "malicioso",
                "confidence": 0.0,
                "error": str(e)
            }

    async def _handle_registration_response(self, tenant_id: str, query: str, user_context: Dict[str, Any], 
                                           registration_analysis: Dict[str, Any], branding_config: Dict[str, Any], 
                                           session_id: str) -> Dict[str, Any]:
        """
        Maneja respuestas de registro cuando el usuario est√° en proceso de registro
        
        Args:
            tenant_id: ID del tenant
            query: Mensaje del usuario
            user_context: Contexto del usuario
            registration_analysis: An√°lisis de la respuesta de registro
            branding_config: Configuraci√≥n de branding
            session_id: ID de la sesi√≥n
            
        Returns:
            Respuesta procesada para el usuario
        """
        try:
            contact_name = branding_config.get("contactName", "el candidato")
            data_type = registration_analysis.get("type", "other")
            data_value = registration_analysis.get("value", "")
            confidence = registration_analysis.get("confidence", 0.0)
            
            logger.info(f"üîÑ Procesando respuesta de registro: tipo={data_type}, valor='{data_value}', confianza={confidence}")
            
            # Construir respuesta espec√≠fica seg√∫n el tipo de datos extra√≠dos
            if data_type == "name" and data_value:
                response = f"¬°Perfecto! Nombre anotado: {data_value}. Ahora necesito tu apellido:"
                # Actualizar contexto del usuario
                user_context["user_name"] = data_value
                session_context_service.update_user_context(session_id, user_context)
                
            elif data_type == "lastname" and data_value:
                user_name = user_context.get("user_name", "Usuario")
                response = f"¬°Perfecto, {user_name}! Apellido anotado: {data_value}. Ahora dime, ¬øen qu√© ciudad vives?"
                # Actualizar contexto del usuario
                user_context["user_lastname"] = data_value
                session_context_service.update_user_context(session_id, user_context)
                
            elif data_type == "city" and data_value:
                user_name = user_context.get("user_name", "Usuario")
                response = f"¬°Excelente, {user_name}! Ciudad anotada: {data_value}. Ahora dime, ¬øen qu√© te puedo asistir hoy desde la oficina de {contact_name}?"
                # Actualizar contexto del usuario
                user_context["user_city"] = data_value
                user_context["user_state"] = "COMPLETED"  # Marcar como completado
                session_context_service.update_user_context(session_id, user_context)
                
            elif data_type == "code" and data_value:
                user_name = user_context.get("user_name", "Usuario")
                response = f"¬°Perfecto, {user_name}! C√≥digo de referido anotado: {data_value}. Ahora dime, ¬øen qu√© te puedo asistir hoy desde la oficina de {contact_name}?"
                # Actualizar contexto del usuario
                user_context["referral_code"] = data_value
                user_context["user_state"] = "COMPLETED"  # Marcar como completado
                session_context_service.update_user_context(session_id, user_context)
                
            elif data_type == "info":
                # Usar IA para generar respuesta natural cuando es informaci√≥n/explicaci√≥n
                logger.info(f"üéØ Generando respuesta con IA para explicaci√≥n: '{query[:30]}...'")
                ai_response = await self._generate_content_optimized(
                    f"""Eres un asistente de campa√±a pol√≠tica. El usuario est√° en proceso de registro.

CONTEXTO:
- Estado del usuario: {user_context.get('user_state', 'UNKNOWN')}
- Mensaje del usuario: "{query}"

INSTRUCCIONES:
1. Si el usuario explica limitaciones (ej: "solo puedo dar nombre y apellido"), entiende y adapta el proceso
2. Si es un saludo, responde amigablemente y contin√∫a el registro
3. Si pregunta sobre el candidato, explica que despu√©s del registro le puedes ayudar
4. Mant√©n un tono amigable y profesional
5. Siempre gu√≠a hacia completar el registro

RESPUESTA NATURAL:""",
                    "registration_response"
                )
                response = ai_response if ai_response else "Entiendo tu consulta. ¬øPodr√≠as proporcionarme la informaci√≥n que necesito?"
                
            else:
                # Si no se pudo extraer datos espec√≠ficos, pedir aclaraci√≥n
                user_state = user_context.get("user_state", "")
                if user_state == "WAITING_NAME":
                    response = "Por favor, comparte tu nombre completo para continuar con el registro."
                elif user_state == "WAITING_LASTNAME":
                    response = "Perfecto, ahora necesito tu apellido para completar tu informaci√≥n."
                elif user_state == "WAITING_CITY":
                    response = "¬øEn qu√© ciudad vives? Esto nos ayuda a conectar con promotores de tu regi√≥n."
                elif user_state == "WAITING_CODE":
                    response = "Si tienes un c√≥digo de referido, comp√°rtelo. Si no, escribe 'no' para continuar."
                else:
                    response = "Por favor, comparte la informaci√≥n solicitada para continuar."
            
            # Agregar respuesta del bot a la sesi√≥n
            session_context_service.add_message(session_id, "assistant", response)
            
            processing_time = time.time() - start_time if 'start_time' in locals() else 0.0
            
            return {
                "response": response,
                "followup_message": "",
                "from_cache": False,
                "processing_time": processing_time,
                "tenant_id": tenant_id,
                "session_id": session_id,
                "intent": "registration_response",
                "confidence": confidence,
                "extracted_data": {
                    "type": data_type,
                    "value": data_value
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error manejando respuesta de registro: {str(e)}")
            # Fallback a respuesta gen√©rica
            return {
                "response": "Por favor, comparte la informaci√≥n solicitada para continuar con el registro.",
                "followup_message": "",
                "from_cache": False,
                "processing_time": time.time() - start_time,
                "tenant_id": tenant_id,
                "session_id": session_id,
                "intent": "registration_response",
                "confidence": 0.0,
                "error": str(e)
            }

    async def extract_user_name_from_message(self, tenant_id: str, message: str) -> Dict[str, Any]:
     """
     Extrae el nombre del usuario de cualquier mensaje (no necesariamente con c√≥digo de referido)
     M√âTODO NO SE USA - COMENTADO
     """
     self._ensure_model_initialized()
     
     if not self.model:
         return {
             "name": None,
             "is_valid": False,
             "confidence": 0.0,
             "reason": "Servicio de IA no disponible"
         }
     
     try:
         prompt = f"""
 Analiza el siguiente mensaje y extrae el nombre completo de la persona:

 Mensaje: "{message}"

 IMPORTANTE:
 - Busca patrones como "Soy [nombre]", "Me llamo [nombre]", "Mi nombre es [nombre]", etc.
 - Extrae el nombre completo (nombre y apellidos si est√°n disponibles)
 - Si el mensaje no contiene un nombre claro, responde "NO_NAME"
 - Ignora saludos como "hola", "buenos d√≠as", etc.

 Ejemplos:
 - "Soy Santiago Buitrago Rojas" -> "Santiago Buitrago Rojas"
 - "Me llamo Mar√≠a Garc√≠a" -> "Mar√≠a Garc√≠a"
 - "Mi nombre es Carlos" -> "Carlos"
 - "Hola, soy Ana" -> "Ana"
 - "hola" -> "NO_NAME"
 - "buenos d√≠as" -> "NO_NAME"

 Responde √öNICAMENTE el nombre extra√≠do o "NO_NAME" si no se puede determinar.
 """

         response_text = await self._generate_content(prompt)
         
         if not response_text:
             return {
                 "name": None,
                 "is_valid": False,
                 "confidence": 0.0,
                 "reason": "No se pudo obtener respuesta de la IA"
             }
         
         response_clean = response_text.strip()
         
         if response_clean.upper() == "NO_NAME":
             return {
                 "name": None,
                 "is_valid": False,
                 "confidence": 0.9,
                 "reason": "El mensaje no contiene un nombre claro"
             }
         
         # Validar que el nombre extra√≠do es v√°lido
         validation_result = await self.validate_user_data(tenant_id, response_clean, "name")
         
         if validation_result.get("is_valid", False):
             return {
                 "name": response_clean,
                 "is_valid": True,
                 "confidence": validation_result.get("confidence", 0.8),
                 "reason": "Nombre extra√≠do y validado correctamente"
             }
         else:
             return {
                 "name": response_clean,
                 "is_valid": False,
                 "confidence": validation_result.get("confidence", 0.5),
                 "reason": f"Nombre extra√≠do pero no v√°lido: {validation_result.get('reason', '')}"
             }
             
     except Exception as e:
         logger.error(f"Error extrayendo nombre del mensaje con IA: {str(e)}")
         return {
             "name": None,
             "is_valid": False,
             "confidence": 0.0,
             "reason": f"Error en extracci√≥n: {str(e)}"
         }

    async def extract_user_name_from_message(self, tenant_id: str, message: str) -> Dict[str, Any]:
        """
        Extrae el nombre del usuario de un mensaje que contiene un c√≥digo de referido
        
        Args:
            tenant_id: ID del tenant
            message: Mensaje que contiene c√≥digo de referido
            
        Returns:
            Dict con el nombre extra√≠do y validaci√≥n
        """
        self._ensure_model_initialized()
        
        if not self.model:
            return {
                "name": None,
                "is_valid": False,
                "confidence": 0.0,
                "reason": "Servicio de IA no disponible"
            }
        
        try:
            prompt = f"""
Analiza el siguiente mensaje y extrae SOLO el nombre de la persona que se est√° registrando:

Mensaje: "{message}"

IMPORTANTE:
- El mensaje puede contener un c√≥digo de referido
- El mensaje puede mencionar a la persona que refiere
- Debes extraer SOLO el nombre de quien se est√° registrando, NO de quien refiere
- Si el mensaje no contiene un nombre claro del usuario, responde "NO_NAME"

Ejemplos:
- "Hola, vengo referido por Juan, codigo: ABC123" -> El usuario NO menciona su nombre, debe responder "NO_NAME"
- "Soy Mar√≠a, vengo referido por Juan, codigo: ABC123" -> El nombre es "Mar√≠a"
- "Me llamo Carlos, codigo: DEF456" -> El nombre es "Carlos"
- "Hola, soy Ana Garc√≠a, vengo referido por Pedro, codigo: GHI789" -> El nombre es "Ana Garc√≠a"

Responde √öNICAMENTE el nombre extra√≠do o "NO_NAME" si no se puede determinar.
"""

            response_text = await self._generate_content(prompt)
            
            if not response_text:
                return {
                    "name": None,
                    "is_valid": False,
                    "confidence": 0.0,
                    "reason": "No se pudo obtener respuesta de la IA"
                }
            
            response_clean = response_text.strip()
            
            if response_clean.upper() == "NO_NAME":
                return {
                    "name": None,
                    "is_valid": False,
                    "confidence": 0.9,
                    "reason": "El mensaje no contiene el nombre del usuario"
                }
            
            # Validar que el nombre extra√≠do es v√°lido
            validation_result = await self.validate_user_data(tenant_id, response_clean, "name")
            
            if validation_result.get("is_valid", False):
                return {
                    "name": response_clean,
                    "is_valid": True,
                    "confidence": validation_result.get("confidence", 0.8),
                    "reason": "Nombre extra√≠do y validado correctamente"
                }
            else:
                return {
                    "name": response_clean,
                    "is_valid": False,
                    "confidence": validation_result.get("confidence", 0.5),
                    "reason": f"Nombre extra√≠do pero no v√°lido: {validation_result.get('reason', '')}"
                }
                
        except Exception as e:
            logger.error(f"Error extrayendo nombre del mensaje con IA: {str(e)}")
            return {
                "name": None,
                "is_valid": False,
                "confidence": 0.0,
                "reason": f"Error en extracci√≥n: {str(e)}"
            }

    async def generate_welcome_message(self, tenant_config: Dict[str, Any] = None) -> str:
        """
        Genera un mensaje de bienvenida personalizado usando IA
        
        Args:
            tenant_config: Configuraci√≥n del tenant (opcional)
            
        Returns:
            Mensaje de bienvenida generado por IA
        """
        try:
            # Obtener informaci√≥n del tenant para personalizaci√≥n
            tenant_info = ""
            if tenant_config:
                branding = tenant_config.get('branding', {})
                if branding:
                    candidate_name = branding.get('candidate_name', '')
                    campaign_name = branding.get('campaign_name', '')
                    if candidate_name:
                        tenant_info += f"Candidato: {candidate_name}. "
                    if campaign_name:
                        tenant_info += f"Campa√±a: {campaign_name}. "
            
            # Generar mensaje con IA usando el m√©todo que s√≠ funciona
            prompt = f"""Genera un mensaje de bienvenida para WhatsApp de la campa√±a de {tenant_info.strip() or 'nuestro candidato'}.

El mensaje debe ser amigable y presentar la campa√±a. M√°ximo 100 caracteres.

Mensaje:"""

            try:
                response = await self._generate_content(prompt, task_type="chat_conversational")
                if response and len(response.strip()) > 0:
                    return response.strip()
            except Exception as e:
                logger.warning(f"Error generando mensaje con IA: {e}")
            
            # Fallback si IA falla
            if candidate_name and campaign_name:
                response = f"¬°Hola! Bienvenido a la campa√±a de {candidate_name} - {campaign_name}."
            elif candidate_name:
                response = f"¬°Hola! Bienvenido a la campa√±a de {candidate_name}."
            elif campaign_name:
                response = f"¬°Hola! Bienvenido a {campaign_name}."
            else:
                response = "¬°Hola! Bienvenido a nuestra campa√±a."
            
            return response.strip()
                
        except Exception as e:
            logger.error(f"Error generando mensaje de bienvenida con IA: {str(e)}")
            return "¬°Hola! Te doy la bienvenida a nuestra campa√±a. ¬°Es un placer conocerte!"

    async def generate_contact_save_message(self, tenant_config: Dict[str, Any] = None) -> str:
        """
        Genera un mensaje para pedir al usuario que guarde el contacto usando IA
        
        Args:
            tenant_config: Configuraci√≥n del tenant (opcional)
            
        Returns:
            Mensaje para guardar contacto generado por IA
        """
        try:
            # Obtener nombre del contacto desde la configuraci√≥n
            contact_name = "Contacto"
            if tenant_config:
                branding = tenant_config.get('branding', {})
                if branding:
                    config_contact_name = branding.get('contact_name', '')
                    if config_contact_name and config_contact_name.strip():
                        contact_name = config_contact_name.strip()
            
            # Generar mensaje con IA usando el m√©todo que s√≠ funciona
            prompt = f"""Genera un mensaje para WhatsApp pidiendo guardar el contacto como "{contact_name}".

El mensaje debe ser educado y explicar por qu√© es importante. M√°ximo 150 caracteres.

Mensaje:"""

            try:
                response = await self._generate_content(prompt, task_type="chat_conversational")
                if response and len(response.strip()) > 0:
                    return response.strip()
            except Exception as e:
                logger.warning(f"Error generando mensaje con IA: {e}")
            
            # Fallback si IA falla
            response = f"Por favor, guarda este n√∫mero como '{contact_name}' para recibir actualizaciones importantes de la campa√±a."
            
            return response.strip()
                
        except Exception as e:
            logger.error(f"Error generando mensaje de guardar contacto con IA: {str(e)}")
            return f"Te pido que lo primero que hagas sea guardar este n√∫mero con el nombre: {contact_name}"

    async def generate_all_initial_messages(self, tenant_config: Dict[str, Any] = None) -> Dict[str, str]:
        """
        Genera los 3 mensajes iniciales de una vez para optimizar el tiempo de respuesta
        
        Args:
            tenant_config: Configuraci√≥n del tenant (opcional)
            
        Returns:
            Diccionario con los 3 mensajes generados
        """
        try:
            # üöÄ OPTIMIZACI√ìN: Usar respuestas precomputadas para casos comunes
            candidate_name = ""
            contact_name = "Mi Candidato"
            
            if tenant_config:
                branding = tenant_config.get('branding', {})
                if branding:
                    candidate_name = branding.get('candidate_name', '')
                    config_contact_name = branding.get('contact_name', '')
                    if config_contact_name and config_contact_name.strip():
                        contact_name = config_contact_name.strip()
            
            # Verificar si podemos usar respuestas precomputadas gen√©ricas
            if not candidate_name or candidate_name.strip() == "":
                logger.info("üöÄ Usando respuestas precomputadas gen√©ricas")
                return self._precomputed_initial_messages["default"].copy()
            
            # Si necesitamos personalizaci√≥n espec√≠fica, usar IA
            logger.info(f"üîÑ Generando mensajes personalizados para candidato: {candidate_name}")
            
            # Obtener informaci√≥n del tenant para personalizaci√≥n
            tenant_info = ""
            if tenant_config:
                branding = tenant_config.get('branding', {})
                if branding:
                    campaign_name = branding.get('campaign_name', '')
                    
                    if candidate_name:
                        tenant_info += f"Candidato: {candidate_name}. "
                    if campaign_name:
                        tenant_info += f"Campa√±a: {campaign_name}. "
            
            # üöÄ OPTIMIZACI√ìN: Prompt ultra-optimizado para velocidad
            prompt = f"""Genera 3 mensajes WhatsApp campa√±a pol√≠tica.

Info: {tenant_info}
Contacto: {contact_name}

1. Bienvenida (100 chars): BIENVENIDA:
2. Guardar contacto (150 chars): CONTACTO:  
3. Pedir nombre (120 chars): NOMBRE:"""

            try:
                # üöÄ OPTIMIZACI√ìN: Usar configuraci√≥n ultra-r√°pida para mensajes iniciales
                response = await self._generate_content_ultra_fast(prompt)
                if response and len(response.strip()) > 0:
                    # Parsear la respuesta
                    lines = response.strip().split('\n')
                    messages = {}
                    
                    for line in lines:
                        if line.startswith('BIENVENIDA:'):
                            messages['welcome'] = line.replace('BIENVENIDA:', '').strip()
                        elif line.startswith('CONTACTO:'):
                            messages['contact'] = line.replace('CONTACTO:', '').strip()
                        elif line.startswith('NOMBRE:'):
                            messages['name'] = line.replace('NOMBRE:', '').strip()
                    
                    if len(messages) == 3:
                        return messages
            except Exception as e:
                logger.warning(f"Error generando mensajes con IA: {e}")
            
            # üöÄ FALLBACK ULTRA-R√ÅPIDO: Usar respuestas precomputadas si IA falla
            logger.warning("‚ö†Ô∏è IA fall√≥, usando respuestas precomputadas como fallback")
            return self._precomputed_initial_messages["default"].copy()
                
        except Exception as e:
            logger.error(f"Error generando mensajes iniciales: {str(e)}")
            # Usar respuestas gen√©ricas como √∫ltimo recurso
            return self._precomputed_initial_messages["default"].copy()

    async def generate_name_request_message(self, tenant_config: Dict[str, Any] = None) -> str:
        """
        Genera un mensaje para pedir el nombre del usuario usando IA
        
        Args:
            tenant_config: Configuraci√≥n del tenant (opcional)
            
        Returns:
            Mensaje para pedir nombre generado por IA
        """
        try:
            # Generar mensaje con IA usando el m√©todo que s√≠ funciona
            prompt = f"""Genera un mensaje para WhatsApp pidiendo el nombre del usuario.

El mensaje debe ser amigable y explicar por qu√© necesitas el nombre. M√°ximo 120 caracteres.

Mensaje:"""

            try:
                response = await self._generate_content(prompt, task_type="chat_conversational")
                if response and len(response.strip()) > 0:
                    return response.strip()
            except Exception as e:
                logger.warning(f"Error generando mensaje con IA: {e}")
            
            # Fallback si IA falla
            response = "¬øMe confirmas tu nombre para guardarte en mis contactos y personalizar tu experiencia?"
            
            return response.strip()
                
        except Exception as e:
            logger.error(f"Error generando mensaje de pedir nombre con IA: {str(e)}")
            return "¬øMe confirmas tu nombre para guardarte en mis contactos?"
    
    def _enhance_query_for_document_search(self, query: str) -> str:
        """
        Mejora la query para mejor recuperaci√≥n de documentos
        A√±ade sin√≥nimos y t√©rminos relacionados relevantes
        """
        query_lower = query.lower()
        
        # Sin√≥nimos y t√©rminos relacionados gen√©ricos
        synonym_map = {
            "culpable": ["responsable", "autor", "involucrado", "implicado"],
            "responsable": ["culpable", "autor", "involucrado", "implicado"],
        }
        
        enhanced_query = query
        
        # A√±adir sin√≥nimos relevantes
        for key, synonyms in synonym_map.items():
            if key in query_lower:
                enhanced_query += " " + " ".join(synonyms)
                break
        
        return enhanced_query
    
    def _is_content_relevant(self, query: str, content: str) -> bool:
        """
        Verifica si el contenido es relevante para la query
        """
        query_lower = query.lower()
        content_lower = content.lower()
        
        # Extraer palabras clave importantes de la query
        query_words = set(query_lower.split())
        
        # Filtrar palabras muy comunes
        stop_words = {'de', 'la', 'el', 'en', 'y', 'a', 'que', 'es', 'un', 'una', 'por', 'con', 'para', 'su', 'los', 'las', 'le', 'se', 'del', 'al', 'lo', 'como', 'si', 'son', 'est√°n', 'm√°s', 'cu√°l', 'cu√°les', 'qu√©', 'qui√©n', 'qui√©nes', 'es', 'son', 'est√°', 'est√°n', 'hay', 'ser', 'est√°'}
        important_words = query_words - stop_words
        
        # Verificar si al menos algunas palabras importantes est√°n en el contenido
        if len(important_words) == 0:
            return True  # No hay palabras importantes, asumir relevante
        
        matches = sum(1 for word in important_words if word in content_lower)
        relevance_score = matches / len(important_words) if important_words else 0
        
        # Considerar relevante si al menos el 20% de las palabras importantes coinciden
        # Reducido a 20% para ser m√°s permisivo
        is_relevant = relevance_score >= 0.2
        
        logger.info(f"üîç DEBUG RELEVANCIA: query_words={important_words}, matches={matches}, score={relevance_score:.2f}, relevante={is_relevant}")
        logger.info(f"üîç DEBUG RELEVANCIA: Preview content: {content_lower[:200]}...")
        
        return is_relevant

# Instancia global para compatibilidad
ai_service = AIService()