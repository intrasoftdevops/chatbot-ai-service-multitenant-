"""
RAG Orchestrator - Cerebro del sistema RAG

Orquesta el flujo completo de Retrieval-Augmented Generation:
1. Query Rewriting (reformular preguntas)
2. Document Retrieval (buscar documentos relevantes)
3. Context Building (construir contexto)
4. Response Generation (generar respuesta)
5. Verification (verificar respuesta)
6. Citation (agregar citas)

Este es el componente central del sistema RAG, coordinando todos
los dem√°s componentes para producir respuestas verificadas y fundamentadas.
"""

import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

from chatbot_ai_service.retrievers.smart_retriever import SmartRetriever, SearchResult
from chatbot_ai_service.verifiers.source_verifier import SourceVerifier, VerificationResult
from chatbot_ai_service.prompts.system_prompts import PromptBuilder, PromptType
from chatbot_ai_service.guardrails.guardrail_verifier import GuardrailVerifier, GuardrailVerificationResult
from chatbot_ai_service.guardrails.response_sanitizer import ResponseSanitizer

logger = logging.getLogger(__name__)


@dataclass
class RAGResponse:
    """Respuesta completa del RAGOrchestrator"""
    response: str
    response_with_citations: str
    verification: VerificationResult
    retrieved_documents: List[SearchResult]
    metadata: Dict[str, Any]
    guardrail_result: Optional[GuardrailVerificationResult] = None
    sanitization_applied: bool = False


class RAGOrchestrator:
    """
    Orquestador principal del sistema RAG
    
    Coordina todo el flujo desde la pregunta del usuario hasta
    la respuesta verificada con citas de fuentes.
    
    Caracter√≠sticas:
    - Query rewriting para mejor b√∫squeda
    - Hybrid retrieval (sem√°ntica + keywords)
    - Context building inteligente
    - Response generation con documentos
    - Source verification
    - Automatic citations
    """
    
    def __init__(
        self, 
        gemini_client=None,
        document_service=None,
        enable_verification: bool = True,
        enable_citations: bool = True,
        enable_guardrails: bool = True,
        strict_guardrails: bool = True
    ):
        """
        Inicializa el RAGOrchestrator
        
        Args:
            gemini_client: Cliente de Gemini para generaci√≥n
            document_service: Servicio de documentos
            enable_verification: Si True, verifica respuestas
            enable_citations: Si True, agrega citas
            enable_guardrails: Si True, aplica guardrails estrictos
            strict_guardrails: Si True, fallos cr√≠ticos invalidan respuesta
        """
        self.gemini_client = gemini_client
        self.document_service = document_service
        self.enable_verification = enable_verification
        self.enable_citations = enable_citations
        self.enable_guardrails = enable_guardrails
        self.strict_guardrails = strict_guardrails
        
        # Inicializar componentes
        self.retriever = SmartRetriever()
        self.verifier = SourceVerifier()
        
        # üõ°Ô∏è FASE 5: Guardrails
        self.prompt_builder = PromptBuilder()
        self.guardrail_verifier = GuardrailVerifier(strict_mode=strict_guardrails)
        self.response_sanitizer = ResponseSanitizer(aggressive_mode=strict_guardrails)
        
        logger.info(
            f"üöÄ RAGOrchestrator inicializado "
            f"(verification={enable_verification}, citations={enable_citations}, "
            f"guardrails={enable_guardrails}, strict={strict_guardrails})"
        )
    
    async def _rewrite_query(self, query: str) -> List[str]:
        """
        Reescribe el query para mejorar la b√∫squeda
        
        Genera variaciones del query original para capturar
        m√°s documentos relevantes.
        
        Args:
            query: Query original del usuario
            
        Returns:
            Lista de queries (incluyendo el original)
        """
        # Por ahora, solo retornamos el query original
        # En futuras versiones, usaremos Gemini para generar variaciones
        
        queries = [query]
        
        # Agregar variaci√≥n con palabras clave adicionales
        if "propuesta" in query.lower() or "propone" in query.lower():
            queries.append(f"{query} plan gobierno programa")
        
        if "candidato" in query.lower():
            queries.append(f"{query} trayectoria biografia perfil")
        
        logger.debug(f"Query rewriting: {len(queries)} variaciones generadas")
        return queries
    
    async def _retrieve_documents(
        self, 
        queries: List[str], 
        tenant_id: str,
        max_results: int = 5
    ) -> List[SearchResult]:
        """
        Recupera documentos relevantes para los queries
        
        Args:
            queries: Lista de queries
            tenant_id: ID del tenant
            max_results: M√°ximo de documentos a recuperar
            
        Returns:
            Lista de documentos recuperados
        """
        all_docs = []
        
        for query in queries:
            docs = await self.retriever.retrieve(
                query=query,
                tenant_id=tenant_id,
                max_results=max_results
            )
            all_docs.extend(docs)
        
        # Deduplicar por doc_id
        unique_docs = {}
        for doc in all_docs:
            if doc.doc_id not in unique_docs:
                unique_docs[doc.doc_id] = doc
            else:
                # Si ya existe, mantener el de mayor score
                if doc.score > unique_docs[doc.doc_id].score:
                    unique_docs[doc.doc_id] = doc
        
        # Ordenar por score y retornar top N
        final_docs = sorted(
            unique_docs.values(), 
            key=lambda x: x.score, 
            reverse=True
        )[:max_results]
        
        logger.info(f"Recuperados {len(final_docs)} documentos √∫nicos")
        return final_docs
    
    def _build_rag_context(
        self, 
        documents: List[SearchResult],
        max_context_length: int = 3000
    ) -> str:
        """
        Construye el contexto para Gemini a partir de los documentos
        
        Args:
            documents: Documentos recuperados
            max_context_length: Longitud m√°xima del contexto
            
        Returns:
            Contexto formateado para el prompt
        """
        if not documents:
            return "No se encontraron documentos relevantes."
        
        context_parts = []
        current_length = 0
        
        for i, doc in enumerate(documents, 1):
            doc_text = f"[Documento {i}] {doc.filename}\n{doc.content}\n"
            
            # Verificar si agregar este documento excede el l√≠mite
            if current_length + len(doc_text) > max_context_length:
                # Truncar el documento si es necesario
                remaining_space = max_context_length - current_length
                if remaining_space > 200:  # Solo agregar si queda espacio razonable
                    doc_text = doc_text[:remaining_space] + "...\n"
                    context_parts.append(doc_text)
                break
            
            context_parts.append(doc_text)
            current_length += len(doc_text)
        
        context = "\n".join(context_parts)
        
        logger.debug(f"Contexto construido: {len(context)} caracteres de {len(documents)} documentos")
        return context
    
    def _build_rag_prompt(
        self, 
        query: str, 
        context: str,
        user_context: Dict[str, Any] = None
    ) -> str:
        """
        Construye el prompt completo para RAG con guardrails
        INCLUYE contexto de sesi√≥n para continuidad
        
        Args:
            query: Pregunta del usuario
            context: Contexto de documentos
            user_context: Contexto adicional del usuario (puede incluir session_context)
            
        Returns:
            Prompt formateado con guardrails
        """
        # üîß FIX: Incluir contexto de sesi√≥n si est√° disponible
        session_context = ""
        if user_context and "session_context" in user_context:
            session_context = user_context["session_context"]
            logger.debug(f"Contexto de sesi√≥n incluido en prompt: {len(session_context)} chars")
        
        # üõ°Ô∏è FASE 5: Usar PromptBuilder con guardrails
        if self.enable_guardrails:
            # Detectar autom√°ticamente el tipo de prompt
            prompt_type = self.prompt_builder.detect_prompt_type(query)
            
            logger.debug(f"Tipo de prompt detectado: {prompt_type.value}")
            
            # Construir prompt con guardrails
            prompt = self.prompt_builder.build_prompt(
                query=query,
                documents=context,
                prompt_type=prompt_type,
                user_context=user_context
            )
            
            return prompt
        
        # Fallback: Prompt original (sin guardrails)
        user_info = ""
        if user_context and user_context.get("user_name"):
            user_info = f"El usuario se llama {user_context['user_name']}. "
        
        prompt = f"""
Eres un asistente virtual para una campa√±a pol√≠tica. Tu objetivo es proporcionar informaci√≥n PRECISA y VERIFICABLE.

**REGLAS FUNDAMENTALES:**
1. SOLO responde con informaci√≥n de los DOCUMENTOS proporcionados
2. Si la informaci√≥n est√° en los documentos, √∫sala pero NO cites la fuente
3. Si NO est√° en los documentos, di expl√≠citamente "No tengo esa informaci√≥n en los documentos disponibles"
4. NUNCA inventes datos, n√∫meros, fechas o detalles que no est√©n en los documentos
5. Si la pregunta requiere informaci√≥n no disponible, sugiere contactar al equipo de campa√±a

**CONTEXTO DEL USUARIO:**
{user_info}

**DOCUMENTOS DISPONIBLES:**
{context}

**PREGUNTA DEL USUARIO:**
{query}

**INSTRUCCIONES PARA LA RESPUESTA:**
- S√© claro, conciso y directo
- Usa la informaci√≥n de los documentos
- Cita el n√∫mero del documento [Documento N] cuando uses informaci√≥n espec√≠fica
- Si no hay informaci√≥n, s√© honesto al respecto
- Mant√©n un tono profesional y amigable

**TU RESPUESTA:**
"""
        return prompt
    
    async def _generate_response(
        self, 
        prompt: str,
        task_type: str = "rag_generation"
    ) -> str:
        """
        Genera la respuesta usando Gemini
        
        Args:
            prompt: Prompt completo
            task_type: Tipo de tarea para configuraci√≥n
            
        Returns:
            Respuesta generada
        """
        if not self.gemini_client:
            logger.error("GeminiClient no disponible")
            return "Lo siento, el servicio de IA no est√° disponible en este momento."
        
        try:
            response = await self.gemini_client.generate_content(
                prompt=prompt,
                task_type=task_type,
                use_custom_config=True
            )
            
            logger.info(f"‚úÖ Respuesta generada ({len(response)} caracteres)")
            return response
            
        except Exception as e:
            logger.error(f"Error generando respuesta: {str(e)}")
            return f"Lo siento, hubo un error al procesar tu consulta: {str(e)}"
    
    async def process_query(
        self, 
        query: str, 
        tenant_id: str,
        user_context: Dict[str, Any] = None,
        max_docs: int = 3
    ) -> RAGResponse:
        """
        Procesa un query completo usando RAG
        
        Este es el m√©todo principal que orquesta todo el flujo:
        1. Rewrite query
        2. Retrieve documents
        3. Build context
        4. Generate response
        5. Verify response
        6. Add citations
        
        Args:
            query: Pregunta del usuario
            tenant_id: ID del tenant
            user_context: Contexto adicional del usuario
            max_docs: M√°ximo n√∫mero de documentos a usar
            
        Returns:
            RAGResponse con respuesta completa y metadata
        """
        logger.info(f"Procesando query RAG: '{query}' para tenant {tenant_id}")
        
        start_time = None
        try:
            import time
            start_time = time.time()
        except:
            pass
        
        # 1. Query Rewriting
        logger.info("1Ô∏è‚É£ Reescribiendo query...")
        queries = await self._rewrite_query(query)
        
        # 2. Document Retrieval
        logger.info("2Ô∏è‚É£ Recuperando documentos...")
        documents = await self._retrieve_documents(queries, tenant_id, max_docs)
        
        if not documents:
            logger.warning("‚ö†Ô∏è No se encontraron documentos relevantes")
            return RAGResponse(
                response="Lo siento, no encontr√© informaci√≥n relevante en los documentos disponibles para responder tu pregunta. ¬øPodr√≠as reformularla o hacer una pregunta m√°s espec√≠fica?",
                response_with_citations="",
                verification=VerificationResult(
                    is_verified=False,
                    confidence=0.0,
                    unsupported_claims=[],
                    sources_used=[],
                    hallucination_risk=0.0,
                    recommendation="No hay documentos disponibles"
                ),
                retrieved_documents=[],
                metadata={"query": query, "tenant_id": tenant_id},
                guardrail_result=None,
                sanitization_applied=False
            )
        
        # 3. Build Context
        logger.info("3Ô∏è‚É£ Construyendo contexto...")
        context = self._build_rag_context(documents)
        
        # 4. Build Prompt
        logger.info("4Ô∏è‚É£ Construyendo prompt...")
        prompt = self._build_rag_prompt(query, context, user_context)
        
        # 5. Generate Response
        logger.info("5Ô∏è‚É£ Generando respuesta...")
        response = await self._generate_response(prompt)
        
        # üõ°Ô∏è FASE 5: Verificaci√≥n con Guardrails
        guardrail_result = None
        sanitization_applied = False
        
        if self.enable_guardrails:
            logger.info("6Ô∏è‚É£ Verificando guardrails...")
            guardrail_result = self.guardrail_verifier.verify(
                response=response,
                documents=documents,
                query=query
            )
            
            logger.info(
                f"   ‚îî‚îÄ Score: {guardrail_result.score:.0%}, "
                f"Critical: {guardrail_result.critical_failures}, "
                f"Warnings: {guardrail_result.warnings}"
            )
            
            # Si falla guardrails, sanitizar
            if not guardrail_result.all_passed:
                logger.warning(f"‚ö†Ô∏è Guardrails no pasados: {guardrail_result.recommendation}")
                
                if self.strict_guardrails and guardrail_result.critical_failures > 0:
                    logger.info("7Ô∏è‚É£ Sanitizando respuesta...")
                    response, changes = self.response_sanitizer.sanitize(
                        response=response,
                        documents=documents,
                        guardrail_result=guardrail_result
                    )
                    sanitization_applied = True
                    logger.info(f"   ‚îî‚îÄ Cambios aplicados: {len(changes)}")
        
        # 8. Verify Response (SourceVerifier)
        verification = None
        if self.enable_verification:
            logger.info("8Ô∏è‚É£ Verificando fuentes...")
            verification = self.verifier.verify_response(response, documents)
        else:
            verification = VerificationResult(
                is_verified=True,
                confidence=1.0,
                unsupported_claims=[],
                sources_used=[],
                hallucination_risk=0.0,
                recommendation="Verificaci√≥n deshabilitada"
            )
        
        # 9. Add Citations
        response_with_citations = response
        if self.enable_citations:
            logger.info("9Ô∏è‚É£ Agregando citas...")
            response_with_citations = self.verifier.add_citations(response, documents)
        
        # Calculate processing time
        processing_time = None
        if start_time:
            try:
                processing_time = time.time() - start_time
            except:
                pass
        
        # Build metadata
        metadata = {
            "query": query,
            "tenant_id": tenant_id,
            "num_documents_retrieved": len(documents),
            "num_queries_generated": len(queries),
            "verification_enabled": self.enable_verification,
            "citations_enabled": self.enable_citations,
            "guardrails_enabled": self.enable_guardrails,
            "processing_time_seconds": processing_time
        }
        
        # Agregar metadata de guardrails si est√° habilitado
        if guardrail_result:
            metadata["guardrail_score"] = guardrail_result.score
            metadata["guardrail_passed"] = guardrail_result.all_passed
            metadata["critical_failures"] = guardrail_result.critical_failures
            metadata["warnings"] = guardrail_result.warnings
            metadata["sanitization_applied"] = sanitization_applied
        
        # Log de documentos utilizados para la respuesta
        if documents:
            doc_names = [doc.filename for doc in documents]
            logger.info(f"DOCUMENTOS UTILIZADOS PARA LA RESPUESTA: {doc_names}")
        else:
            logger.info("RESPUESTA GENERADA SIN DOCUMENTOS (informaci√≥n general)")
        
        logger.info(
            f"Query RAG procesado exitosamente "
            f"({len(documents)} docs, {processing_time:.2f}s)" if processing_time else ""
        )
        
        return RAGResponse(
            response=response,
            response_with_citations=response_with_citations,
            verification=verification,
            retrieved_documents=documents,
            metadata=metadata,
            guardrail_result=guardrail_result,
            sanitization_applied=sanitization_applied
        )
    
    async def process_query_simple(
        self, 
        query: str, 
        tenant_id: str,
        user_context: Dict[str, Any] = None,
        session_id: str = None
    ) -> str:
        """
        Versi√≥n simplificada que solo retorna la respuesta con citas
        INCLUYE contexto de sesi√≥n para mantener continuidad
        
        Args:
            query: Pregunta del usuario
            tenant_id: ID del tenant
            user_context: Contexto del usuario
            session_id: ID de sesi√≥n para contexto persistente
            
        Returns:
            Respuesta con citas (string)
        """
        # üîß FIX: Incluir contexto de sesi√≥n si est√° disponible
        if session_id:
            try:
                from chatbot_ai_service.services.session_context_service import session_context_service
                session_context = session_context_service.build_context_for_ai(session_id)
                if session_context:
                    # Agregar contexto de sesi√≥n al user_context
                    if not user_context:
                        user_context = {}
                    user_context["session_context"] = session_context
                    logger.info(f"‚úÖ Contexto de sesi√≥n incluido en RAG: {len(session_context)} chars")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error obteniendo contexto de sesi√≥n: {str(e)}")
        
        rag_response = await self.process_query(query, tenant_id, user_context)
        
        if self.enable_citations:
            return rag_response.response_with_citations
        else:
            return rag_response.response

