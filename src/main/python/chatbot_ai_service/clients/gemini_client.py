"""
Cliente dedicado para interactuar con Gemini AI usando LlamaIndex

Este cliente usa LlamaIndex Gemini (como chatbotIA-original) para evitar bloqueos
de safety filters que ocurren con google.generativeai directo.

Extra√≠do de: src/main/python/chatbot_ai_service/services/ai_service.py
L√≠neas de referencia: 24-33, 39-56, 150-217
"""
import logging
import time
import os
from typing import Optional, Dict, Any
import httpx

# Intentar importar LlamaIndex Gemini (como chatbotIA-original)
LLAMA_INDEX_AVAILABLE = False
try:
    from llama_index.llms.gemini import Gemini as LlamaGemini
    LLAMA_INDEX_AVAILABLE = True
    logger_import = logging.getLogger(__name__)
    logger_import.info("‚úÖ LlamaIndex Gemini cargado correctamente")
except ImportError as e:
    logger_import = logging.getLogger(__name__)
    logger_import.error(f"‚ùå Error cargando LlamaIndex Gemini: {e}")
    LlamaGemini = None

logger = logging.getLogger(__name__)


class GeminiClient:
    """
    Cliente para Gemini AI con configuraci√≥n avanzada y resiliencia
    
    Caracter√≠sticas:
    - Inicializaci√≥n lazy del modelo
    - Rate limiting (15 requests/minuto)
    - Fallback autom√°tico de gRPC a REST API
    - Manejo robusto de errores
    
    Uso:
        client = GeminiClient()
        response = await client.generate_content("¬øC√≥mo est√°s?")
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """
        Inicializa el cliente de Gemini
        
        Args:
            api_key: API key de Gemini (opcional, usa GEMINI_API_KEY por defecto)
        """
        self.model = None
        self._initialized = False
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")
        
        # Rate limiting (copiado de AIService l√≠nea 27-29)
        self.request_times = []
        self.max_requests_per_minute = 15
        
        # NUEVO: Cache de modelos por configuraci√≥n (Fase 2)
        self.models_cache = {}  # Key: config_hash -> model instance
        
        logger.info("GeminiClient inicializado (lazy loading)")
    
    def preload_models(self):
        """
        Pre-carga los modelos usando LlamaIndex Gemini (como chatbotIA-original)
        
        Este m√©todo inicializa los modelos que se usan frecuentemente
        para evitar la latencia de inicializaci√≥n en tiempo real.
        """
        if not self.api_key:
            logger.warning("‚ö†Ô∏è No se puede pre-cargar modelos: GEMINI_API_KEY no configurado")
            return
            
        if not LLAMA_INDEX_AVAILABLE or LlamaGemini is None:
            logger.error("‚ùå LlamaIndex Gemini no disponible - no se puede pre-cargar modelos")
            logger.error("‚ùå Instala llama-index-llms-gemini para usar esta funcionalidad")
            return
            
        logger.info("üöÄ Pre-cargando modelos de IA usando LlamaIndex Gemini...")
        print("üöÄ DEBUG PRELOAD: Iniciando preload_models() con LlamaIndex")
        
        # üöÄ OPTIMIZACI√ìN: Usar LlamaIndex Gemini como chatbotIA-original
        import os
        ultra_fast_mode = os.getenv("ULTRA_FAST_MODE", "false").lower() == "true"
        logger.info(f"üöÄ ULTRA_FAST_MODE detectado en preload: {ultra_fast_mode}")
        print(f"üöÄ DEBUG PRELOAD: ULTRA_FAST_MODE = {ultra_fast_mode}")
        
        # Configuraciones de modelos m√°s comunes
        common_configs = [
            # Configuraci√≥n para clasificaci√≥n de intenciones
            {
                "model_name": "gemini-2.5-flash",
                "temperature": 0.01 if ultra_fast_mode else 0.1,
                "top_p": 0.1 if ultra_fast_mode else 0.8,
                "top_k": 1 if ultra_fast_mode else 20,
                "max_output_tokens": 64 if ultra_fast_mode else 100,
                "description": "Para clasificaci√≥n de intenciones"
            },
            # Configuraci√≥n para generaci√≥n de mensajes de bienvenida
            {
                "model_name": "gemini-2.5-flash", 
                "temperature": 0.8,
                "top_p": 0.9,
                "top_k": 50,
                "max_output_tokens": 500,
                "description": "Para generar mensajes de bienvenida personalizados"
            },
            # Configuraci√≥n para generaci√≥n de mensajes de contacto
            {
                "model_name": "gemini-2.5-flash",
                "temperature": 0.8,
                "top_p": 0.9,
                "top_k": 50,
                "max_output_tokens": 500,
                "description": "Para generar mensajes de guardado de contacto"
            },
            # Configuraci√≥n para solicitud de nombre
            {
                "model_name": "gemini-2.5-flash",
                "temperature": 0.8,
                "top_p": 0.9,
                "top_k": 50,
                "max_output_tokens": 500,
                "description": "Para solicitar informaci√≥n del usuario"
            },
            # Configuraci√≥n para an√°lisis de registro
            {
                "model_name": "gemini-2.5-flash",
                "temperature": 0.1,
                "top_p": 0.8,
                "top_k": 20,
                "max_output_tokens": 200,
                "description": "Para analizar respuestas de registro"
            }
        ]
        
        # Pre-cargar cada configuraci√≥n
        for i, config in enumerate(common_configs):
            try:
                logger.info(f"üì¶ Pre-cargando modelo {i+1}/{len(common_configs)}: {config['description']}")
                self._get_or_create_model(config)
                logger.info(f"‚úÖ Modelo {i+1} pre-cargado exitosamente")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error pre-cargando modelo {i+1}: {str(e)}")
        
        logger.info(f"üéØ Pre-carga completada: {len(self.models_cache)} modelos disponibles en cache")
        
        # üöÄ CR√çTICO: Inicializar tambi√©n el modelo principal
        logger.info("üöÄ Inicializando modelo principal durante preload...")
        self._ensure_model_initialized()
        logger.info(f"üîç Modelo principal inicializado: {self.model is not None}")
    
    def _ensure_model_initialized(self):
        """
        Inicializa el modelo usando LlamaIndex Gemini (como chatbotIA-original)
        
        Solo se ejecuta una vez, en el primer uso del modelo.
        Esto mejora el tiempo de startup del servicio.
        """
        logger.info("üöÄ _ensure_model_initialized() llamado con LlamaIndex")
        logger.info(f"üîç _initialized: {self._initialized}")
        logger.info(f"üîç api_key disponible: {self.api_key is not None}")
        
        if self._initialized:
            logger.info("‚úÖ Modelo ya inicializado, saltando")
            return
            
        if not LLAMA_INDEX_AVAILABLE or LlamaGemini is None:
            logger.error("‚ùå LlamaIndex Gemini no disponible")
            self.model = None
            self._initialized = False  # NO marcar como inicializado si no hay librer√≠a
            return
            
        if self.api_key:
            try:
                # üöÄ CONFIGURACI√ìN: Usar LlamaIndex Gemini como chatbotIA-original
                import os
                ultra_fast_mode = os.getenv("ULTRA_FAST_MODE", "false").lower() == "true"
                
                # Usar gemini-2.5-flash (r√°pido y moderno) como chatbotIA-original
                model_name = "gemini-2.5-flash"
                
                logger.info(f"üöÄ Inicializando LlamaIndex Gemini con modelo: {model_name}")
                
                # Configurar temperatura seg√∫n modo
                temperature = 0.1 if ultra_fast_mode else 0.8
                max_tokens = 256 if ultra_fast_mode else 1000
                
                # Inicializar modelo con LlamaIndex (como chatbotIA-original)
                self.model = LlamaGemini(
                    model_name=f"models/{model_name}",
                    temperature=temperature,
                    max_tokens=max_tokens,
                    request_timeout=30.0,  # Timeout de 30 segundos como chatbotIA-original
                    api_key=self.api_key
                )
                
                print(f"üöÄ DEBUG PRELOAD: LlamaIndex Gemini inicializado exitosamente")
                logger.info(f"‚úÖ Modelo LlamaIndex Gemini {model_name} inicializado correctamente")
                logger.info(f"üîç Configuraci√≥n: temp={temperature}, max_tokens={max_tokens}")
                
                # Solo marcar como inicializado si el modelo se cre√≥ correctamente
                self._initialized = True
                
            except Exception as e:
                logger.error(f"‚ùå Error inicializando LlamaIndex Gemini: {str(e)}")
                self.model = None
                # NO marcar como inicializado si hubo error
                self._initialized = False
        else:
            logger.warning("‚ö†Ô∏è GEMINI_API_KEY no configurado")
            self.model = None
            self._initialized = False
    
    def _extract_text_from_response(self, response) -> str:
        """
        Extrae texto de una respuesta de Gemini, manejando respuestas simples y multi-part
        
        Args:
            response: Respuesta de Gemini (puede ser simple o multi-part)
            
        Returns:
            Texto extra√≠do de la respuesta
        """
        try:
            # Intentar acceso r√°pido para respuestas simples
            return response.text
        except (ValueError, AttributeError) as e:
            # Si falla, es una respuesta multi-part
            logger.warning(f"‚ö†Ô∏è response.text fall√≥: {str(e)}, intentando extracci√≥n manual...")
            try:
                # Debug: Ver estructura de respuesta
                logger.debug(f"üîç Response type: {type(response)}")
                logger.debug(f"üîç Has candidates: {hasattr(response, 'candidates')}")
                
                if hasattr(response, 'candidates') and response.candidates and len(response.candidates) > 0:
                    candidate = response.candidates[0]
                    logger.debug(f"üîç Candidate type: {type(candidate)}")
                    logger.debug(f"üîç Has content: {hasattr(candidate, 'content')}")
                    
                    # Chequear si fue bloqueado por safety
                    if hasattr(candidate, 'finish_reason'):
                        logger.debug(f"üîç Finish reason: {candidate.finish_reason}")
                        # finish_reason=2 corresponde a SAFETY
                        if str(candidate.finish_reason) in ['SAFETY', '3', '2']:  # 2 = SAFETY enum value
                            logger.warning(f"‚ö†Ô∏è Respuesta bloqueada por safety filters. Finish reason: {candidate.finish_reason}")
                            if hasattr(candidate, 'safety_ratings'):
                                logger.warning(f"   Safety ratings: {candidate.safety_ratings}")
                            # Retornar respuesta JSON v√°lida para safety block
                            return '{"category": "saludo_apoyo", "confidence": 0.8, "reason": "safety_block"}'
                    
                    if hasattr(candidate, 'content') and candidate.content:
                        content = candidate.content
                        logger.debug(f"üîç Content has parts: {hasattr(content, 'parts')}")
                        
                        if hasattr(content, 'parts'):
                            if content.parts:
                                # Concatenar todas las partes de texto
                                text_parts = []
                                for i, part in enumerate(content.parts):
                                    logger.debug(f"üîç Part {i} type: {type(part)}, has text: {hasattr(part, 'text')}")
                                    if hasattr(part, 'text'):
                                        text_parts.append(part.text)
                                
                                if text_parts:
                                    result = ''.join(text_parts)
                                    logger.info(f"‚úÖ Texto extra√≠do de respuesta multi-part: {len(result)} chars")
                                    return result
                                else:
                                    logger.error(f"‚ùå Parts existen pero ninguna tiene texto. Parts: {[str(p) for p in content.parts[:3]]}")
                            else:
                                logger.error(f"‚ùå Content no tiene parts o parts est√° vac√≠o. Content: {content}")
                        else:
                            logger.error(f"‚ùå Content no tiene atributo 'parts'. Content: {content}")
                
                # Si nada funciona, retornar mensaje de error m√°s espec√≠fico
                logger.error(f"‚ùå No se pudo extraer texto de la respuesta de Gemini")
                # Verificar si hay finish_reason disponible para dar mejor feedback
                if hasattr(response, 'candidates') and response.candidates and len(response.candidates) > 0:
                    candidate = response.candidates[0]
                    if hasattr(candidate, 'finish_reason'):
                        finish_reason = str(candidate.finish_reason)
                        if finish_reason in ['2', '3']:  # SAFETY
                            logger.warning("‚ö†Ô∏è Respuesta bloqueada por safety filters. Finish reason: 2")
                            logger.warning(f"   Safety ratings: {candidate.safety_ratings if hasattr(candidate, 'safety_ratings') else 'N/A'}")
                            return '{"category": "saludo_apoyo", "confidence": 0.8, "reason": "safety_block"}'
                        elif finish_reason == '1':  # STOP (normal)
                            return "Hola, ¬øen qu√© puedo ayudarte hoy?"
                        else:
                            logger.warning(f"‚ö†Ô∏è Finish reason inesperado: {finish_reason}")
                
                return '{"category": "general_query", "confidence": 0.5, "reason": "processing_error"}'
                
            except Exception as ex:
                logger.error(f"‚ùå Error extrayendo texto de respuesta multi-part: {str(ex)}", exc_info=True)
                return "Lo siento, hubo un error procesando la respuesta."
    
    def _check_rate_limit(self):
        """
        Verifica y aplica rate limiting para evitar exceder cuota de API
        
        COPIADO de AIService l√≠nea 39-56
        
        L√≠mite: 15 requests por minuto
        Si se excede, espera autom√°ticamente hasta que se libere cuota.
        """
        current_time = time.time()
        
        # Limpiar requests antiguos (m√°s de 1 minuto)
        self.request_times = [t for t in self.request_times if current_time - t < 60]
        
        # Si hemos excedido el l√≠mite, esperar
        if len(self.request_times) >= self.max_requests_per_minute:
            sleep_time = 60 - (current_time - self.request_times[0])
            if sleep_time > 0:
                logger.warning(f"‚è≥ Rate limit alcanzado. Esperando {sleep_time:.1f} segundos...")
                time.sleep(sleep_time)
                # Limpiar la lista despu√©s de esperar
                self.request_times = []
        
        # Registrar este request
        self.request_times.append(current_time)
    
    async def _call_gemini_rest_api(self, prompt: str) -> str:
        """
        Llama a Gemini usando REST API en lugar de gRPC
        
        COPIADO de AIService l√≠nea 171-196
        
        Este m√©todo se usa como fallback cuando gRPC falla.
        Es m√°s compatible pero ligeramente m√°s lento.
        
        Args:
            prompt: Texto a enviar a Gemini
            
        Returns:
            Respuesta generada por Gemini
            
        Raises:
            Exception: Si la llamada REST tambi√©n falla
        """
        try:
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={self.api_key}"
            
            payload = {
                "contents": [{
                    "parts": [{
                        "text": prompt
                    }]
                }]
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(url, json=payload)
                response.raise_for_status()
                
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    return result['candidates'][0]['content']['parts'][0]['text']
                else:
                    return "No se pudo generar respuesta"
                    
        except Exception as e:
            logger.error(f"‚ùå Error llamando a Gemini REST API: {str(e)}")
            raise
    
    def _get_or_create_model(self, config: Dict[str, Any]):
        """
        NOTA: Este m√©todo ya no se usa con LlamaIndex.
        Se mantiene para compatibilidad pero siempre retorna None.
        
        Con LlamaIndex, usamos solo un modelo principal pre-cargado.
        """
        logger.debug("‚ö†Ô∏è _get_or_create_model llamado pero no usado con LlamaIndex")
        return None
    
    def _get_or_create_model_old(self, config: Dict[str, Any]):
        """
        M√âTODO DEPRECADO: Usado solo con google.generativeai.
        Ahora usamos LlamaIndex en su lugar.
        """
        try:
            # Configurar Gemini AI
            genai.configure(api_key=self.api_key)
            
            # üöÄ OPTIMIZACI√ìN CR√çTICA: Configuraci√≥n ultra-r√°pida solo cuando ULTRA_FAST_MODE est√° activo
            import os
            ultra_fast_mode = os.getenv("ULTRA_FAST_MODE", "false").lower() == "true"
            is_local_dev = os.getenv("LOCAL_DEVELOPMENT", "false").lower() == "true"
            
            logger.info(f"üöÄ ULTRA_FAST_MODE detectado en generaci√≥n: {ultra_fast_mode}")
            logger.info(f"üöÄ LOCAL_DEVELOPMENT detectado en generaci√≥n: {is_local_dev}")
            
            if ultra_fast_mode:
                # Configuraci√≥n ultra-agresiva solo cuando ULTRA_FAST_MODE est√° activo
                generation_config = {
                    "temperature": 0.01,      # Ultra-bajo para respuestas m√°s r√°pidas y determin√≠sticas
                    "top_p": 0.1,            # Ultra-bajo para respuestas m√°s enfocadas
                    "max_output_tokens": 64,  # Ultra-reducido para respuestas m√°s r√°pidas
                    "top_k": 1,              # Ultra-bajo para respuestas m√°s determin√≠sticas
                    "candidate_count": 1,    # Solo 1 candidato para m√°xima velocidad
                }
                logger.info("üöÄ CONFIGURACI√ìN ULTRA-R√ÅPIDA ACTIVADA (ULTRA_FAST_MODE activo)")
            else:
                # Configuraci√≥n normal cuando ULTRA_FAST_MODE est√° inactivo
                generation_config = {
                    "temperature": 0.1,      # Normal para respuestas balanceadas
                    "top_p": 0.5,            # Normal para respuestas balanceadas
                    "max_output_tokens": 256,  # Normal para respuestas completas
                    "top_k": 10,             # Normal para respuestas balanceadas
                }
                logger.info("üöÄ CONFIGURACI√ìN NORMAL ACTIVADA (ULTRA_FAST_MODE inactivo)")
            
            # üöÄ CONFIGURACI√ìN: Desactivar safety filters expl√≠citamente
            from google.generativeai.types import HarmCategory, HarmBlockThreshold
            safety_settings = {
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }
            logger.info("üöÄ FILTROS DE SEGURIDAD DESACTIVADOS para contenido pol√≠tico")
            
            # Agregar response_mime_type si est√° presente (para JSON)
            # NOTA: Este campo solo funciona en versiones recientes de google-generativeai
            if "response_mime_type" in config:
                generation_config["response_mime_type"] = config["response_mime_type"]
            
            # Crear modelo SIN generation_config (siguiendo recomendaci√≥n de comunidad)
            # Los configs se pasar√°n directamente en generate_content()
            model_name = config.get("model_name", "gemini-2.5-flash")
            
            # Crear modelo con safety settings, probando desde el m√°s moderno
            models_to_try = ['gemini-2.5-flash', 'gemini-2.0-flash', 'gemini-1.5-flash', 'gemini-1.5-pro']
            if model_name in models_to_try:
                models_to_try = [model_name] + [m for m in models_to_try if m != model_name]
            
            model = None
            for try_model_name in models_to_try:
                try:
                    model = genai.GenerativeModel(
                        model_name=try_model_name,
                        safety_settings=safety_settings
                    )
                    logger.info(f"‚úÖ Modelo {try_model_name} creado exitosamente")
                    break
                except Exception as model_error:
                    logger.warning(f"‚ö†Ô∏è Modelo {try_model_name} no disponible: {str(model_error)}")
                    if try_model_name == models_to_try[-1]:  # Si es el √∫ltimo modelo
                        logger.error(f"‚ùå Ning√∫n modelo de Gemini est√° disponible para la configuraci√≥n")
                        model = None
            
            # Guardar modelo Y configuraci√≥n en cache (juntos)
            self.models_cache[config_hash] = {
                "model": model,
                "generation_config": generation_config
            }
            
            logger.info(
                f"‚úÖ Modelo creado: {model_name} | "
                f"temp={generation_config['temperature']}, "
                f"top_p={generation_config.get('top_p', 'N/A')}, "
                f"top_k={generation_config.get('top_k', 'N/A')}, "
                f"max_tokens={generation_config.get('max_output_tokens', 'N/A')}"
            )
            
            return self.models_cache[config_hash]
            
        except Exception as e:
            logger.error(f"‚ùå Error creando modelo con config personalizada: {str(e)}")
            return None
    
    async def generate_content(
        self, 
        prompt: str, 
        task_type: str = "chat_conversational",
        use_custom_config: bool = True
    ) -> str:
        """
        Genera contenido usando Gemini con fallback autom√°tico
        
        MEJORADO en Fase 2: Soporte para configuraciones por tarea
        
        Estrategia de fallback:
        1. Intenta con gRPC y configuraci√≥n espec√≠fica (si use_custom_config=True)
        2. Si falla, usa modelo por defecto
        3. Si falla, usa REST API (m√°s compatible)
        4. Si todo falla, lanza excepci√≥n
        
        Args:
            prompt: Texto a enviar a Gemini
            task_type: Tipo de tarea (ej: "intent_classification", "chat_conversational")
            use_custom_config: Si True, usa configuraci√≥n espec√≠fica para la tarea
            
        Returns:
            Respuesta generada por Gemini
            
        Raises:
            Exception: Si ambos m√©todos (gRPC y REST) fallan
        """
        # Log de inicio
        logger.debug(f"üéØ Generando contenido | task_type={task_type}, use_custom_config={use_custom_config}")
        
        # Aplicar rate limiting
        self._check_rate_limit()
        
        # Usar LlamaIndex como comportamiento principal (como chatbotIA-original)
        try:
            # Asegurar que el modelo est√© inicializado
            self._ensure_model_initialized()
            
            if self.model:
                logger.debug("üöÄ Usando modelo LlamaIndex Gemini")
                print(f"üöÄ DEBUG PROMPT: Prompt enviado: {prompt[:200]}...")
                
                # LlamaIndex model.complete() retorna la respuesta directa
                response = self.model.complete(prompt)
                
                print(f"üöÄ DEBUG RESPONSE: Respuesta recibida de LlamaIndex")
                
                # La respuesta de LlamaIndex es un objeto con atributo .text
                response_text = None
                if hasattr(response, 'text'):
                    response_text = response.text
                elif hasattr(response, 'response'):
                    response_text = response.response
                else:
                    response_text = str(response)
                
                # üîí GARANTIZAR: No exceder 1000 caracteres bajo ninguna circunstancia
                if response_text and len(response_text) > 1000:
                    last_space = response_text[:1000].rfind(' ')
                    response_text = response_text[:last_space] if last_space > 900 else response_text[:1000]
                
                return response_text
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è LlamaIndex fall√≥: {str(e)}")
        
        # Fallback 2: REST API como √∫ltimo recurso
        logger.debug("üîÑ Usando REST API como fallback")
        return await self._call_gemini_rest_api(prompt)
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Obtiene estad√≠sticas del cliente
        
        Returns:
            Diccionario con estad√≠sticas de uso
        """
        current_time = time.time()
        recent_requests = [t for t in self.request_times if current_time - t < 60]
        
        return {
            "initialized": self._initialized,
            "has_api_key": bool(self.api_key),
            "model_loaded": self.model is not None,
            "requests_last_minute": len(recent_requests),
            "max_requests_per_minute": self.max_requests_per_minute,
            "rate_limit_utilization": len(recent_requests) / self.max_requests_per_minute if self.max_requests_per_minute > 0 else 0,
        }

