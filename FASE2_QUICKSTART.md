# ðŸš€ QUICKSTART - FASE 2: Configuraciones Avanzadas

## âš¡ TL;DR - Empieza en 3 pasos

### **Paso 1: Activar feature flags**
```bash
export USE_GEMINI_CLIENT=true
export USE_ADVANCED_MODEL_CONFIGS=true
```

### **Paso 2: Arrancar servidor**
```bash
cd /Users/user/Desktop/chatbot-ai-service-multitenant-
./run_server.sh
```

### **Paso 3: Probar endpoint**
```bash
curl -X POST "http://localhost:8000/tenants/473173/classify" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hola, quiero agendar una cita",
    "user_context": {},
    "session_id": "test_123"
  }'
```

**Busca en los logs:**
```
INFO - âœ… Configuraciones avanzadas de modelo habilitadas (USE_ADVANCED_MODEL_CONFIGS=true)
DEBUG - ðŸ”„ Delegando a GeminiClient con task_type='intent_classification'
INFO - âœ… Modelo gemini-2.0-flash creado con config personalizada (temp=0.0)
```

---

## ðŸŽ¯ Â¿QuÃ© hace la Fase 2?

Optimiza automÃ¡ticamente el modelo Gemini segÃºn la tarea:

| **Tarea** | **Temperature** | **Objetivo** | **Mejora Esperada** |
|-----------|----------------|--------------|---------------------|
| Clasificar intenciones | 0.0 | PrecisiÃ³n mÃ¡xima | +5-10% precisiÃ³n |
| Extraer datos | 0.0 | Sin errores | Menos falsos positivos |
| Validar datos | 0.0 | Estricto | Rechaza invÃ¡lidos |
| Normalizar ciudades | 0.0 | Consistente | +10% precisiÃ³n |
| Analizar registro | 0.1 | Casi determinÃ­stico | Mejor comprensiÃ³n |
| Chat con sesiÃ³n | 0.6 | Balanceado | MÃ¡s consistente |
| Chat conversacional | 0.7 | Natural | MÃ¡s humano |

---

## ðŸ§ª CÃ³mo Validar que Funciona

### **Test 1: Verificar que estÃ¡ activado**
```bash
# En los logs del servidor debes ver:
INFO - âœ… GeminiClient habilitado via feature flag USE_GEMINI_CLIENT=true
INFO - âœ… Configuraciones avanzadas de modelo habilitadas (USE_ADVANCED_MODEL_CONFIGS=true)
```

### **Test 2: ClasificaciÃ³n con temperature=0.0**
```bash
# Hacer la misma clasificaciÃ³n 3 veces
curl -X POST "http://localhost:8000/tenants/473173/classify" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hola, quiero agendar una cita",
    "user_context": {},
    "session_id": "test_123"
  }'

# Resultado esperado: SIEMPRE la misma categorÃ­a (determinÃ­stico)
# Logs: task_type='intent_classification', temp=0.0
```

### **Test 3: Chat con temperature=0.7**
```bash
# Hacer el mismo chat 3 veces
curl -X POST "http://localhost:8000/tenants/473173/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "CuÃ©ntame sobre el candidato",
    "user_context": {},
    "session_id": "test_123"
  }'

# Resultado esperado: Respuestas VARIADAS pero coherentes (natural)
# Logs: task_type='chat_conversational', temp=0.7
```

### **Test 4: Cache de modelos**
```bash
# Primera clasificaciÃ³n: Crea el modelo
# Logs: INFO - âœ… Modelo gemini-2.0-flash creado con config personalizada (temp=0.0)

# Segunda clasificaciÃ³n: Usa cache
# Logs: DEBUG - ðŸ“¦ Usando modelo cacheado para task_type
```

---

## ðŸ” Debugging

### **Ver quÃ© task_type se usa en cada mÃ©todo:**
```python
from chatbot_ai_service.config.model_configs import METHOD_TO_TASK_TYPE

for method, task in METHOD_TO_TASK_TYPE.items():
    print(f"{method} â†’ {task}")

# Salida:
# classify_intent â†’ intent_classification
# extract_data â†’ data_extraction
# validate_data â†’ data_validation
# ...
```

### **Ver configuraciÃ³n de un task_type:**
```python
from chatbot_ai_service.config.model_configs import get_config_for_task

config = get_config_for_task("intent_classification")
print(config)

# {
#   "model_name": "gemini-2.0-flash",
#   "temperature": 0.0,
#   "top_p": 0.1,
#   "top_k": 1,
#   "max_output_tokens": 100,
#   "response_mime_type": "application/json",
#   "description": "Para clasificar intenciones con alta precisiÃ³n"
# }
```

### **Ver estadÃ­sticas del GeminiClient:**
```python
from chatbot_ai_service.services.ai_service import ai_service

if ai_service.gemini_client:
    stats = ai_service.gemini_client.get_stats()
    print(f"Modelos en cache: {len(ai_service.gemini_client.models_cache)}")
    print(f"Requests Ãºltimo minuto: {stats['requests_last_minute']}")
```

---

## ðŸ›¡ï¸ Rollback (Si algo sale mal)

### **OpciÃ³n 1: Desactivar solo configs avanzadas**
```bash
export USE_GEMINI_CLIENT=true
export USE_ADVANCED_MODEL_CONFIGS=false  # â† Desactivar
# Reiniciar servidor
```

### **OpciÃ³n 2: Desactivar todo (volver a original)**
```bash
export USE_GEMINI_CLIENT=false  # â† Desactivar
export USE_ADVANCED_MODEL_CONFIGS=false
# Reiniciar servidor
```

**Sistema vuelve a 100% funcionalidad original** âœ…

---

## ðŸ“Š ComparaciÃ³n: Antes vs DespuÃ©s

### **ANTES (Sin Fase 2):**
```python
# Todas las tareas usan la misma configuraciÃ³n
prompt = "Clasifica esta intenciÃ³n: ..."
response = model.generate_content(prompt)  # temperature default
```

### **DESPUÃ‰S (Con Fase 2):**
```python
# Cada tarea usa su configuraciÃ³n Ã³ptima
task_type = "intent_classification"
config = get_config_for_task(task_type)  # temperature=0.0, top_k=1
model = get_or_create_model(config)      # Cache
response = model.generate_content(prompt)
```

**Resultado:**
- ClasificaciÃ³n: MÃ¡s precisa (+5-10%)
- ExtracciÃ³n: MÃ¡s confiable
- Chat: MÃ¡s natural
- Zero overhead (cache de modelos)

---

## ðŸš€ PrÃ³ximo Paso: Fase 3

Una vez validado que Fase 2 funciona, el prÃ³ximo paso es:

### **Fase 3: Structured Output (JSON Schemas)**
- Crear schemas JSON para cada task_type
- ValidaciÃ³n automÃ¡tica con Pydantic
- Garantizar respuestas JSON vÃ¡lidas >95%

**DuraciÃ³n estimada:** 3-4 dÃ­as

---

## ðŸ’¡ Tips

1. **Logs detallados:** Usa `LOG_LEVEL=DEBUG` para ver todo
2. **Testing local:** Empieza con feature flags activados
3. **Monitoreo:** Observa los logs del task_type usado
4. **Cache:** Segunda llamada siempre mÃ¡s rÃ¡pida
5. **Rollback:** Un solo comando y listo

---

**Â¿Dudas?** Revisa `FASE2_IMPLEMENTADA.md` para documentaciÃ³n completa.

