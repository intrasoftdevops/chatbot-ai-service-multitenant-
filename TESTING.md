# ðŸ§ª GuÃ­a de Testing - Chatbot AI Service Multi-Tenant

## ðŸ“‹ DescripciÃ³n

Esta guÃ­a explica cÃ³mo ejecutar y mantener los tests del sistema de clasificaciÃ³n de intenciones polÃ­ticas. Los tests validan todas las 12 categorÃ­as de intenciones y sus acciones correspondientes.

## ðŸ—ï¸ Estructura de Tests

```
tests/
â”œâ”€â”€ conftest.py                    # ConfiguraciÃ³n y fixtures de pytest
â”œâ”€â”€ test_intent_classification.py  # Tests de clasificaciÃ³n de intenciones
â”œâ”€â”€ test_action_handlers.py        # Tests de manejadores de acciones
â”œâ”€â”€ test_tenant_integration.py     # Tests de integraciÃ³n con tenants
â”œâ”€â”€ test_api_endpoints.py          # Tests de endpoints de la API
â””â”€â”€ __init__.py                    # Package marker
```

## ðŸŽ¯ CategorÃ­as de Tests

### Tests Unitarios
- **ClasificaciÃ³n de Intenciones**: Valida que cada categorÃ­a se clasifique correctamente
- **Manejadores de Acciones**: Verifica que las acciones se ejecuten segÃºn la intenciÃ³n
- **ConfiguraciÃ³n de Tenants**: Prueba diferentes configuraciones por tenant

### Tests de IntegraciÃ³n
- **Flujo Completo**: ClasificaciÃ³n + AcciÃ³n + Respuesta
- **Multi-Tenant**: Diferentes tenants con configuraciones especÃ­ficas
- **APIs**: Endpoints de la API con mocks y casos reales

## ðŸš€ Ejecutar Tests

### InstalaciÃ³n de Dependencias
```bash
# Instalar dependencias de testing
python run_tests.py --install-deps

# O manualmente
pip install pytest pytest-asyncio pytest-cov httpx fastapi[all]
```

### EjecuciÃ³n BÃ¡sica
```bash
# Ejecutar todos los tests
python run_tests.py

# Tests con output detallado
python run_tests.py --verbose

# Tests con reporte de cobertura
python run_tests.py --coverage
```

### Tipos de Tests EspecÃ­ficos
```bash
# Solo tests unitarios
python run_tests.py --type unit

# Solo tests de integraciÃ³n
python run_tests.py --type integration

# Tests rÃ¡pidos (sin tests lentos)
python run_tests.py --type fast

# Tests especÃ­ficos
python run_tests.py --type specific
```

### EjecuciÃ³n Directa con pytest
```bash
# Tests especÃ­ficos
pytest tests/test_intent_classification.py::TestIntentClassification::test_malicious_intent_classification -v

# Tests con marcadores
pytest -m "not slow" -v
pytest -m integration -v
pytest -m unit -v

# Tests con cobertura
pytest --cov=chatbot_ai_service --cov-report=html
```

## ðŸ“Š CategorÃ­as de Intenciones Validadas

| CategorÃ­a | Test Cases | Validaciones |
|-----------|------------|--------------|
| **malicioso** | 4 ejemplos | ClasificaciÃ³n correcta, bloqueo de usuario |
| **cita_campaÃ±a** | 5 ejemplos | RedirecciÃ³n a Calendly |
| **saludo_apoyo** | 5 ejemplos | Respuesta de gratitud, compartir links |
| **publicidad_info** | 4 ejemplos | RedirecciÃ³n a formularios |
| **conocer_candidato** | 5 ejemplos | RedirecciÃ³n a DQBot, notificaciÃ³n ciudad |
| **actualizacion_datos** | 4 ejemplos | ActualizaciÃ³n dinÃ¡mica de datos |
| **solicitud_funcional** | 4 ejemplos | Info de puntos/tribu/referidos |
| **colaboracion_voluntariado** | 4 ejemplos | ClasificaciÃ³n por Ã¡reas (9 opciones) |
| **quejas** | 4 ejemplos | Registro en base de datos |
| **lider** | 4 ejemplos | Registro en base de datos de leads |
| **atencion_humano** | 4 ejemplos | RedirecciÃ³n a voluntario humano |
| **atencion_equipo_interno** | 4 ejemplos | ValidaciÃ³n permisos, BackOffice |

## ðŸ”§ ConfiguraciÃ³n de Tests

### Fixtures Principales
- **sample_messages**: Mensajes de ejemplo para cada categorÃ­a
- **sample_tenant_configs**: Configuraciones de tenant (activo, inactivo, sin IA, sin links)
- **mock_tenant_service**: Mock del servicio de tenants
- **mock_ai_service**: Mock del servicio de IA
- **mock_firebase_config**: Mock de configuraciÃ³n de Firebase

### Marcadores de Tests
- `@pytest.mark.unit`: Tests unitarios
- `@pytest.mark.integration`: Tests de integraciÃ³n
- `@pytest.mark.slow`: Tests que toman mÃ¡s tiempo
- `@pytest.mark.ai`: Tests que requieren servicios de IA
- `@pytest.mark.firebase`: Tests que requieren conexiÃ³n a Firebase

## ðŸ“ˆ MÃ©tricas de Cobertura

### Objetivos de Cobertura
- **LÃ­neas de cÃ³digo**: > 90%
- **Funciones**: > 95%
- **Clases**: > 90%
- **Branches**: > 85%

### Generar Reporte de Cobertura
```bash
# HTML report
pytest --cov=chatbot_ai_service --cov-report=html

# Terminal report
pytest --cov=chatbot_ai_service --cov-report=term-missing

# XML report (para CI/CD)
pytest --cov=chatbot_ai_service --cov-report=xml
```

## ðŸ› Debugging Tests

### Tests Failing
```bash
# Ver detalles del error
pytest tests/test_intent_classification.py::test_malicious_intent_classification -v -s

# Ver solo el primer test que falla
pytest -x

# Ver output de print statements
pytest -s
```

### Logs Detallados
```bash
# Con logs de la aplicaciÃ³n
pytest --log-cli-level=DEBUG

# Con logs especÃ­ficos
pytest --log-cli-level=INFO --log-cli-format="%(asctime)s [%(levelname)8s] %(message)s"
```

## ðŸ”„ CI/CD Integration

### GitHub Actions (ejemplo)
```yaml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.11
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov
      - name: Run tests
        run: pytest --cov=chatbot_ai_service --cov-report=xml
      - name: Upload coverage
        uses: codecov/codecov-action@v1
```

## ðŸ“ Escribir Nuevos Tests

### Estructura de un Test
```python
@pytest.mark.asyncio
async def test_new_intent_category(self, intent_classification_service):
    """Test para nueva categorÃ­a de intenciÃ³n"""
    request = ClassificationRequest(
        message="Mensaje de ejemplo",
        tenant_id="test_tenant",
        user_context={"phone": "+573001234567"}
    )
    
    classification = await intent_classification_service.classify_intent(request)
    
    assert classification.category == IntentCategory.NUEVA_CATEGORIA
    assert classification.confidence > 0.5
    assert classification.action.action_type == "nueva_accion"
```

### Mejores PrÃ¡cticas
1. **Usar fixtures** para configuraciÃ³n comÃºn
2. **Mockear servicios externos** (Firebase, IA)
3. **Testear casos edge** (tenant inactivo, errores)
4. **Validar tanto Ã©xito como fallos**
5. **Usar nombres descriptivos** para tests
6. **Agrupar tests relacionados** en clases

## ðŸš¨ Troubleshooting

### Problemas Comunes

#### ImportError
```bash
# Agregar path del proyecto
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src/main/python"
```

#### Tests Asyncio
```python
# Usar pytest-asyncio
pip install pytest-asyncio
```

#### Mock Issues
```python
# Verificar que el mock estÃ¡ configurado correctamente
from unittest.mock import AsyncMock, patch
```

## ðŸ“Š Resultados Esperados

### EjecuciÃ³n Exitosa
```
========================= test session starts =========================
tests/test_intent_classification.py::TestIntentClassification::test_malicious_intent_classification PASSED
tests/test_intent_classification.py::TestIntentClassification::test_campaign_appointment_classification PASSED
...
========================= 48 passed in 2.34s =========================
```

### Cobertura Objetivo
```
Name                                                    Stmts   Miss  Cover
---------------------------------------------------------------------
chatbot_ai_service/services/intent_classification_service.py     95      5    95%
chatbot_ai_service/services/action_handler_service.py           120     12    90%
chatbot_ai_service/controllers/classification_controller.py      85      8    91%
---------------------------------------------------------------------
TOTAL                                                             300     25    92%
```

Â¡Los tests estÃ¡n listos para validar todo el sistema de clasificaciÃ³n de intenciones polÃ­ticas! ðŸŽ¯

